{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "instant-victim",
   "metadata": {},
   "source": [
    "# Natural Language Processing(NLP)\n",
    "\n",
    "- 앞, 뒤 연관성이 있는 네트워크 (예로 RNN) 신경망을 사용하여 구성\n",
    "\n",
    "### 1. Statistical Language Model (통계적 언어 모델)\n",
    "\n",
    "\n",
    "### 2. Neural Network Language Model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generous-stack",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "qualified-front",
   "metadata": {},
   "source": [
    "## Statistical Language Model (통계적 언어 모델)\n",
    "- SLMs are the type of models that assign Probabilities to the squences of words (단어의 배열에 확률을 할당하는 모델)\n",
    "\n",
    "- 코랩 사용하기(Colaboratory)\n",
    "  - 런타임 - 런타임 유형 변경 -> Tpu or Gpu로 변경 가능하다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustainable-soldier",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "- 문장과 단어를 인지할 수 있도록 자르는 방식.\n",
    "- 문장 토큰화, 단어 터큰화, Subword 토큰화 등 다양한 단위의 토큰화가 존재."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prepared-switch",
   "metadata": {},
   "source": [
    "- 단순하게 띄어쓰기로만 나누는 것 외에 다른 방식이 필요 (축약형, 특수문자에 따라 다르게 인지할 수 있다)\n",
    "- 단순히 특수문자만 제거하면 축약형에 문제 발생 \n",
    "- -> 영어 단어 토크나이저 중 하나인 TreebankWordTokenizer 사용하기 (Penn Treebank Tokenization 규칙을 사용)\n",
    "- Penn Treebank Tokenization 규칙\n",
    "    - 하이푼으로 구성된 단어는 하나로 유지한다.\n",
    "    - doesn't과 같이 어포스트로피로 '접어'가 함께하는 단어는 분리해준다.\n",
    "- pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gorgeous-colonial",
   "metadata": {},
   "source": [
    "## Tokenization for Korean(KoNLPy)\n",
    "- 기본적으로 한국어 토큰화가 영어 토큰화보다 난이도가 훨씬 높다.\n",
    "- 띄어쓰기가 틀리거나 맞춤법이 틀려도 쉽게 읽을 수 있어 다른 언어로 읽을 수 있다\n",
    "- 한국어는 교착어이다. (조사나 어미와 같은 문법 형태소들이 결합하여 문법적인 기능이 부여되는 언어)-> 주어나 목적어가 없으면 의미가 없는 단어들은 불용어 처리를 한다.\n",
    "- 한국어는 띄어쓰기가 잘 지켜지지 않는다.(띄어쓰기가 잘 안지켜져도 읽을 수 있다) -> py-hanspe2 패키지, ko-spacing 패키지를 이용하여 띄어쓰기 교정.\n",
    "- 한국어는 주어 생략은 물론 어순도 중요하지 않다\n",
    "- 한자어라는 특성상 하나의 음절조차도 다른 의미를 가질 수 있다. (영어는 알파벳 자체에 큰 의미가 있지 않다)\n",
    "- 한국어를 사용하는 패키지 설치 : !pip install konlpy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resistant-hughes",
   "metadata": {},
   "source": [
    "## 토크나이저마다 규칙이 달라, 표현이 다르다\n",
    "- 목적에 맞는 토크나이저를 이용하라"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marine-pickup",
   "metadata": {},
   "source": [
    "## 문장 토크나이징\n",
    "- 단순히 물음표, 온점, 느낌표 만으로는 문장을 나눌 수가 없다. 온점은 문장 끝이 아니더라도 올 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driving-petersburg",
   "metadata": {},
   "source": [
    "##  Cleaning (정제) and Normalization(정규화)\n",
    "- 정제 : 불필요한 데이터를 제가하는 작업. 텍스트 중간중간 껴잇는 숫자나 필요없는 기호를 제거하거나, 한글의 경우 은,는,이,가 등은 비즈니스상 필요가 없을 수 있다. 또는 띄어쓰기나 맞춤법을 확인해 깨끗한 데이터를 만들어 내는 작업도 포함. \n",
    "  - 정규 표현식을 이용한 노이즈 데이터 제거\n",
    "  - 인코딩 문제 해결 (utf-9, euc-kr 등)\n",
    "  - 등장 빈도가 적은 단어 제거\n",
    "  - 길이가 짧은 단어의 제거 (영어에서는 by, at 등)\n",
    "  - 불용어 제거\n",
    "    - 영어ㅡ의 경우 the 는 거의 모든 텍스트 데이터에서 등장 빈도수가 많지만 실제 의미를 갖지 않기 때문에 제거\n",
    "    - 한국어의 경우 그럼, 위하, 때, 있, 그것, 사실, 경우, 어떤, 은, 는, 을, 를 등 이 존재\n",
    "    \n",
    "    \n",
    "    \n",
    "- 정규화 : 문장의 복잡도를 줄여주는 과정. 같은 의미를 가지고 있는 여러 단어를 하나로 통합하는 드의 작업. 형태소 분석의 의미 보다는 사람이 직접 판단을 해주는 일.\n",
    "  - Lemmatization\n",
    "    - am, are, were, was -->be\n",
    "    - has, had -> have\n",
    "  - 10, 159, 123 -> 숫자가 중요하지 않은 경우\n",
    "  - ㅋ, ㅋㅋ, ㅋㅋㅋ, ㅋㅋㅋㅋㅋ --> ㅋㅋ\n",
    "  - Hmm, hmm, Hmmmmm --> hmm\n",
    "  - 대소문자 통합 등"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vietnamese-reception",
   "metadata": {},
   "source": [
    "## 정제 함수\n",
    "- KoSpacing : 한국어 정제를 위한 띄어쓰기 및 맞춤법 검사 패키지. 한국어는 띄어쓰기와 맞춤법이 지켜지지 않는 경우가 꽤나 많다.\n",
    "  - !pip install git+https://github.com/haven-jeon/PyKoSpacing.git\n",
    "  \n",
    "- hanspell : 문장의 띄어쓰기와 맞춤법 검사도 같이 해준다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controversial-confusion",
   "metadata": {},
   "source": [
    "## 정규화\n",
    "  - 텍스트 정규화는 통일할 수 있는 단어를 하나로 통일하기 위한 전처리 과정\n",
    "  - 사용자가 만들어 사용할 수 있으나, Stemming이나 Lemmatization을 활용\n",
    "  \n",
    "  - Stemming - 영어 정규화 : 어간을 추출하는 작업. 형태학적 분석을 단순화한 버전. 단어의 어미를 자르는 어미짐작의 작업. 섬세한 작업이 아니기 어간 추출 후에 나오는 결과 단어는 사전에 존재하지 않는 단어 일 수 도 있다. \n",
    "  - Lemmatization : 표제어 추출은 단어들이 다른 형태를 가지더라도, 뿌리 단어를 찾아가서 단어의 개수를 줄일 수 있는지 여부를 판단합니다. \n",
    "    예로, am, are, is는 단어는 다르지만 뿌리는 be라고 볼 수 있습니다. 이 때 단어들의 표제어는 be라고 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "curious-passion",
   "metadata": {},
   "source": [
    "## 한국어 정규화 - okt "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "descending-upper",
   "metadata": {},
   "source": [
    "## 한국어 토큰화 패키지\n",
    "- Mecab : 연산속도가 빠르고 분석 성능도 준수하여 선호도가 높다(window 사용 x)\n",
    "- Khaii : 카카오에서 개발, 최근 급부상\n",
    "- KOMORAN : 오탈자에 강건함\n",
    "- Soynlp : 학습기반으로 복합 명사 추출에 강함."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupied-texas",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "recognized-clock",
   "metadata": {},
   "source": [
    "## 텍스트의 수치화\n",
    "- 컴퓨터는 텍스트보다 숫자를 더 잘 처리하기 떄문에 자연어 처리 분야에서 텍스트를 숫자로 바꾸는 여러가지 기법이 있다.\n",
    "  - Interger Encoding\n",
    "      - 단어 토큰화 또는 형태소 토큰화를 수행했다면, 각 단어에 고유한 정수를 부여\n",
    "      - 중복 허용하지 않는 모든 단어들의 집합 (Vocabularry)\n",
    "  - Padding \n",
    "      - 각 문장(또는 문서)은 서로 길이가 다를 수 있습니다. 그런데 기계는 길이가 전부 동일한 문서들에 대해서는 하나의 행렬로 보고, 한꺼번에 묶어서 처리할 수 있습니다. 그래서 동일한 문장의 길이를 만들기 위해 길이를 확인하고, 부족시에 가상어 단어를 만들어 길이를 추가해줌\n",
    "  - Vectorization\n",
    "      - \n",
    "      - \n",
    "      - Document Term Matrix\n",
    "      - Tf-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colored-karma",
   "metadata": {},
   "source": [
    "###   Interger Encoding\n",
    "- 토큰화 된 단어 또는 형태소들에 숫자를 부여, 빈도수 높은 순으로 순서를 구성하거나 보통 ABC 또는 가나다 순으로 순서를 구성한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educated-valley",
   "metadata": {},
   "source": [
    "### [English] 단어 집합 만들기\n",
    "- 단어 집합이란 중복을 제거한 단어들의 집합"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "large-skating",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "chronic-springfield",
   "metadata": {},
   "source": [
    "# 2일차"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "large-pride",
   "metadata": {},
   "source": [
    "- 정수 인코딩 proprocessing.text.Tokenizer 사용\n",
    "- 상위 n개의 단어만 사용하게 하는 옵션 keras의 Tokenizer의 num_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instant-somerset",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "hidden-litigation",
   "metadata": {},
   "source": [
    "## 문장의 유사도\n",
    "- https://team-platform.tistory.com/34\n",
    "\n",
    "- n-gram : https://wikidocs.net/21692\n",
    "  - 이전 단어 n개를 고려하여 접근하는 방법\n",
    "  - A가 나왔을 때 B가 나올 확률 $P(B|A)$\n",
    "  - 일반적인 통계인 $P(is|An adorable little boy)$ 를 $P(is|boy)$ 가 아닌 $P(is|little  boy)$로 나타내기 위함\n",
    "  - n 1일때는 유니그램, 2일때는 바이그램, 3일때는 트라이그램, n이 4 이상일 때에는 숫자를 붙여 명명한다.\n",
    "  - 같은 문장에서, spreading 다음에 나올 단어를 예측할 때, 4-gram을 사용한다면 $P(\\omega |spreading)$ 이 아닌, spreading을 제외한 n-1에 해당되는 앞의 3개 단어(boy, is , ?)를 사용하여 $P(\\omega|boy is spreading) = \\left(\\frac{count(boy is spreading \\omega)}{count(boy is spreading)} \\right)$ 와 같이 계산할 수 있다\n",
    "  - 문제점\n",
    "      - 희소 문제\n",
    "      - n을 선택하는 것은 trade - off 문제"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sexual-chess",
   "metadata": {},
   "source": [
    "## DTM\n",
    "- 문서 단어 행렬\n",
    "- 다수의 문서에서 등장하는 각 단어들의 빈도를 행렬로 표현한 것.\n",
    "\n",
    "## TF-IDF\n",
    "- tf : 단어 빈도수. 특정 문서 d에서 특정 단어 T의 '등장 횟수'\n",
    "- df : 특정 단어 t가 등장한 '문서'의 수\n",
    "- IDF : df(t)에 반비례하는 수\n",
    "- df(t)가 0이 나올 수 있기 때문에 1를 더해주고, log를 사용하여 값이 너무 크게 발생하는 것을 방지하기 위함.\n",
    "$idf(d,t) =  log\\left(\\frac{n}{1 + df(t)} \\right)$\n",
    "- TF-IDF는 모든 문서에서 자주 등장하는 단어는 중요도가 낮고, 특정 문서에서 나오는 단어는 중요도가 높다고 판단, the 같은 경우 df는 엄청나게 나올 수 있다. 이런 단어의 IDF, 즉 ,  중요도는 매우 적다고 판단할 수 있다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "average-light",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "anonymous-tulsa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finite-appeal",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interracial-relative",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
