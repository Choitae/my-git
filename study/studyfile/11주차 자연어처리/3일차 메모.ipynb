{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "detailed-bangkok",
   "metadata": {},
   "source": [
    "# 소민호 선생님 \n",
    "## 2 / 24까지 수업, 2 / 25 부터 3 / 4까지 미니 프로젝트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strong-parts",
   "metadata": {},
   "source": [
    "## Language Model \n",
    "- 단어 시퀸스(연결 구조)에 확률을 할당하여 진행\n",
    "- 어떤 단어가 등장해야 자연스러운지를 판단\n",
    "- 1. $P(W) = P(w_{1},w_{2},\\cdots,w_{n}) = n$개의 단어가 동시에 등장할 확률 = $n$개의 단어가 자연스러운 문장을 만들 확률\n",
    "- 2. $P(w_5 | w_1, w_2, w_3, w_4)$ 1,2,3,4,라는 단어가 배열되었을 떄 $w_5$가 배열 될 확률\n",
    "- 어떤 문장이 가장 그럴듯한지 판단 -> 사람이 이미 만들어놓은 문장을 확인해서 기계가 배열한다\n",
    "- $P(\"Its water is so transparent\") = P(its) \\cdot P(water|its) \\cdot P()....$\n",
    "- P = count / count => 내가 이러한 문장을 만든 빈도수를 세겠다는 것이 Count.\n",
    "    - 때문에 데이터가 방대하지 않으면 제대로 카운트가 불가능하다\n",
    " \n",
    "- n-gram으로 확률 모델 구하기\n",
    "    - Bigram = $P(w_i|w_n) = \\frac {count(w_nw_i)}{count(w_n)}$\n",
    "    \n",
    "- n이 너무 작으면 장기 의존성 문제 발생.\n",
    "    - 이전 데이터로 다음 데이터를 (마음에 드는 데이터를) 구하지 못하는 문제.-> N이 작으면, 분모가 너무 커지기 때문에 확률이 너무작아짐.\n",
    " \n",
    "- 딥러닝으로 단어의 유사도를 이용하여 훈련 데이터에 없던 시퀸스를 생성. 문제를 해결"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offensive-large",
   "metadata": {},
   "source": [
    "## Vectorization (문장의 벡터화) : Word Embedding\n",
    "- 원-핫 인코딩 사용 : 문장 내에 단어가 있는지, 없는지 여부 확인 -> 단어 벡터 간 유사도를 구할 수 없다는 것이 단점.\n",
    "- 벡터의 차원이 단어의 집합의 크기\n",
    "- 가로 줄은 문장 개수, 세로는 문장의 크기. / 가로는 문장 전체를, 세로는 단어 자체를 확인하겠다는 것.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aging-adrian",
   "metadata": {},
   "source": [
    "## 머신러닝 \n",
    "- 기존 프로그래밍 개발은 함수를 만들어서 출력을 만드는 것. 사람이 찾아서 기계한테 알려준다.\n",
    "- 머신러닝은 인수와 결과값을 이용해서 관계식을 구하는 방식.\n",
    "- 머신러닝- 지도(데이터, 출력값->값, 추측, 회귀 등), 비지도(데이터 -> 군집), 준지도(섞여있는 데이터 중 유사한 얼굴 찾고, 테그를 찾는 방식-> 지도, 비지도의 사이), 강화 학습\n",
    "- 이정도 될거야~ 회귀, \n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ongoing-religious",
   "metadata": {},
   "source": [
    "시그모이드 : 확률(0~1). 데이터가 없으면 예측이 불가\n",
    "relu : 음수 데이터 삭제. 데이터 압축 (CNN에서 사용)\n",
    "tahn : 방향(-1 ~ 1). 예측. 닮은 정도, 유사성 (text 분류에 사용). \n",
    "시퀀스 : 1번씬, 2번씬,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharing-nursing",
   "metadata": {},
   "source": [
    "tokenization(데이터 정렬) -> Vectorization() -> model -> predcitation ->\n",
    "predication (sigmoid -> 0 or 1, 다중 -> softmax, \n",
    "colunm 문장 길이, row 단어 개수  -> 첫 번째 hidden layer 뉴런 개수 (vocab size -> imbedding 할 때 필요)\n",
    "AI에서 학습 -> FFND \n",
    "NNLM (Neural Network Language Model)\n",
    "-> (임베딩을 하기 위한 작업). n개의 단어로 n+1 단어를 예측하는 모델.\n",
    "-> input layer : 원핫 인코딩으로 된 단어가 입력. 원-핫-벡터로 한다. (단위행렬 형식이 된다) 어떠한 위치에 어떤 단어가 있는지 가중치를 알고자 해서. \n",
    "-> projection layer(투사층 : 빛을 비춰본다) = 임베딩 층 : 가중치 행렬과 곱해져서 투사층을 형성( 랜덤으로 들어온다. 시퀸스.) 투사를 통해 input의 lookup table을 구하자.\n",
    "-> 임의의 투사층의 크기를 M이라고 하면, M이 크면 과대적합(모델, 데이터가 복잡해지면 이에 너무 연관되어 계산하여, 예상이 힘듬), 작으면 과소적합(CNN의 층이 많으면 손실률이 너무 많아지기 때문에)\n",
    "- $W_{VxM}$ : v : vocab size, M : 투사층의 크기\n",
    "- $x_{fat} \\cdot W_{VxM} = e_{fat}$ : Lookup Table 내가 보고자 하는 위치에 있는 단어의 벡터값\n",
    "- M이 5라고 가정했을 때, vocab size가 7이면, 실질적으로 출력되는 단어의 lookup table = 1x5\n",
    "- 쌓는 연산 : concatenate . 총 결과 4x5. 여기까지가 hidden layer 이전 단계. 최종적 원-핫-인코더로 출력\n",
    "- 히든 레이어를 통해 나오는 출력 값이 유사도.\n",
    "\n",
    "- 단어를 모아놓고 단어들의 유사성을 찾는 것 word Embedding\n",
    "- 워드 임베딩\n",
    "     - 랜덤 초기화 임베딩 \n",
    "          - 딥러닝, 자연어 처리에 사용. \n",
    "     - 사전 훈련된 임베딩\n",
    "          - 임베딩 시간이 없을 때 사용. 대표적으로 Word2Vec, FastText, Glove 가 존재\n",
    "          \n",
    "- Embedding layer(랜덤 초기화 방법)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defensive-strap",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
