{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "\n",
    "\n",
    "   - model = Sequential()\n",
    "     : 층을 쌓아야 하는데, Sequential(순차적)이라는 틀을 만들고 하나씩 추가해주는 것. 거푸집 같은 역할\n",
    "\n",
    "   - model.compile(loss='binary_crossentropy',\n",
    "           optimizer='adam',\n",
    "           metrics=['accuracy'])\n",
    "\n",
    "     : 손실함수, 최적화알고리즘 함수를 어떤 것으로 쓸지를 정하는 것. metrics=[accuracy] 정확도표기 (케라스에서만 쓰는 것) \n",
    "  -  numpy.random.seed(seed)\n",
    "  -  tf.compat.v1.set_random_seed(3)\n",
    "     :  층을 쌓을 수록 가중치 파라미터, bias 파라미터 랜덤으로 생성해주는데, 예제는 다시 실행했을 때 같은 순서로 같은 값이 나오게 하기 위해서 seed 값을 저장했다\n",
    "     : random 어떤 조건으로 숫자를 발생시켜주는 것 / seed가 고정되어있으면 항상 나오는 순서가 동일하다.\n",
    "       seed값을 바꾸는 방법 : 시작할 때 마다 밀리 세컨드마다 바꾸어서 전체적으로 바꾸어야한다.\n",
    "\n",
    "\n",
    "  -  예시\n",
    "     y = f(x,y,z)\n",
    "     x = 1\n",
    "     p = 2\n",
    "     z = 3 \n",
    "     특정 입력값에서 특정 출력값이 나오는 것이 함수 \n",
    "     랜덤도 함수\n",
    "  - 동일한 인풋 --> 동일한 아웃풋\n",
    "     바운더리(범위) /속성/ 순서를 등의 변칙이 있는데, 순서 order를 결정하는 변수가 seed.\n",
    "     seed에 보통 time을 이용하여 사용한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(4) #random의 seed 값을 고정한다. 이러한 경우 횟수마다 나타나느 값이 같다. # 이 구문이 없을 경우 값이 다르게 나온다 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "1\n",
      "6\n",
      "4\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,7):\n",
    "    print(random.randrange(1,7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seed 가 같으면, 어디서 작동시키던간에 같은 숫자가 나온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 세즈노프스키 교수의 광물과 돌을 구분하기 - 과적합 피하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed 값 설정\n",
    "numpy.random.seed(3)\n",
    "tf.compat.v1.set_random_seed(3)\n",
    "\n",
    "# 랜덤으로 생기는 가중치, bias 값은 결과값에 영향을 미친다. --> 여러번 돌려야 한다.\n",
    "\n",
    "#데이터 적용\n",
    "df = pd.read_csv('example/080228-master/deeplearning/dataset/sonar.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 208 entries, 0 to 207\n",
      "Data columns (total 61 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   0       208 non-null    float64\n",
      " 1   1       208 non-null    float64\n",
      " 2   2       208 non-null    float64\n",
      " 3   3       208 non-null    float64\n",
      " 4   4       208 non-null    float64\n",
      " 5   5       208 non-null    float64\n",
      " 6   6       208 non-null    float64\n",
      " 7   7       208 non-null    float64\n",
      " 8   8       208 non-null    float64\n",
      " 9   9       208 non-null    float64\n",
      " 10  10      208 non-null    float64\n",
      " 11  11      208 non-null    float64\n",
      " 12  12      208 non-null    float64\n",
      " 13  13      208 non-null    float64\n",
      " 14  14      208 non-null    float64\n",
      " 15  15      208 non-null    float64\n",
      " 16  16      208 non-null    float64\n",
      " 17  17      208 non-null    float64\n",
      " 18  18      208 non-null    float64\n",
      " 19  19      208 non-null    float64\n",
      " 20  20      208 non-null    float64\n",
      " 21  21      208 non-null    float64\n",
      " 22  22      208 non-null    float64\n",
      " 23  23      208 non-null    float64\n",
      " 24  24      208 non-null    float64\n",
      " 25  25      208 non-null    float64\n",
      " 26  26      208 non-null    float64\n",
      " 27  27      208 non-null    float64\n",
      " 28  28      208 non-null    float64\n",
      " 29  29      208 non-null    float64\n",
      " 30  30      208 non-null    float64\n",
      " 31  31      208 non-null    float64\n",
      " 32  32      208 non-null    float64\n",
      " 33  33      208 non-null    float64\n",
      " 34  34      208 non-null    float64\n",
      " 35  35      208 non-null    float64\n",
      " 36  36      208 non-null    float64\n",
      " 37  37      208 non-null    float64\n",
      " 38  38      208 non-null    float64\n",
      " 39  39      208 non-null    float64\n",
      " 40  40      208 non-null    float64\n",
      " 41  41      208 non-null    float64\n",
      " 42  42      208 non-null    float64\n",
      " 43  43      208 non-null    float64\n",
      " 44  44      208 non-null    float64\n",
      " 45  45      208 non-null    float64\n",
      " 46  46      208 non-null    float64\n",
      " 47  47      208 non-null    float64\n",
      " 48  48      208 non-null    float64\n",
      " 49  49      208 non-null    float64\n",
      " 50  50      208 non-null    float64\n",
      " 51  51      208 non-null    float64\n",
      " 52  52      208 non-null    float64\n",
      " 53  53      208 non-null    float64\n",
      " 54  54      208 non-null    float64\n",
      " 55  55      208 non-null    float64\n",
      " 56  56      208 non-null    float64\n",
      " 57  57      208 non-null    float64\n",
      " 58  58      208 non-null    float64\n",
      " 59  59      208 non-null    float64\n",
      " 60  60      208 non-null    object \n",
      "dtypes: float64(60), object(1)\n",
      "memory usage: 99.2+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df.info()) # 샘플 208개, 마지막 타입은 object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0       1       2       3       4       5       6       7       8   \\\n",
      "0  0.0200  0.0371  0.0428  0.0207  0.0954  0.0986  0.1539  0.1601  0.3109   \n",
      "1  0.0453  0.0523  0.0843  0.0689  0.1183  0.2583  0.2156  0.3481  0.3337   \n",
      "2  0.0262  0.0582  0.1099  0.1083  0.0974  0.2280  0.2431  0.3771  0.5598   \n",
      "3  0.0100  0.0171  0.0623  0.0205  0.0205  0.0368  0.1098  0.1276  0.0598   \n",
      "4  0.0762  0.0666  0.0481  0.0394  0.0590  0.0649  0.1209  0.2467  0.3564   \n",
      "\n",
      "       9   ...      51      52      53      54      55      56      57  \\\n",
      "0  0.2111  ...  0.0027  0.0065  0.0159  0.0072  0.0167  0.0180  0.0084   \n",
      "1  0.2872  ...  0.0084  0.0089  0.0048  0.0094  0.0191  0.0140  0.0049   \n",
      "2  0.6194  ...  0.0232  0.0166  0.0095  0.0180  0.0244  0.0316  0.0164   \n",
      "3  0.1264  ...  0.0121  0.0036  0.0150  0.0085  0.0073  0.0050  0.0044   \n",
      "4  0.4459  ...  0.0031  0.0054  0.0105  0.0110  0.0015  0.0072  0.0048   \n",
      "\n",
      "       58      59  60  \n",
      "0  0.0090  0.0032   R  \n",
      "1  0.0052  0.0044   R  \n",
      "2  0.0095  0.0078   R  \n",
      "3  0.0040  0.0117   R  \n",
      "4  0.0107  0.0094   R  \n",
      "\n",
      "[5 rows x 61 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "            17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "            34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
       "            51, 52, 53, 54, 55, 56, 57, 58, 59, 60],\n",
       "           dtype='int64')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = df.values\n",
    "X = dataset[:,0:60].astype(float)\n",
    "Y_obj = dataset[:,60]\n",
    "\n",
    "# 문자열 변환\n",
    "e = LabelEncoder()\n",
    "e.fit(Y_obj)\n",
    "Y = e.transform(Y_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다중 출력이 아니라, 두개의 출력을 나타내는 것이기 떄문에 tf.keras.utils.to_categorical(Y)  이를 사용할 필요는 없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 설정\n",
    "model = Sequential()\n",
    "model.add(Dense(24,  input_dim=60, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(loss='mean_squared_error',\n",
    "            optimizer='adam',\n",
    "            metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 24)                1464      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                250       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 1,725\n",
      "Trainable params: 1,725\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1층 모델 : 가중치 파라미터 60*24 = 1440개/ 바이오스 파라미터 24 개, 합 = 1464개\n",
    "# 2층 모델 : 가중치 파라미터 24*10 = 240개/ 바이오스 파라미터 10 개, 합 =  250개\n",
    "# 3층 모델 : 가중치 파라미터 10*1 = 10개 / 바이오스 파라미터 1개, 합 = 11개\n",
    "# 3개의 모델 총 1464 + 250 + 11 = 1725개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.2432 - accuracy: 0.5865\n",
      "Epoch 2/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.2288 - accuracy: 0.6394\n",
      "Epoch 3/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.2187 - accuracy: 0.6490\n",
      "Epoch 4/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.2031 - accuracy: 0.7260\n",
      "Epoch 5/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1893 - accuracy: 0.7692\n",
      "Epoch 6/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.1802 - accuracy: 0.7596\n",
      "Epoch 7/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.1677 - accuracy: 0.7740\n",
      "Epoch 8/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.1590 - accuracy: 0.7837\n",
      "Epoch 9/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1558 - accuracy: 0.7885\n",
      "Epoch 10/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.1483 - accuracy: 0.7788\n",
      "Epoch 11/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.1467 - accuracy: 0.7933\n",
      "Epoch 12/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.1380 - accuracy: 0.8029\n",
      "Epoch 13/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1346 - accuracy: 0.8173\n",
      "Epoch 14/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1329 - accuracy: 0.8077\n",
      "Epoch 15/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1291 - accuracy: 0.8077\n",
      "Epoch 16/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.1317 - accuracy: 0.8269\n",
      "Epoch 17/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.1242 - accuracy: 0.8462\n",
      "Epoch 18/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.1227 - accuracy: 0.8269\n",
      "Epoch 19/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1251 - accuracy: 0.8317\n",
      "Epoch 20/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.1202 - accuracy: 0.8269\n",
      "Epoch 21/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.1159 - accuracy: 0.8365\n",
      "Epoch 22/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.1163 - accuracy: 0.8462\n",
      "Epoch 23/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.1109 - accuracy: 0.8606\n",
      "Epoch 24/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.1103 - accuracy: 0.8654\n",
      "Epoch 25/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1097 - accuracy: 0.8462\n",
      "Epoch 26/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1073 - accuracy: 0.8462\n",
      "Epoch 27/200\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 0.1118 - accuracy: 0.8413\n",
      "Epoch 28/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.1085 - accuracy: 0.8702\n",
      "Epoch 29/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1022 - accuracy: 0.8846\n",
      "Epoch 30/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0992 - accuracy: 0.8846\n",
      "Epoch 31/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.0968 - accuracy: 0.8798\n",
      "Epoch 32/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1024 - accuracy: 0.8654\n",
      "Epoch 33/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0904 - accuracy: 0.8990\n",
      "Epoch 34/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0975 - accuracy: 0.8558\n",
      "Epoch 35/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0942 - accuracy: 0.8654\n",
      "Epoch 36/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0917 - accuracy: 0.8990\n",
      "Epoch 37/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0902 - accuracy: 0.8990\n",
      "Epoch 38/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0917 - accuracy: 0.8846\n",
      "Epoch 39/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.0837 - accuracy: 0.8990\n",
      "Epoch 40/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0822 - accuracy: 0.9087\n",
      "Epoch 41/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0852 - accuracy: 0.8846\n",
      "Epoch 42/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0795 - accuracy: 0.9135\n",
      "Epoch 43/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0821 - accuracy: 0.8990\n",
      "Epoch 44/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0823 - accuracy: 0.9279\n",
      "Epoch 45/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0790 - accuracy: 0.9038\n",
      "Epoch 46/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0739 - accuracy: 0.9327\n",
      "Epoch 47/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0722 - accuracy: 0.9135\n",
      "Epoch 48/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0709 - accuracy: 0.9279\n",
      "Epoch 49/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0715 - accuracy: 0.9183\n",
      "Epoch 50/200\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 0.0720 - accuracy: 0.9231\n",
      "Epoch 51/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0670 - accuracy: 0.9327\n",
      "Epoch 52/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0649 - accuracy: 0.9327\n",
      "Epoch 53/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0705 - accuracy: 0.9183\n",
      "Epoch 54/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0639 - accuracy: 0.9327\n",
      "Epoch 55/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0649 - accuracy: 0.9327\n",
      "Epoch 56/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0627 - accuracy: 0.9423\n",
      "Epoch 57/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0580 - accuracy: 0.9519\n",
      "Epoch 58/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0590 - accuracy: 0.9423\n",
      "Epoch 59/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0588 - accuracy: 0.9327\n",
      "Epoch 60/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0586 - accuracy: 0.9375\n",
      "Epoch 61/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0560 - accuracy: 0.9375\n",
      "Epoch 62/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.0569 - accuracy: 0.9423\n",
      "Epoch 63/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0523 - accuracy: 0.9423\n",
      "Epoch 64/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.0526 - accuracy: 0.9471\n",
      "Epoch 65/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.0484 - accuracy: 0.9615\n",
      "Epoch 66/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0486 - accuracy: 0.9567\n",
      "Epoch 67/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.0523 - accuracy: 0.9471\n",
      "Epoch 68/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0459 - accuracy: 0.9663\n",
      "Epoch 69/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0480 - accuracy: 0.9615\n",
      "Epoch 70/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0453 - accuracy: 0.9567\n",
      "Epoch 71/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0456 - accuracy: 0.9519\n",
      "Epoch 72/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0440 - accuracy: 0.9663\n",
      "Epoch 73/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.0445 - accuracy: 0.9567\n",
      "Epoch 74/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0439 - accuracy: 0.9567\n",
      "Epoch 75/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.0427 - accuracy: 0.9663\n",
      "Epoch 76/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.0458 - accuracy: 0.9519\n",
      "Epoch 77/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0397 - accuracy: 0.9712\n",
      "Epoch 78/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.0390 - accuracy: 0.9712: 0s - loss: 0.0254 - accuracy: 0.\n",
      "Epoch 79/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.0405 - accuracy: 0.9663\n",
      "Epoch 80/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.0360 - accuracy: 0.9663\n",
      "Epoch 81/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0378 - accuracy: 0.9615\n",
      "Epoch 82/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.0396 - accuracy: 0.9567\n",
      "Epoch 83/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.0357 - accuracy: 0.9712\n",
      "Epoch 84/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0356 - accuracy: 0.9663\n",
      "Epoch 85/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0349 - accuracy: 0.9567\n",
      "Epoch 86/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0347 - accuracy: 0.9712\n",
      "Epoch 87/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0320 - accuracy: 0.9760\n",
      "Epoch 88/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0381 - accuracy: 0.9615\n",
      "Epoch 89/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0312 - accuracy: 0.9760\n",
      "Epoch 90/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0332 - accuracy: 0.9760\n",
      "Epoch 91/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0312 - accuracy: 0.9760\n",
      "Epoch 92/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0329 - accuracy: 0.9663\n",
      "Epoch 93/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0300 - accuracy: 0.9808\n",
      "Epoch 94/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0285 - accuracy: 0.9808\n",
      "Epoch 95/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0292 - accuracy: 0.9712\n",
      "Epoch 96/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0284 - accuracy: 0.9808: 0s - loss: 0.0284 - accuracy: 0.98\n",
      "Epoch 97/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0322 - accuracy: 0.9663\n",
      "Epoch 98/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0301 - accuracy: 0.9760\n",
      "Epoch 99/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.0292 - accuracy: 0.9856\n",
      "Epoch 100/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.0257 - accuracy: 0.9808\n",
      "Epoch 101/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.0250 - accuracy: 0.9760\n",
      "Epoch 102/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0248 - accuracy: 0.9808\n",
      "Epoch 103/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0265 - accuracy: 0.9760\n",
      "Epoch 104/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0298 - accuracy: 0.9760\n",
      "Epoch 105/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0242 - accuracy: 0.9808\n",
      "Epoch 106/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0271 - accuracy: 0.9808\n",
      "Epoch 107/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0236 - accuracy: 0.9808\n",
      "Epoch 108/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0257 - accuracy: 0.9760\n",
      "Epoch 109/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0252 - accuracy: 0.9663\n",
      "Epoch 110/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0230 - accuracy: 0.9760\n",
      "Epoch 111/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0206 - accuracy: 0.9856\n",
      "Epoch 112/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0204 - accuracy: 0.9808\n",
      "Epoch 113/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0199 - accuracy: 0.9856\n",
      "Epoch 114/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0204 - accuracy: 0.9808\n",
      "Epoch 115/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0188 - accuracy: 0.9856\n",
      "Epoch 116/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0204 - accuracy: 0.9856\n",
      "Epoch 117/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0264 - accuracy: 0.9712\n",
      "Epoch 118/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0184 - accuracy: 0.9904\n",
      "Epoch 119/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0186 - accuracy: 0.9856\n",
      "Epoch 120/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0172 - accuracy: 0.9856\n",
      "Epoch 121/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0162 - accuracy: 0.9904\n",
      "Epoch 122/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0153 - accuracy: 0.9856\n",
      "Epoch 123/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0168 - accuracy: 0.9904\n",
      "Epoch 124/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0198 - accuracy: 0.9856\n",
      "Epoch 125/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0144 - accuracy: 0.9904\n",
      "Epoch 126/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0157 - accuracy: 0.9952\n",
      "Epoch 127/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0147 - accuracy: 0.9904\n",
      "Epoch 128/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0145 - accuracy: 0.9856\n",
      "Epoch 129/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0132 - accuracy: 0.9904\n",
      "Epoch 130/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0136 - accuracy: 0.9904\n",
      "Epoch 131/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0130 - accuracy: 0.9904\n",
      "Epoch 132/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0128 - accuracy: 0.9904\n",
      "Epoch 133/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0147 - accuracy: 0.9808\n",
      "Epoch 134/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0143 - accuracy: 0.9904\n",
      "Epoch 135/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0126 - accuracy: 0.9904\n",
      "Epoch 136/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0113 - accuracy: 0.9952\n",
      "Epoch 137/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0106 - accuracy: 0.9952\n",
      "Epoch 138/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0109 - accuracy: 0.9952\n",
      "Epoch 139/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0111 - accuracy: 0.9952\n",
      "Epoch 140/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0116 - accuracy: 0.9952\n",
      "Epoch 141/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.0117 - accuracy: 0.9904\n",
      "Epoch 142/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0113 - accuracy: 0.9904\n",
      "Epoch 143/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0104 - accuracy: 0.9952\n",
      "Epoch 144/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0113 - accuracy: 0.9952\n",
      "Epoch 145/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0152 - accuracy: 0.9856\n",
      "Epoch 146/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0116 - accuracy: 0.9952\n",
      "Epoch 147/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0087 - accuracy: 0.9952\n",
      "Epoch 148/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0098 - accuracy: 0.9952\n",
      "Epoch 149/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0088 - accuracy: 0.9952\n",
      "Epoch 150/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0090 - accuracy: 0.9952\n",
      "Epoch 151/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0085 - accuracy: 0.9952\n",
      "Epoch 152/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0089 - accuracy: 0.9952\n",
      "Epoch 153/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0083 - accuracy: 0.9952\n",
      "Epoch 154/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0080 - accuracy: 0.9952\n",
      "Epoch 155/200\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 0.0078 - accuracy: 0.9952\n",
      "Epoch 156/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.0079 - accuracy: 0.9952\n",
      "Epoch 157/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0082 - accuracy: 0.9952\n",
      "Epoch 158/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0086 - accuracy: 0.9952\n",
      "Epoch 159/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0077 - accuracy: 0.9952\n",
      "Epoch 160/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.0074 - accuracy: 0.9952\n",
      "Epoch 161/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.0073 - accuracy: 0.9952\n",
      "Epoch 162/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0071 - accuracy: 0.9952\n",
      "Epoch 163/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0099 - accuracy: 0.9952\n",
      "Epoch 164/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0088 - accuracy: 0.9952\n",
      "Epoch 165/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0073 - accuracy: 0.9952\n",
      "Epoch 166/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0074 - accuracy: 0.9952\n",
      "Epoch 167/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0070 - accuracy: 0.9952\n",
      "Epoch 168/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0078 - accuracy: 0.9952\n",
      "Epoch 169/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0068 - accuracy: 0.9952\n",
      "Epoch 170/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0064 - accuracy: 0.9952\n",
      "Epoch 171/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0064 - accuracy: 0.9952\n",
      "Epoch 172/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0063 - accuracy: 0.9952\n",
      "Epoch 173/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0063 - accuracy: 0.9952\n",
      "Epoch 174/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0062 - accuracy: 0.9952\n",
      "Epoch 175/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0067 - accuracy: 0.9952\n",
      "Epoch 176/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0069 - accuracy: 0.9952\n",
      "Epoch 177/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0064 - accuracy: 0.9952\n",
      "Epoch 178/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0062 - accuracy: 0.9952\n",
      "Epoch 179/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0062 - accuracy: 0.9952\n",
      "Epoch 180/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0062 - accuracy: 0.9952\n",
      "Epoch 181/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0061 - accuracy: 0.9952\n",
      "Epoch 182/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0059 - accuracy: 0.9952\n",
      "Epoch 183/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.0060 - accuracy: 0.9952\n",
      "Epoch 184/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0060 - accuracy: 0.9952\n",
      "Epoch 185/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0059 - accuracy: 0.9952\n",
      "Epoch 186/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.0059 - accuracy: 0.9952\n",
      "Epoch 187/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0060 - accuracy: 0.9952\n",
      "Epoch 188/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0057 - accuracy: 0.9952\n",
      "Epoch 189/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0058 - accuracy: 0.9952\n",
      "Epoch 190/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0058 - accuracy: 0.9952\n",
      "Epoch 191/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0056 - accuracy: 0.9952\n",
      "Epoch 192/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0057 - accuracy: 0.9952\n",
      "Epoch 193/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0062 - accuracy: 0.9952\n",
      "Epoch 194/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0059 - accuracy: 0.9952\n",
      "Epoch 195/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0057 - accuracy: 0.9952: 0s - loss: 6.9360e-04 - accuracy: 1.\n",
      "Epoch 196/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0056 - accuracy: 0.9952\n",
      "Epoch 197/200\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0055 - accuracy: 0.9952\n",
      "Epoch 198/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0056 - accuracy: 0.9952\n",
      "Epoch 199/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0054 - accuracy: 0.9952\n",
      "Epoch 200/200\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.0054 - accuracy: 0.9952\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0054 - accuracy: 0.9952\n",
      "\n",
      " Accuracy: 0.9952\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 모델 실행\n",
    "model.fit(X, Y, epochs=200, batch_size=5)\n",
    "# 결과 출력\n",
    "print(\"\\n Accuracy: %.4f\" % (model.evaluate(X, Y)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100%가 나왔다 --> 과적합(overfit)이 일어났다 --> 딥러닝을 진행시에 데이터가 추가될 수록 loss가 줄어들어야 하는데, 그렇지 않고 새로운 데이터에 적용될 때 lose가 증가하는 경우\n",
    "--> layer(층)가 너무 많거나, 변수가 복잡해서 발생하거나 테스트셋과 학습셋이 중복될 때 생기기도 한다.\n",
    "--> 학습하는 데이터셋과 이를 테스트를 할 데이터셋을 완전히 구분하고, 학습과 동시에 테스트를 병행해야 한다.\n",
    "--> 일반적으로 데이터는 학습 : 테스트 = 7:3 비율로 설정.\n",
    "--> 모델 저장 ; layer, 노드, 가중치 파라미터, bias 파라미터, hyper 파라미터 정보를 저장.\n",
    "--> hyper 파라미터 ; optimizer='adam', 등 직접적으로 세팅해주는 여러 변수들\n",
    "# 학습이 부족한 경우는 underfit(부족학 학습)이라고 한다\n",
    "# 우리가 실험 시, 약간의 오차가 일어나더라도, 신규 데이터에도 적용될 수 있는 머신 러닝 모듈을 원하고,\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 과적합 피하기 \n",
    "  - 테스트와 학습 셋을 나누기 3:7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/130\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.2528 - accuracy: 0.4414\n",
      "Epoch 2/130\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.2420 - accuracy: 0.6828\n",
      "Epoch 3/130\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.2311 - accuracy: 0.7172\n",
      "Epoch 4/130\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 0.2221 - accuracy: 0.7655\n",
      "Epoch 5/130\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.2129 - accuracy: 0.7448\n",
      "Epoch 6/130\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.2055 - accuracy: 0.7103\n",
      "Epoch 7/130\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1879 - accuracy: 0.7379\n",
      "Epoch 8/130\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.1828 - accuracy: 0.7379\n",
      "Epoch 9/130\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1770 - accuracy: 0.7655\n",
      "Epoch 10/130\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1684 - accuracy: 0.7793\n",
      "Epoch 11/130\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1600 - accuracy: 0.7793\n",
      "Epoch 12/130\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1561 - accuracy: 0.7931\n",
      "Epoch 13/130\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 0.1523 - accuracy: 0.8000\n",
      "Epoch 14/130\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1492 - accuracy: 0.7931\n",
      "Epoch 15/130\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.1442 - accuracy: 0.8207\n",
      "Epoch 16/130\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 0.1423 - accuracy: 0.8414\n",
      "Epoch 17/130\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.1371 - accuracy: 0.8345\n",
      "Epoch 18/130\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1341 - accuracy: 0.8276\n",
      "Epoch 19/130\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.1312 - accuracy: 0.8552\n",
      "Epoch 20/130\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.1273 - accuracy: 0.8276\n",
      "Epoch 21/130\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.1261 - accuracy: 0.8414\n",
      "Epoch 22/130\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.1263 - accuracy: 0.8276\n",
      "Epoch 23/130\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 0.1354 - accuracy: 0.7931\n",
      "Epoch 24/130\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 0.1208 - accuracy: 0.8552\n",
      "Epoch 25/130\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 0.1181 - accuracy: 0.8759\n",
      "Epoch 26/130\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 0.1154 - accuracy: 0.8621\n",
      "Epoch 27/130\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.1128 - accuracy: 0.8552\n",
      "Epoch 28/130\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.1192 - accuracy: 0.8552\n",
      "Epoch 29/130\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.1102 - accuracy: 0.8966\n",
      "Epoch 30/130\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1105 - accuracy: 0.8621\n",
      "Epoch 31/130\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.1074 - accuracy: 0.8828\n",
      "Epoch 32/130\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 0.1058 - accuracy: 0.8759\n",
      "Epoch 33/130\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1037 - accuracy: 0.8828\n",
      "Epoch 34/130\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 0.1059 - accuracy: 0.8759\n",
      "Epoch 35/130\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0995 - accuracy: 0.8759\n",
      "Epoch 36/130\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.1028 - accuracy: 0.8828\n",
      "Epoch 37/130\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.1006 - accuracy: 0.8828\n",
      "Epoch 38/130\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.1018 - accuracy: 0.8828\n",
      "Epoch 39/130\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.0962 - accuracy: 0.8828\n",
      "Epoch 40/130\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0942 - accuracy: 0.8897\n",
      "Epoch 41/130\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 0.0970 - accuracy: 0.8621\n",
      "Epoch 42/130\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0912 - accuracy: 0.9034\n",
      "Epoch 43/130\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0915 - accuracy: 0.8966\n",
      "Epoch 44/130\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0878 - accuracy: 0.8897\n",
      "Epoch 45/130\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0920 - accuracy: 0.8828\n",
      "Epoch 46/130\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0861 - accuracy: 0.9034\n",
      "Epoch 47/130\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0838 - accuracy: 0.9103\n",
      "Epoch 48/130\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0871 - accuracy: 0.8966\n",
      "Epoch 49/130\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.0808 - accuracy: 0.9034\n",
      "Epoch 50/130\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 0.0825 - accuracy: 0.8897\n",
      "Epoch 51/130\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0870 - accuracy: 0.9103\n",
      "Epoch 52/130\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0811 - accuracy: 0.9103\n",
      "Epoch 53/130\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 0.0818 - accuracy: 0.9034\n",
      "Epoch 54/130\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0772 - accuracy: 0.8966\n",
      "Epoch 55/130\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 0.0727 - accuracy: 0.9310\n",
      "Epoch 56/130\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 0.0729 - accuracy: 0.9172\n",
      "Epoch 57/130\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0727 - accuracy: 0.9103\n",
      "Epoch 58/130\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0695 - accuracy: 0.9241\n",
      "Epoch 59/130\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0692 - accuracy: 0.9241\n",
      "Epoch 60/130\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.0686 - accuracy: 0.9241\n",
      "Epoch 61/130\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0703 - accuracy: 0.9172\n",
      "Epoch 62/130\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0701 - accuracy: 0.9448\n",
      "Epoch 63/130\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0685 - accuracy: 0.9172\n",
      "Epoch 64/130\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0620 - accuracy: 0.9310\n",
      "Epoch 65/130\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0626 - accuracy: 0.9310\n",
      "Epoch 66/130\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0604 - accuracy: 0.9379\n",
      "Epoch 67/130\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0594 - accuracy: 0.9379\n",
      "Epoch 68/130\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0569 - accuracy: 0.9379\n",
      "Epoch 69/130\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0557 - accuracy: 0.9379\n",
      "Epoch 70/130\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0525 - accuracy: 0.9448\n",
      "Epoch 71/130\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0544 - accuracy: 0.9379\n",
      "Epoch 72/130\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0517 - accuracy: 0.9379\n",
      "Epoch 73/130\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0472 - accuracy: 0.9724\n",
      "Epoch 74/130\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0508 - accuracy: 0.9310\n",
      "Epoch 75/130\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0483 - accuracy: 0.9586\n",
      "Epoch 76/130\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0472 - accuracy: 0.9517\n",
      "Epoch 77/130\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0567 - accuracy: 0.9379\n",
      "Epoch 78/130\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0451 - accuracy: 0.9655\n",
      "Epoch 79/130\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 0.0488 - accuracy: 0.9586\n",
      "Epoch 80/130\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0492 - accuracy: 0.9655\n",
      "Epoch 81/130\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0418 - accuracy: 0.9655\n",
      "Epoch 82/130\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0395 - accuracy: 0.9655\n",
      "Epoch 83/130\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0430 - accuracy: 0.9655\n",
      "Epoch 84/130\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0370 - accuracy: 0.9793\n",
      "Epoch 85/130\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0347 - accuracy: 0.9724\n",
      "Epoch 86/130\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0348 - accuracy: 0.9793\n",
      "Epoch 87/130\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0330 - accuracy: 0.9862\n",
      "Epoch 88/130\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0313 - accuracy: 0.9862\n",
      "Epoch 89/130\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0320 - accuracy: 0.9862\n",
      "Epoch 90/130\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0298 - accuracy: 0.9862\n",
      "Epoch 91/130\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0280 - accuracy: 0.9862\n",
      "Epoch 92/130\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0292 - accuracy: 0.9931\n",
      "Epoch 93/130\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0257 - accuracy: 0.9931\n",
      "Epoch 94/130\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0281 - accuracy: 0.9793\n",
      "Epoch 95/130\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0249 - accuracy: 0.9931\n",
      "Epoch 96/130\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0275 - accuracy: 0.9862\n",
      "Epoch 97/130\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0206 - accuracy: 0.9931\n",
      "Epoch 98/130\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0262 - accuracy: 0.9862\n",
      "Epoch 99/130\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0231 - accuracy: 0.9862\n",
      "Epoch 100/130\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0261 - accuracy: 0.9931\n",
      "Epoch 101/130\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0194 - accuracy: 0.9862\n",
      "Epoch 102/130\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0193 - accuracy: 0.9862\n",
      "Epoch 103/130\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0199 - accuracy: 0.9862\n",
      "Epoch 104/130\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0183 - accuracy: 0.9862\n",
      "Epoch 105/130\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0163 - accuracy: 0.9931\n",
      "Epoch 106/130\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0174 - accuracy: 0.9931\n",
      "Epoch 107/130\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 0.0178 - accuracy: 0.9931\n",
      "Epoch 108/130\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0143 - accuracy: 0.9931\n",
      "Epoch 109/130\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0141 - accuracy: 1.0000\n",
      "Epoch 110/130\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0165 - accuracy: 1.0000\n",
      "Epoch 111/130\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0140 - accuracy: 1.0000\n",
      "Epoch 112/130\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0157 - accuracy: 1.0000\n",
      "Epoch 113/130\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0189 - accuracy: 0.9862\n",
      "Epoch 114/130\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0162 - accuracy: 0.9862\n",
      "Epoch 115/130\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0122 - accuracy: 1.0000\n",
      "Epoch 116/130\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0107 - accuracy: 1.0000\n",
      "Epoch 117/130\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0104 - accuracy: 1.0000\n",
      "Epoch 118/130\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0171 - accuracy: 1.0000\n",
      "Epoch 119/130\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0111 - accuracy: 1.0000\n",
      "Epoch 120/130\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0095 - accuracy: 1.0000\n",
      "Epoch 121/130\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0143 - accuracy: 0.9931\n",
      "Epoch 122/130\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0107 - accuracy: 1.0000\n",
      "Epoch 123/130\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0083 - accuracy: 1.0000\n",
      "Epoch 124/130\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0087 - accuracy: 1.0000\n",
      "Epoch 125/130\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0115 - accuracy: 1.0000\n",
      "Epoch 126/130\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0124 - accuracy: 0.9862\n",
      "Epoch 127/130\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0108 - accuracy: 1.0000\n",
      "Epoch 128/130\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0066 - accuracy: 1.0000\n",
      "Epoch 129/130\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0075 - accuracy: 1.0000\n",
      "Epoch 130/130\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0062 - accuracy: 1.0000\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.1528 - accuracy: 0.7937\n",
      "\n",
      " Test Accuracy: 0.7937\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split # 테스트 set, 학습 set 비율 나누기\n",
    "from tensorflow.keras.models import load_model # 모델 저장하기\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy\n",
    "\n",
    "#seed (고정 값 제거하기)\n",
    "#데이터 적용\n",
    "df = pd.read_csv('example/080228-master/deeplearning/dataset/sonar.csv', header=None)\n",
    "\n",
    "dataset = df.values\n",
    "X = dataset[:,0:60].astype(float)\n",
    "Y_obj = dataset[:,60]\n",
    "\n",
    "# 문자열 변환\n",
    "e = LabelEncoder()\n",
    "e.fit(Y_obj)\n",
    "Y = e.transform(Y_obj)\n",
    "\n",
    "# 학습 셋과 테스트 셋의 구분\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(24,  input_dim=60, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='mean_squared_error',\n",
    "            optimizer='adam',\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, Y_train, epochs=130, batch_size=5)\n",
    "model.save('my_model.h5')#모델을 컴퓨터에 저장\n",
    "\n",
    "\n",
    "\n",
    "del model # 현재 model이라는 object를 제거하여, 이후 불러오는 모델이 적용이 되는가 확인하기.\n",
    "model = load_model('my_model.h5') #모델을 세로 불러움\n",
    "# 테스트셋에 모델 적용\n",
    "print(\"\\n Test Accuracy: %.4f\" % (model.evaluate(X_test, Y_test)[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k겹 교차 검증\n",
    "  - 테스트 할 데이터 값이 충분하지 않을 때 \n",
    "  - 데이터셋을 여러 개로 나누어 하나씩 테스트셋으로 사용하고 나머지를 모두 합해서 학습셋으로 사용하는 방법\n",
    "  - sklearn의 StratifiedKFold() 함수를 사용"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('example/080228-master/deeplearning/dataset/sonar.csv', header=None)\n",
    "\n",
    "dataset = df.values\n",
    "X = dataset[:, 0:60].astype(float)\n",
    "Y_obj = dataset[:, 60]\n",
    "\n",
    "e = LabelEncoder()\n",
    "e.fit(Y_obj)\n",
    "Y = e.transform(Y_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#10개의 파일로 쪼갬\n",
    "n_fold = 10\n",
    "skf = StratifiedKFold(n_splits=n_fold, shuffle=True) #10개를 쪼개서 섞음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2579 - accuracy: 0.5348\n",
      "Epoch 2/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2467 - accuracy: 0.5348\n",
      "Epoch 3/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.2408 - accuracy: 0.5348\n",
      "Epoch 4/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2328 - accuracy: 0.5936\n",
      "Epoch 5/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.2256 - accuracy: 0.6578\n",
      "Epoch 6/100\n",
      "38/38 [==============================] - ETA: 0s - loss: 0.2207 - accuracy: 0.68 - 0s 5ms/step - loss: 0.2199 - accuracy: 0.6845\n",
      "Epoch 7/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.2137 - accuracy: 0.6845: 0s - loss: 0.2127 - accuracy: \n",
      "Epoch 8/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2066 - accuracy: 0.6898\n",
      "Epoch 9/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.1984 - accuracy: 0.6952\n",
      "Epoch 10/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1929 - accuracy: 0.7701\n",
      "Epoch 11/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1875 - accuracy: 0.7273\n",
      "Epoch 12/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.1759 - accuracy: 0.7540\n",
      "Epoch 13/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1741 - accuracy: 0.7914\n",
      "Epoch 14/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1625 - accuracy: 0.8182\n",
      "Epoch 15/100\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.1602 - accuracy: 0.7701\n",
      "Epoch 16/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1548 - accuracy: 0.7861\n",
      "Epoch 17/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1508 - accuracy: 0.8182\n",
      "Epoch 18/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1462 - accuracy: 0.8075\n",
      "Epoch 19/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.1401 - accuracy: 0.8342\n",
      "Epoch 20/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1415 - accuracy: 0.8235\n",
      "Epoch 21/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1389 - accuracy: 0.7968\n",
      "Epoch 22/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.1470 - accuracy: 0.7701\n",
      "Epoch 23/100\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 0.1278 - accuracy: 0.8610\n",
      "Epoch 24/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1292 - accuracy: 0.8342\n",
      "Epoch 25/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.1195 - accuracy: 0.8717\n",
      "Epoch 26/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1231 - accuracy: 0.8556\n",
      "Epoch 27/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1199 - accuracy: 0.8556\n",
      "Epoch 28/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1150 - accuracy: 0.8503\n",
      "Epoch 29/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1114 - accuracy: 0.8770\n",
      "Epoch 30/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.1102 - accuracy: 0.8663\n",
      "Epoch 31/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.1111 - accuracy: 0.8877\n",
      "Epoch 32/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1115 - accuracy: 0.8770\n",
      "Epoch 33/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1061 - accuracy: 0.8770\n",
      "Epoch 34/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1056 - accuracy: 0.8877\n",
      "Epoch 35/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1043 - accuracy: 0.8717\n",
      "Epoch 36/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1022 - accuracy: 0.8717\n",
      "Epoch 37/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.0992 - accuracy: 0.8877\n",
      "Epoch 38/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.1004 - accuracy: 0.8824\n",
      "Epoch 39/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.0961 - accuracy: 0.8930\n",
      "Epoch 40/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.0929 - accuracy: 0.9037\n",
      "Epoch 41/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.0935 - accuracy: 0.8930\n",
      "Epoch 42/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.0909 - accuracy: 0.8984\n",
      "Epoch 43/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.0880 - accuracy: 0.9037\n",
      "Epoch 44/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.0875 - accuracy: 0.9091\n",
      "Epoch 45/100\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.0849 - accuracy: 0.8984\n",
      "Epoch 46/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.0877 - accuracy: 0.8930\n",
      "Epoch 47/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.0850 - accuracy: 0.8877\n",
      "Epoch 48/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.0834 - accuracy: 0.8984\n",
      "Epoch 49/100\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 0.0832 - accuracy: 0.9198\n",
      "Epoch 50/100\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.0831 - accuracy: 0.9037\n",
      "Epoch 51/100\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 0.0791 - accuracy: 0.9091\n",
      "Epoch 52/100\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.0778 - accuracy: 0.9144\n",
      "Epoch 53/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.0765 - accuracy: 0.9198\n",
      "Epoch 54/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.0741 - accuracy: 0.9144\n",
      "Epoch 55/100\n",
      "38/38 [==============================] - 0s 9ms/step - loss: 0.0754 - accuracy: 0.9198\n",
      "Epoch 56/100\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.0719 - accuracy: 0.9198\n",
      "Epoch 57/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0705 - accuracy: 0.9198\n",
      "Epoch 58/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0728 - accuracy: 0.9144\n",
      "Epoch 59/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.0715 - accuracy: 0.9251\n",
      "Epoch 60/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.0681 - accuracy: 0.9198\n",
      "Epoch 61/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0677 - accuracy: 0.9251\n",
      "Epoch 62/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0680 - accuracy: 0.9305\n",
      "Epoch 63/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0683 - accuracy: 0.9198\n",
      "Epoch 64/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0655 - accuracy: 0.9412\n",
      "Epoch 65/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0669 - accuracy: 0.9412\n",
      "Epoch 66/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0620 - accuracy: 0.9358\n",
      "Epoch 67/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0635 - accuracy: 0.9358\n",
      "Epoch 68/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0618 - accuracy: 0.9305\n",
      "Epoch 69/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0600 - accuracy: 0.9519\n",
      "Epoch 70/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0599 - accuracy: 0.9305\n",
      "Epoch 71/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0624 - accuracy: 0.9251\n",
      "Epoch 72/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0572 - accuracy: 0.9465\n",
      "Epoch 73/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0568 - accuracy: 0.9412\n",
      "Epoch 74/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0548 - accuracy: 0.9572\n",
      "Epoch 75/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.0571 - accuracy: 0.9519\n",
      "Epoch 76/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0559 - accuracy: 0.9572\n",
      "Epoch 77/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0553 - accuracy: 0.9412\n",
      "Epoch 78/100\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.0618 - accuracy: 0.9358\n",
      "Epoch 79/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.0531 - accuracy: 0.9519\n",
      "Epoch 80/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.0548 - accuracy: 0.9358\n",
      "Epoch 81/100\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.0560 - accuracy: 0.9412\n",
      "Epoch 82/100\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 0.0503 - accuracy: 0.9519\n",
      "Epoch 83/100\n",
      "38/38 [==============================] - 1s 14ms/step - loss: 0.0469 - accuracy: 0.9572\n",
      "Epoch 84/100\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 0.0460 - accuracy: 0.9626\n",
      "Epoch 85/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.0561 - accuracy: 0.9412\n",
      "Epoch 86/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0475 - accuracy: 0.9519\n",
      "Epoch 87/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0496 - accuracy: 0.9572\n",
      "Epoch 88/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0464 - accuracy: 0.9412\n",
      "Epoch 89/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0425 - accuracy: 0.9626\n",
      "Epoch 90/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0414 - accuracy: 0.9626\n",
      "Epoch 91/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0402 - accuracy: 0.9626\n",
      "Epoch 92/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.0437 - accuracy: 0.9465\n",
      "Epoch 93/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0389 - accuracy: 0.9626\n",
      "Epoch 94/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0400 - accuracy: 0.9626\n",
      "Epoch 95/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0394 - accuracy: 0.9626\n",
      "Epoch 96/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0385 - accuracy: 0.9572\n",
      "Epoch 97/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0355 - accuracy: 0.9626\n",
      "Epoch 98/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0357 - accuracy: 0.9626\n",
      "Epoch 99/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0379 - accuracy: 0.9679\n",
      "Epoch 100/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.0352 - accuracy: 0.9626\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1234 - accuracy: 0.8095\n",
      "Epoch 1/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2381 - accuracy: 0.5882\n",
      "Epoch 2/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2266 - accuracy: 0.6578: 0s - loss: 0.2214 - accuracy: 0.68\n",
      "Epoch 3/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2204 - accuracy: 0.6738\n",
      "Epoch 4/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.2141 - accuracy: 0.7059\n",
      "Epoch 5/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.2030 - accuracy: 0.7166\n",
      "Epoch 6/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1918 - accuracy: 0.7219\n",
      "Epoch 7/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1839 - accuracy: 0.7540\n",
      "Epoch 8/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1726 - accuracy: 0.8182\n",
      "Epoch 9/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1660 - accuracy: 0.7647\n",
      "Epoch 10/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1570 - accuracy: 0.8289\n",
      "Epoch 11/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.1572 - accuracy: 0.7594\n",
      "Epoch 12/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1479 - accuracy: 0.8075\n",
      "Epoch 13/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.1526 - accuracy: 0.8021\n",
      "Epoch 14/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.1403 - accuracy: 0.8235\n",
      "Epoch 15/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1380 - accuracy: 0.8182\n",
      "Epoch 16/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1350 - accuracy: 0.8128\n",
      "Epoch 17/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1326 - accuracy: 0.8610\n",
      "Epoch 18/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.1312 - accuracy: 0.8182\n",
      "Epoch 19/100\n",
      "38/38 [==============================] - 0s 9ms/step - loss: 0.1264 - accuracy: 0.8503\n",
      "Epoch 20/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1270 - accuracy: 0.8289\n",
      "Epoch 21/100\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 0.1212 - accuracy: 0.8342\n",
      "Epoch 22/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1356 - accuracy: 0.7861\n",
      "Epoch 23/100\n",
      "38/38 [==============================] - ETA: 0s - loss: 0.1213 - accuracy: 0.84 - 0s 4ms/step - loss: 0.1147 - accuracy: 0.8610\n",
      "Epoch 24/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1252 - accuracy: 0.8289\n",
      "Epoch 25/100\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.1130 - accuracy: 0.8610\n",
      "Epoch 26/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.1105 - accuracy: 0.8396\n",
      "Epoch 27/100\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 0.1101 - accuracy: 0.8449\n",
      "Epoch 28/100\n",
      "38/38 [==============================] - ETA: 0s - loss: 0.1091 - accuracy: 0.87 - 0s 6ms/step - loss: 0.1091 - accuracy: 0.8717\n",
      "Epoch 29/100\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.1049 - accuracy: 0.8770\n",
      "Epoch 30/100\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 0.1024 - accuracy: 0.8770\n",
      "Epoch 31/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.0999 - accuracy: 0.8824\n",
      "Epoch 32/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.0982 - accuracy: 0.8770\n",
      "Epoch 33/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1012 - accuracy: 0.8717\n",
      "Epoch 34/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0970 - accuracy: 0.8877\n",
      "Epoch 35/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.0973 - accuracy: 0.8610\n",
      "Epoch 36/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.0974 - accuracy: 0.8824\n",
      "Epoch 37/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0931 - accuracy: 0.8770\n",
      "Epoch 38/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0937 - accuracy: 0.8984\n",
      "Epoch 39/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0906 - accuracy: 0.8930\n",
      "Epoch 40/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.0907 - accuracy: 0.8984\n",
      "Epoch 41/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0946 - accuracy: 0.8824\n",
      "Epoch 42/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0908 - accuracy: 0.8770\n",
      "Epoch 43/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0834 - accuracy: 0.9198\n",
      "Epoch 44/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0808 - accuracy: 0.9144\n",
      "Epoch 45/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0828 - accuracy: 0.8877\n",
      "Epoch 46/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0799 - accuracy: 0.9091\n",
      "Epoch 47/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0844 - accuracy: 0.8770\n",
      "Epoch 48/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0798 - accuracy: 0.8930\n",
      "Epoch 49/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0757 - accuracy: 0.9144\n",
      "Epoch 50/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0792 - accuracy: 0.8984\n",
      "Epoch 51/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.0725 - accuracy: 0.9358\n",
      "Epoch 52/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0774 - accuracy: 0.8984\n",
      "Epoch 53/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0729 - accuracy: 0.9251\n",
      "Epoch 54/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0716 - accuracy: 0.9251\n",
      "Epoch 55/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0743 - accuracy: 0.9198\n",
      "Epoch 56/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0746 - accuracy: 0.9198\n",
      "Epoch 57/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0708 - accuracy: 0.9144\n",
      "Epoch 58/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0680 - accuracy: 0.9305\n",
      "Epoch 59/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0674 - accuracy: 0.9305\n",
      "Epoch 60/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0651 - accuracy: 0.9465\n",
      "Epoch 61/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0661 - accuracy: 0.9251\n",
      "Epoch 62/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0706 - accuracy: 0.9144\n",
      "Epoch 63/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0673 - accuracy: 0.9144\n",
      "Epoch 64/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0714 - accuracy: 0.8984\n",
      "Epoch 65/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0672 - accuracy: 0.9144\n",
      "Epoch 66/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0620 - accuracy: 0.9358\n",
      "Epoch 67/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0655 - accuracy: 0.9305\n",
      "Epoch 68/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0624 - accuracy: 0.9358\n",
      "Epoch 69/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0590 - accuracy: 0.9412\n",
      "Epoch 70/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0561 - accuracy: 0.9412\n",
      "Epoch 71/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0582 - accuracy: 0.9465\n",
      "Epoch 72/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0578 - accuracy: 0.9412\n",
      "Epoch 73/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0535 - accuracy: 0.9358\n",
      "Epoch 74/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0523 - accuracy: 0.9465\n",
      "Epoch 75/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0617 - accuracy: 0.9251\n",
      "Epoch 76/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0601 - accuracy: 0.9305\n",
      "Epoch 77/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0545 - accuracy: 0.9251\n",
      "Epoch 78/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0525 - accuracy: 0.9465\n",
      "Epoch 79/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0503 - accuracy: 0.9465\n",
      "Epoch 80/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0488 - accuracy: 0.9465\n",
      "Epoch 81/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0512 - accuracy: 0.9412\n",
      "Epoch 82/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0438 - accuracy: 0.9572\n",
      "Epoch 83/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0463 - accuracy: 0.9465\n",
      "Epoch 84/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0421 - accuracy: 0.9572\n",
      "Epoch 85/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0616 - accuracy: 0.9091\n",
      "Epoch 86/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0438 - accuracy: 0.9465\n",
      "Epoch 87/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.0471 - accuracy: 0.9626\n",
      "Epoch 88/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.0447 - accuracy: 0.9572\n",
      "Epoch 89/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.0399 - accuracy: 0.9572\n",
      "Epoch 90/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0395 - accuracy: 0.9733\n",
      "Epoch 91/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0375 - accuracy: 0.9519\n",
      "Epoch 92/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0357 - accuracy: 0.9679\n",
      "Epoch 93/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.0381 - accuracy: 0.9572\n",
      "Epoch 94/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0344 - accuracy: 0.9626\n",
      "Epoch 95/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0333 - accuracy: 0.9679\n",
      "Epoch 96/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0311 - accuracy: 0.9572\n",
      "Epoch 97/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0346 - accuracy: 0.9626\n",
      "Epoch 98/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.0329 - accuracy: 0.9679\n",
      "Epoch 99/100\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 0.0294 - accuracy: 0.9679\n",
      "Epoch 100/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.0321 - accuracy: 0.9733\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1327 - accuracy: 0.8571\n",
      "Epoch 1/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.2455 - accuracy: 0.5561\n",
      "Epoch 2/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2401 - accuracy: 0.5561: 0s - loss: 0.2376 - accuracy: 0.\n",
      "Epoch 3/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2348 - accuracy: 0.5882\n",
      "Epoch 4/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2276 - accuracy: 0.6684\n",
      "Epoch 5/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2192 - accuracy: 0.7112\n",
      "Epoch 6/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2114 - accuracy: 0.7326\n",
      "Epoch 7/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2038 - accuracy: 0.7433\n",
      "Epoch 8/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1949 - accuracy: 0.7914\n",
      "Epoch 9/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1835 - accuracy: 0.7647\n",
      "Epoch 10/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1763 - accuracy: 0.7540\n",
      "Epoch 11/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1736 - accuracy: 0.7487\n",
      "Epoch 12/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.1631 - accuracy: 0.8128\n",
      "Epoch 13/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1618 - accuracy: 0.7701\n",
      "Epoch 14/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1526 - accuracy: 0.8128\n",
      "Epoch 15/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1496 - accuracy: 0.7914\n",
      "Epoch 16/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1462 - accuracy: 0.8128\n",
      "Epoch 17/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1459 - accuracy: 0.7807\n",
      "Epoch 18/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1416 - accuracy: 0.8075\n",
      "Epoch 19/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1372 - accuracy: 0.8128\n",
      "Epoch 20/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1382 - accuracy: 0.8075\n",
      "Epoch 21/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1380 - accuracy: 0.8235\n",
      "Epoch 22/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1445 - accuracy: 0.7914\n",
      "Epoch 23/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1289 - accuracy: 0.8235\n",
      "Epoch 24/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1385 - accuracy: 0.7914\n",
      "Epoch 25/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1281 - accuracy: 0.8021\n",
      "Epoch 26/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1289 - accuracy: 0.8235\n",
      "Epoch 27/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1241 - accuracy: 0.8289\n",
      "Epoch 28/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1231 - accuracy: 0.8235\n",
      "Epoch 29/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1217 - accuracy: 0.8235\n",
      "Epoch 30/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1198 - accuracy: 0.8449\n",
      "Epoch 31/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1176 - accuracy: 0.8396\n",
      "Epoch 32/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1188 - accuracy: 0.8556\n",
      "Epoch 33/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1172 - accuracy: 0.8235\n",
      "Epoch 34/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1162 - accuracy: 0.8503\n",
      "Epoch 35/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1171 - accuracy: 0.8449\n",
      "Epoch 36/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1128 - accuracy: 0.8610\n",
      "Epoch 37/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1119 - accuracy: 0.8610\n",
      "Epoch 38/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1158 - accuracy: 0.8342\n",
      "Epoch 39/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1135 - accuracy: 0.8556\n",
      "Epoch 40/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1174 - accuracy: 0.8396\n",
      "Epoch 41/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1139 - accuracy: 0.8503\n",
      "Epoch 42/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1069 - accuracy: 0.8610\n",
      "Epoch 43/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1088 - accuracy: 0.8342\n",
      "Epoch 44/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1055 - accuracy: 0.8556\n",
      "Epoch 45/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1041 - accuracy: 0.8556\n",
      "Epoch 46/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1037 - accuracy: 0.8556\n",
      "Epoch 47/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1011 - accuracy: 0.8717\n",
      "Epoch 48/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0999 - accuracy: 0.8770\n",
      "Epoch 49/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0989 - accuracy: 0.8663\n",
      "Epoch 50/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1039 - accuracy: 0.8556\n",
      "Epoch 51/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0977 - accuracy: 0.8610\n",
      "Epoch 52/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1031 - accuracy: 0.8663\n",
      "Epoch 53/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0947 - accuracy: 0.8824\n",
      "Epoch 54/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0936 - accuracy: 0.8877\n",
      "Epoch 55/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0945 - accuracy: 0.8770\n",
      "Epoch 56/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0934 - accuracy: 0.8770\n",
      "Epoch 57/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0891 - accuracy: 0.8877\n",
      "Epoch 58/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0916 - accuracy: 0.8824\n",
      "Epoch 59/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0904 - accuracy: 0.8824\n",
      "Epoch 60/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0876 - accuracy: 0.8930\n",
      "Epoch 61/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0911 - accuracy: 0.8824\n",
      "Epoch 62/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0885 - accuracy: 0.8984\n",
      "Epoch 63/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0837 - accuracy: 0.8824\n",
      "Epoch 64/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.0857 - accuracy: 0.8877\n",
      "Epoch 65/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.0826 - accuracy: 0.8877\n",
      "Epoch 66/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0847 - accuracy: 0.8717\n",
      "Epoch 67/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0844 - accuracy: 0.8930\n",
      "Epoch 68/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0795 - accuracy: 0.8930\n",
      "Epoch 69/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0789 - accuracy: 0.8984\n",
      "Epoch 70/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0807 - accuracy: 0.8877\n",
      "Epoch 71/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0795 - accuracy: 0.8984\n",
      "Epoch 72/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0751 - accuracy: 0.8984\n",
      "Epoch 73/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.0758 - accuracy: 0.9037\n",
      "Epoch 74/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0745 - accuracy: 0.9305\n",
      "Epoch 75/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0758 - accuracy: 0.8930\n",
      "Epoch 76/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0844 - accuracy: 0.8984\n",
      "Epoch 77/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0767 - accuracy: 0.8877\n",
      "Epoch 78/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.0827 - accuracy: 0.8877\n",
      "Epoch 79/100\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.0744 - accuracy: 0.9091\n",
      "Epoch 80/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.0687 - accuracy: 0.9144\n",
      "Epoch 81/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.0699 - accuracy: 0.9091\n",
      "Epoch 82/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.0675 - accuracy: 0.9144\n",
      "Epoch 83/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.0664 - accuracy: 0.9251\n",
      "Epoch 84/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.0683 - accuracy: 0.9198\n",
      "Epoch 85/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.0723 - accuracy: 0.9198\n",
      "Epoch 86/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0663 - accuracy: 0.9144\n",
      "Epoch 87/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0664 - accuracy: 0.9144\n",
      "Epoch 88/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0655 - accuracy: 0.9198\n",
      "Epoch 89/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0633 - accuracy: 0.9358\n",
      "Epoch 90/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0774 - accuracy: 0.9037\n",
      "Epoch 91/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.0643 - accuracy: 0.9251\n",
      "Epoch 92/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0637 - accuracy: 0.9358\n",
      "Epoch 93/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0598 - accuracy: 0.9412\n",
      "Epoch 94/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0595 - accuracy: 0.9412\n",
      "Epoch 95/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.0593 - accuracy: 0.9465\n",
      "Epoch 96/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0601 - accuracy: 0.9412\n",
      "Epoch 97/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0587 - accuracy: 0.9465\n",
      "Epoch 98/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0570 - accuracy: 0.9412\n",
      "Epoch 99/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0571 - accuracy: 0.9465\n",
      "Epoch 100/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0583 - accuracy: 0.9358\n",
      "WARNING:tensorflow:5 out of the last 12 calls to <function Model.make_test_function.<locals>.test_function at 0x000002851E0EE288> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 996us/step - loss: 0.0994 - accuracy: 0.9048\n",
      "Epoch 1/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2441 - accuracy: 0.5829\n",
      "Epoch 2/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2254 - accuracy: 0.6738: 0s - loss: 0.2293 - accuracy: \n",
      "Epoch 3/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2150 - accuracy: 0.6684\n",
      "Epoch 4/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2033 - accuracy: 0.7326\n",
      "Epoch 5/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1911 - accuracy: 0.7433\n",
      "Epoch 6/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1790 - accuracy: 0.7754\n",
      "Epoch 7/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1708 - accuracy: 0.7807\n",
      "Epoch 8/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1594 - accuracy: 0.8075\n",
      "Epoch 9/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1520 - accuracy: 0.8128\n",
      "Epoch 10/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1413 - accuracy: 0.8503\n",
      "Epoch 11/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1462 - accuracy: 0.8235\n",
      "Epoch 12/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1327 - accuracy: 0.8396\n",
      "Epoch 13/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1381 - accuracy: 0.8182\n",
      "Epoch 14/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1255 - accuracy: 0.8663\n",
      "Epoch 15/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1232 - accuracy: 0.8610\n",
      "Epoch 16/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1189 - accuracy: 0.8556\n",
      "Epoch 17/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1150 - accuracy: 0.8770\n",
      "Epoch 18/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1148 - accuracy: 0.8717\n",
      "Epoch 19/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1091 - accuracy: 0.8770\n",
      "Epoch 20/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1026 - accuracy: 0.8770\n",
      "Epoch 21/100\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 0.1034 - accuracy: 0.8770\n",
      "Epoch 22/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1127 - accuracy: 0.8449\n",
      "Epoch 23/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0955 - accuracy: 0.9037\n",
      "Epoch 24/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1070 - accuracy: 0.8824\n",
      "Epoch 25/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0904 - accuracy: 0.9037\n",
      "Epoch 26/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0910 - accuracy: 0.8984\n",
      "Epoch 27/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0886 - accuracy: 0.9037\n",
      "Epoch 28/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0890 - accuracy: 0.8824\n",
      "Epoch 29/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0837 - accuracy: 0.9037\n",
      "Epoch 30/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0823 - accuracy: 0.9091\n",
      "Epoch 31/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0770 - accuracy: 0.9091\n",
      "Epoch 32/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.0845 - accuracy: 0.8930\n",
      "Epoch 33/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0792 - accuracy: 0.9198\n",
      "Epoch 34/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.0723 - accuracy: 0.9305\n",
      "Epoch 35/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0727 - accuracy: 0.9198\n",
      "Epoch 36/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0721 - accuracy: 0.9358\n",
      "Epoch 37/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0699 - accuracy: 0.9198\n",
      "Epoch 38/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0658 - accuracy: 0.9305\n",
      "Epoch 39/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0689 - accuracy: 0.9305\n",
      "Epoch 40/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.0689 - accuracy: 0.9358\n",
      "Epoch 41/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0630 - accuracy: 0.9412\n",
      "Epoch 42/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0599 - accuracy: 0.9412\n",
      "Epoch 43/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0583 - accuracy: 0.9358\n",
      "Epoch 44/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0583 - accuracy: 0.9465\n",
      "Epoch 45/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0555 - accuracy: 0.9465\n",
      "Epoch 46/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0569 - accuracy: 0.9519: 0s - loss: 0.0668 - accuracy: 0.\n",
      "Epoch 47/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0610 - accuracy: 0.9358\n",
      "Epoch 48/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0531 - accuracy: 0.9519\n",
      "Epoch 49/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0517 - accuracy: 0.9626\n",
      "Epoch 50/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0530 - accuracy: 0.9465\n",
      "Epoch 51/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0495 - accuracy: 0.9519\n",
      "Epoch 52/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0477 - accuracy: 0.9572\n",
      "Epoch 53/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.0501 - accuracy: 0.9412\n",
      "Epoch 54/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0461 - accuracy: 0.9519\n",
      "Epoch 55/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0456 - accuracy: 0.9626\n",
      "Epoch 56/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0479 - accuracy: 0.9519\n",
      "Epoch 57/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0417 - accuracy: 0.9626\n",
      "Epoch 58/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0410 - accuracy: 0.9679\n",
      "Epoch 59/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0409 - accuracy: 0.9626\n",
      "Epoch 60/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0388 - accuracy: 0.9679\n",
      "Epoch 61/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0389 - accuracy: 0.9733\n",
      "Epoch 62/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0361 - accuracy: 0.9733\n",
      "Epoch 63/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0354 - accuracy: 0.9733\n",
      "Epoch 64/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0454 - accuracy: 0.9412\n",
      "Epoch 65/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0429 - accuracy: 0.9572\n",
      "Epoch 66/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0354 - accuracy: 0.9733\n",
      "Epoch 67/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0355 - accuracy: 0.9679\n",
      "Epoch 68/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.0324 - accuracy: 0.9733\n",
      "Epoch 69/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.0301 - accuracy: 0.9786\n",
      "Epoch 70/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0308 - accuracy: 0.9733\n",
      "Epoch 71/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0320 - accuracy: 0.9679\n",
      "Epoch 72/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0273 - accuracy: 0.9733\n",
      "Epoch 73/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0258 - accuracy: 0.9840\n",
      "Epoch 74/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0267 - accuracy: 0.9893\n",
      "Epoch 75/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.0243 - accuracy: 0.9840\n",
      "Epoch 76/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0240 - accuracy: 0.9840\n",
      "Epoch 77/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0270 - accuracy: 0.9786\n",
      "Epoch 78/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0218 - accuracy: 0.9893\n",
      "Epoch 79/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0206 - accuracy: 0.9893\n",
      "Epoch 80/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.0205 - accuracy: 0.9893\n",
      "Epoch 81/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0222 - accuracy: 0.9840\n",
      "Epoch 82/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0192 - accuracy: 0.9893\n",
      "Epoch 83/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0205 - accuracy: 0.9893\n",
      "Epoch 84/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.0192 - accuracy: 0.9893\n",
      "Epoch 85/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.0203 - accuracy: 0.9893\n",
      "Epoch 86/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.0183 - accuracy: 0.9893\n",
      "Epoch 87/100\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.0176 - accuracy: 0.9893\n",
      "Epoch 88/100\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 0.0180 - accuracy: 0.9786\n",
      "Epoch 89/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.0164 - accuracy: 0.9893\n",
      "Epoch 90/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0162 - accuracy: 0.9893\n",
      "Epoch 91/100\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 0.0145 - accuracy: 0.9893\n",
      "Epoch 92/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.0139 - accuracy: 0.9893\n",
      "Epoch 93/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0154 - accuracy: 0.9893\n",
      "Epoch 94/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0132 - accuracy: 0.9893\n",
      "Epoch 95/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0134 - accuracy: 0.9893\n",
      "Epoch 96/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0119 - accuracy: 0.9893\n",
      "Epoch 97/100\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.0111 - accuracy: 0.9893\n",
      "Epoch 98/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.0111 - accuracy: 0.9893\n",
      "Epoch 99/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0095 - accuracy: 0.9893\n",
      "Epoch 100/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0104 - accuracy: 0.9893\n",
      "WARNING:tensorflow:6 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x0000028514D99678> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.1943 - accuracy: 0.7143\n",
      "Epoch 1/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.2417 - accuracy: 0.5348\n",
      "Epoch 2/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2287 - accuracy: 0.6364\n",
      "Epoch 3/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2191 - accuracy: 0.6631\n",
      "Epoch 4/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2101 - accuracy: 0.6845\n",
      "Epoch 5/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1998 - accuracy: 0.7166\n",
      "Epoch 6/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.1898 - accuracy: 0.7487\n",
      "Epoch 7/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1844 - accuracy: 0.7273\n",
      "Epoch 8/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1734 - accuracy: 0.7861\n",
      "Epoch 9/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1666 - accuracy: 0.7807\n",
      "Epoch 10/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1573 - accuracy: 0.8503\n",
      "Epoch 11/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1622 - accuracy: 0.7701\n",
      "Epoch 12/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1485 - accuracy: 0.7968\n",
      "Epoch 13/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.1517 - accuracy: 0.7807\n",
      "Epoch 14/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1405 - accuracy: 0.8235\n",
      "Epoch 15/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1394 - accuracy: 0.8128\n",
      "Epoch 16/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1352 - accuracy: 0.8235\n",
      "Epoch 17/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1329 - accuracy: 0.8128\n",
      "Epoch 18/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1356 - accuracy: 0.8075\n",
      "Epoch 19/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1275 - accuracy: 0.8342\n",
      "Epoch 20/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1205 - accuracy: 0.8556\n",
      "Epoch 21/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1195 - accuracy: 0.8503\n",
      "Epoch 22/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1307 - accuracy: 0.8021\n",
      "Epoch 23/100\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.1134 - accuracy: 0.8663\n",
      "Epoch 24/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1172 - accuracy: 0.8770\n",
      "Epoch 25/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1085 - accuracy: 0.8717\n",
      "Epoch 26/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1088 - accuracy: 0.8556\n",
      "Epoch 27/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1067 - accuracy: 0.8770\n",
      "Epoch 28/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1089 - accuracy: 0.8610\n",
      "Epoch 29/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0994 - accuracy: 0.8770\n",
      "Epoch 30/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0992 - accuracy: 0.8984\n",
      "Epoch 31/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.0956 - accuracy: 0.8930\n",
      "Epoch 32/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1017 - accuracy: 0.8663\n",
      "Epoch 33/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0935 - accuracy: 0.9037\n",
      "Epoch 34/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0929 - accuracy: 0.9037\n",
      "Epoch 35/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0932 - accuracy: 0.8824\n",
      "Epoch 36/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0943 - accuracy: 0.8824\n",
      "Epoch 37/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0923 - accuracy: 0.9091\n",
      "Epoch 38/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0847 - accuracy: 0.9144\n",
      "Epoch 39/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0862 - accuracy: 0.9037\n",
      "Epoch 40/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.0842 - accuracy: 0.9305\n",
      "Epoch 41/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0848 - accuracy: 0.9037\n",
      "Epoch 42/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0785 - accuracy: 0.9358\n",
      "Epoch 43/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0770 - accuracy: 0.9305\n",
      "Epoch 44/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0757 - accuracy: 0.9305\n",
      "Epoch 45/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0761 - accuracy: 0.9358\n",
      "Epoch 46/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.0740 - accuracy: 0.9251\n",
      "Epoch 47/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0745 - accuracy: 0.9251\n",
      "Epoch 48/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0705 - accuracy: 0.9412\n",
      "Epoch 49/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0693 - accuracy: 0.9198\n",
      "Epoch 50/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0683 - accuracy: 0.9358\n",
      "Epoch 51/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0671 - accuracy: 0.9358\n",
      "Epoch 52/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0667 - accuracy: 0.9358\n",
      "Epoch 53/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0627 - accuracy: 0.9572\n",
      "Epoch 54/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0632 - accuracy: 0.9305\n",
      "Epoch 55/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0652 - accuracy: 0.9412\n",
      "Epoch 56/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0625 - accuracy: 0.9412\n",
      "Epoch 57/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0579 - accuracy: 0.9465\n",
      "Epoch 58/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0565 - accuracy: 0.9572\n",
      "Epoch 59/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0558 - accuracy: 0.9519\n",
      "Epoch 60/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0564 - accuracy: 0.9572\n",
      "Epoch 61/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0543 - accuracy: 0.9519\n",
      "Epoch 62/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0547 - accuracy: 0.9519\n",
      "Epoch 63/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0508 - accuracy: 0.9519\n",
      "Epoch 64/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0496 - accuracy: 0.9626\n",
      "Epoch 65/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0540 - accuracy: 0.9412\n",
      "Epoch 66/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0485 - accuracy: 0.9572\n",
      "Epoch 67/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0460 - accuracy: 0.9572\n",
      "Epoch 68/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0446 - accuracy: 0.9626\n",
      "Epoch 69/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0464 - accuracy: 0.9519\n",
      "Epoch 70/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0443 - accuracy: 0.9572\n",
      "Epoch 71/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0430 - accuracy: 0.9679\n",
      "Epoch 72/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0419 - accuracy: 0.9626\n",
      "Epoch 73/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0417 - accuracy: 0.9626\n",
      "Epoch 74/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0402 - accuracy: 0.9626\n",
      "Epoch 75/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0397 - accuracy: 0.9679\n",
      "Epoch 76/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0437 - accuracy: 0.9626\n",
      "Epoch 77/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0444 - accuracy: 0.9572\n",
      "Epoch 78/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0392 - accuracy: 0.9733\n",
      "Epoch 79/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0356 - accuracy: 0.9679\n",
      "Epoch 80/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0345 - accuracy: 0.9733\n",
      "Epoch 81/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0348 - accuracy: 0.9786\n",
      "Epoch 82/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0355 - accuracy: 0.9626\n",
      "Epoch 83/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0347 - accuracy: 0.9733\n",
      "Epoch 84/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0342 - accuracy: 0.9679\n",
      "Epoch 85/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0335 - accuracy: 0.9786\n",
      "Epoch 86/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0313 - accuracy: 0.9786\n",
      "Epoch 87/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0302 - accuracy: 0.9733\n",
      "Epoch 88/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0296 - accuracy: 0.9786\n",
      "Epoch 89/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0287 - accuracy: 0.9840\n",
      "Epoch 90/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0292 - accuracy: 0.9733\n",
      "Epoch 91/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0284 - accuracy: 0.9840\n",
      "Epoch 92/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.0277 - accuracy: 0.9840\n",
      "Epoch 93/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.0273 - accuracy: 0.9840\n",
      "Epoch 94/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0280 - accuracy: 0.9786\n",
      "Epoch 95/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.0274 - accuracy: 0.9786\n",
      "Epoch 96/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0282 - accuracy: 0.9840\n",
      "Epoch 97/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.0253 - accuracy: 0.9840\n",
      "Epoch 98/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0259 - accuracy: 0.9840\n",
      "Epoch 99/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0242 - accuracy: 0.9840\n",
      "Epoch 100/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0244 - accuracy: 0.9840\n",
      "WARNING:tensorflow:7 out of the last 14 calls to <function Model.make_test_function.<locals>.test_function at 0x0000028514DC5558> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0821 - accuracy: 0.8571\n",
      "Epoch 1/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2554 - accuracy: 0.4920\n",
      "Epoch 2/100\n",
      "38/38 [==============================] - ETA: 0s - loss: 0.2403 - accuracy: 0.69 - 0s 4ms/step - loss: 0.2413 - accuracy: 0.6524\n",
      "Epoch 3/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2326 - accuracy: 0.6684\n",
      "Epoch 4/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2235 - accuracy: 0.7166\n",
      "Epoch 5/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.2156 - accuracy: 0.7326\n",
      "Epoch 6/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2059 - accuracy: 0.7914\n",
      "Epoch 7/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1963 - accuracy: 0.7807\n",
      "Epoch 8/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1876 - accuracy: 0.7914\n",
      "Epoch 9/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1786 - accuracy: 0.8075\n",
      "Epoch 10/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1710 - accuracy: 0.7594\n",
      "Epoch 11/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1719 - accuracy: 0.7540\n",
      "Epoch 12/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1605 - accuracy: 0.8075\n",
      "Epoch 13/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1633 - accuracy: 0.7968\n",
      "Epoch 14/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.1523 - accuracy: 0.7968\n",
      "Epoch 15/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1502 - accuracy: 0.7861\n",
      "Epoch 16/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.1462 - accuracy: 0.7968\n",
      "Epoch 17/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.1431 - accuracy: 0.8235\n",
      "Epoch 18/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1462 - accuracy: 0.7968\n",
      "Epoch 19/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1409 - accuracy: 0.8182\n",
      "Epoch 20/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1377 - accuracy: 0.8182\n",
      "Epoch 21/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1372 - accuracy: 0.8075\n",
      "Epoch 22/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1415 - accuracy: 0.7754\n",
      "Epoch 23/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1277 - accuracy: 0.8342\n",
      "Epoch 24/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1363 - accuracy: 0.7914\n",
      "Epoch 25/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1233 - accuracy: 0.8075\n",
      "Epoch 26/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1246 - accuracy: 0.8449\n",
      "Epoch 27/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1204 - accuracy: 0.8503\n",
      "Epoch 28/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1241 - accuracy: 0.8289\n",
      "Epoch 29/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1159 - accuracy: 0.8610: 0s - loss: 0.1188 - accuracy: 0.84\n",
      "Epoch 30/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1147 - accuracy: 0.8717\n",
      "Epoch 31/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1126 - accuracy: 0.8556\n",
      "Epoch 32/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.1127 - accuracy: 0.8235\n",
      "Epoch 33/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1099 - accuracy: 0.8663\n",
      "Epoch 34/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1084 - accuracy: 0.8770\n",
      "Epoch 35/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1131 - accuracy: 0.8342\n",
      "Epoch 36/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.1066 - accuracy: 0.8717\n",
      "Epoch 37/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1060 - accuracy: 0.8396\n",
      "Epoch 38/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1049 - accuracy: 0.8663\n",
      "Epoch 39/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1062 - accuracy: 0.8503\n",
      "Epoch 40/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1031 - accuracy: 0.8610\n",
      "Epoch 41/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1063 - accuracy: 0.8717\n",
      "Epoch 42/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0946 - accuracy: 0.8663\n",
      "Epoch 43/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.0920 - accuracy: 0.8824\n",
      "Epoch 44/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0897 - accuracy: 0.8984\n",
      "Epoch 45/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0891 - accuracy: 0.8930\n",
      "Epoch 46/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0909 - accuracy: 0.8770\n",
      "Epoch 47/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0893 - accuracy: 0.8930\n",
      "Epoch 48/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0827 - accuracy: 0.9198\n",
      "Epoch 49/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0821 - accuracy: 0.8770\n",
      "Epoch 50/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0880 - accuracy: 0.8610\n",
      "Epoch 51/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.0821 - accuracy: 0.9091\n",
      "Epoch 52/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.0814 - accuracy: 0.8984\n",
      "Epoch 53/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0759 - accuracy: 0.9198\n",
      "Epoch 54/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0763 - accuracy: 0.9198\n",
      "Epoch 55/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0770 - accuracy: 0.9091\n",
      "Epoch 56/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0776 - accuracy: 0.8984\n",
      "Epoch 57/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.0709 - accuracy: 0.9144\n",
      "Epoch 58/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.0725 - accuracy: 0.9091\n",
      "Epoch 59/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.0696 - accuracy: 0.9198\n",
      "Epoch 60/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0663 - accuracy: 0.9412\n",
      "Epoch 61/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.0654 - accuracy: 0.9305\n",
      "Epoch 62/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0637 - accuracy: 0.9358\n",
      "Epoch 63/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.0676 - accuracy: 0.9144\n",
      "Epoch 64/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.0700 - accuracy: 0.8930\n",
      "Epoch 65/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0672 - accuracy: 0.9251\n",
      "Epoch 66/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0603 - accuracy: 0.9465\n",
      "Epoch 67/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0615 - accuracy: 0.9519\n",
      "Epoch 68/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0561 - accuracy: 0.9465\n",
      "Epoch 69/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0551 - accuracy: 0.9519\n",
      "Epoch 70/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.0540 - accuracy: 0.9465\n",
      "Epoch 71/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0527 - accuracy: 0.9519\n",
      "Epoch 72/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0507 - accuracy: 0.9572\n",
      "Epoch 73/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.0504 - accuracy: 0.9572\n",
      "Epoch 74/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0490 - accuracy: 0.9519\n",
      "Epoch 75/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0509 - accuracy: 0.9572\n",
      "Epoch 76/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0489 - accuracy: 0.9572\n",
      "Epoch 77/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0468 - accuracy: 0.9626\n",
      "Epoch 78/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.0467 - accuracy: 0.9572\n",
      "Epoch 79/100\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 0.0452 - accuracy: 0.9679\n",
      "Epoch 80/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.0446 - accuracy: 0.9626\n",
      "Epoch 81/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0421 - accuracy: 0.9679\n",
      "Epoch 82/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.0438 - accuracy: 0.9519\n",
      "Epoch 83/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0390 - accuracy: 0.9679\n",
      "Epoch 84/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0365 - accuracy: 0.9733\n",
      "Epoch 85/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0425 - accuracy: 0.9679\n",
      "Epoch 86/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0372 - accuracy: 0.9733\n",
      "Epoch 87/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0357 - accuracy: 0.9786\n",
      "Epoch 88/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0360 - accuracy: 0.9733\n",
      "Epoch 89/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0331 - accuracy: 0.9840\n",
      "Epoch 90/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0340 - accuracy: 0.9733\n",
      "Epoch 91/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0368 - accuracy: 0.9572\n",
      "Epoch 92/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0313 - accuracy: 0.9786\n",
      "Epoch 93/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0293 - accuracy: 0.9733\n",
      "Epoch 94/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0292 - accuracy: 0.9733\n",
      "Epoch 95/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0292 - accuracy: 0.9733\n",
      "Epoch 96/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0306 - accuracy: 0.9733\n",
      "Epoch 97/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0265 - accuracy: 0.9840\n",
      "Epoch 98/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0263 - accuracy: 0.9733\n",
      "Epoch 99/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0235 - accuracy: 0.9840\n",
      "Epoch 100/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0251 - accuracy: 0.9840\n",
      "WARNING:tensorflow:8 out of the last 15 calls to <function Model.make_test_function.<locals>.test_function at 0x0000028514D999D8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1050 - accuracy: 0.8571\n",
      "Epoch 1/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2502 - accuracy: 0.5348\n",
      "Epoch 2/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2384 - accuracy: 0.5722\n",
      "Epoch 3/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2322 - accuracy: 0.5455\n",
      "Epoch 4/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2258 - accuracy: 0.6845\n",
      "Epoch 5/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2188 - accuracy: 0.7273\n",
      "Epoch 6/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2107 - accuracy: 0.7647\n",
      "Epoch 7/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2039 - accuracy: 0.7326\n",
      "Epoch 8/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1953 - accuracy: 0.8021\n",
      "Epoch 9/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1862 - accuracy: 0.7647\n",
      "Epoch 10/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1783 - accuracy: 0.8289\n",
      "Epoch 11/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1765 - accuracy: 0.7540\n",
      "Epoch 12/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1638 - accuracy: 0.8021\n",
      "Epoch 13/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1629 - accuracy: 0.8128\n",
      "Epoch 14/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1516 - accuracy: 0.8075\n",
      "Epoch 15/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1472 - accuracy: 0.8235\n",
      "Epoch 16/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1444 - accuracy: 0.8075\n",
      "Epoch 17/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1424 - accuracy: 0.8128\n",
      "Epoch 18/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1363 - accuracy: 0.8235\n",
      "Epoch 19/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1346 - accuracy: 0.8396\n",
      "Epoch 20/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1296 - accuracy: 0.8342\n",
      "Epoch 21/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1286 - accuracy: 0.7914\n",
      "Epoch 22/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1347 - accuracy: 0.8075\n",
      "Epoch 23/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1203 - accuracy: 0.8556\n",
      "Epoch 24/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1230 - accuracy: 0.8289\n",
      "Epoch 25/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.1128 - accuracy: 0.8717\n",
      "Epoch 26/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1113 - accuracy: 0.8663\n",
      "Epoch 27/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1078 - accuracy: 0.8877\n",
      "Epoch 28/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1089 - accuracy: 0.8503\n",
      "Epoch 29/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1044 - accuracy: 0.8663\n",
      "Epoch 30/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1038 - accuracy: 0.8824\n",
      "Epoch 31/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1027 - accuracy: 0.8877\n",
      "Epoch 32/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1034 - accuracy: 0.8984\n",
      "Epoch 33/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0994 - accuracy: 0.8717\n",
      "Epoch 34/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0959 - accuracy: 0.9037\n",
      "Epoch 35/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0997 - accuracy: 0.8663\n",
      "Epoch 36/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0957 - accuracy: 0.9091\n",
      "Epoch 37/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0975 - accuracy: 0.8556\n",
      "Epoch 38/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0953 - accuracy: 0.8824\n",
      "Epoch 39/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0916 - accuracy: 0.9091\n",
      "Epoch 40/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0864 - accuracy: 0.9091\n",
      "Epoch 41/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0864 - accuracy: 0.9091\n",
      "Epoch 42/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0846 - accuracy: 0.9144\n",
      "Epoch 43/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0810 - accuracy: 0.9144\n",
      "Epoch 44/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0792 - accuracy: 0.9144\n",
      "Epoch 45/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0788 - accuracy: 0.9198\n",
      "Epoch 46/100\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.0808 - accuracy: 0.9144\n",
      "Epoch 47/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.0766 - accuracy: 0.9091\n",
      "Epoch 48/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0721 - accuracy: 0.9251\n",
      "Epoch 49/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0714 - accuracy: 0.9358\n",
      "Epoch 50/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0719 - accuracy: 0.9251\n",
      "Epoch 51/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0696 - accuracy: 0.9305\n",
      "Epoch 52/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0692 - accuracy: 0.9358\n",
      "Epoch 53/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0704 - accuracy: 0.9198\n",
      "Epoch 54/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0659 - accuracy: 0.9358\n",
      "Epoch 55/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0632 - accuracy: 0.9144\n",
      "Epoch 56/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0673 - accuracy: 0.9412\n",
      "Epoch 57/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0611 - accuracy: 0.9412\n",
      "Epoch 58/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0602 - accuracy: 0.9412\n",
      "Epoch 59/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0598 - accuracy: 0.9519\n",
      "Epoch 60/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0576 - accuracy: 0.9465\n",
      "Epoch 61/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0570 - accuracy: 0.9412\n",
      "Epoch 62/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0543 - accuracy: 0.9465\n",
      "Epoch 63/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0548 - accuracy: 0.9412\n",
      "Epoch 64/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0590 - accuracy: 0.9305\n",
      "Epoch 65/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0595 - accuracy: 0.9358\n",
      "Epoch 66/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0509 - accuracy: 0.9465\n",
      "Epoch 67/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0494 - accuracy: 0.9519\n",
      "Epoch 68/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0497 - accuracy: 0.9465\n",
      "Epoch 69/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0486 - accuracy: 0.9465\n",
      "Epoch 70/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0462 - accuracy: 0.9572\n",
      "Epoch 71/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0450 - accuracy: 0.9572\n",
      "Epoch 72/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0451 - accuracy: 0.9626\n",
      "Epoch 73/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0434 - accuracy: 0.9412\n",
      "Epoch 74/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0421 - accuracy: 0.9679\n",
      "Epoch 75/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0417 - accuracy: 0.9679\n",
      "Epoch 76/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0410 - accuracy: 0.9626\n",
      "Epoch 77/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0408 - accuracy: 0.9679\n",
      "Epoch 78/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0424 - accuracy: 0.9626\n",
      "Epoch 79/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0399 - accuracy: 0.9572\n",
      "Epoch 80/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0384 - accuracy: 0.9572\n",
      "Epoch 81/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0357 - accuracy: 0.9679\n",
      "Epoch 82/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0352 - accuracy: 0.9679\n",
      "Epoch 83/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0334 - accuracy: 0.9786\n",
      "Epoch 84/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0334 - accuracy: 0.9733\n",
      "Epoch 85/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0442 - accuracy: 0.9412\n",
      "Epoch 86/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0331 - accuracy: 0.9786\n",
      "Epoch 87/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0309 - accuracy: 0.9786\n",
      "Epoch 88/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0318 - accuracy: 0.9733\n",
      "Epoch 89/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0286 - accuracy: 0.9840\n",
      "Epoch 90/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0281 - accuracy: 0.9840\n",
      "Epoch 91/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0268 - accuracy: 0.9840\n",
      "Epoch 92/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0266 - accuracy: 0.9786\n",
      "Epoch 93/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0253 - accuracy: 0.9786\n",
      "Epoch 94/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0239 - accuracy: 0.9840\n",
      "Epoch 95/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0255 - accuracy: 0.9786\n",
      "Epoch 96/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0249 - accuracy: 0.9893\n",
      "Epoch 97/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0240 - accuracy: 0.9840\n",
      "Epoch 98/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0244 - accuracy: 0.9840\n",
      "Epoch 99/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0263 - accuracy: 0.9733\n",
      "Epoch 100/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0210 - accuracy: 0.9840\n",
      "WARNING:tensorflow:9 out of the last 16 calls to <function Model.make_test_function.<locals>.test_function at 0x0000028514D99318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 996us/step - loss: 0.0841 - accuracy: 0.9524\n",
      "Epoch 1/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2582 - accuracy: 0.4652\n",
      "Epoch 2/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2442 - accuracy: 0.5936\n",
      "Epoch 3/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2376 - accuracy: 0.6150\n",
      "Epoch 4/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2285 - accuracy: 0.7059\n",
      "Epoch 5/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2171 - accuracy: 0.7326\n",
      "Epoch 6/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2034 - accuracy: 0.7701\n",
      "Epoch 7/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1919 - accuracy: 0.7594\n",
      "Epoch 8/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1805 - accuracy: 0.7807\n",
      "Epoch 9/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1698 - accuracy: 0.8075\n",
      "Epoch 10/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1624 - accuracy: 0.7701\n",
      "Epoch 11/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1645 - accuracy: 0.7701\n",
      "Epoch 12/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.1475 - accuracy: 0.8075\n",
      "Epoch 13/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1494 - accuracy: 0.8289\n",
      "Epoch 14/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1374 - accuracy: 0.8235\n",
      "Epoch 15/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.1300 - accuracy: 0.8449\n",
      "Epoch 16/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1274 - accuracy: 0.8182\n",
      "Epoch 17/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.1233 - accuracy: 0.8396\n",
      "Epoch 18/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1237 - accuracy: 0.8449\n",
      "Epoch 19/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1155 - accuracy: 0.8770\n",
      "Epoch 20/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1164 - accuracy: 0.8289\n",
      "Epoch 21/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1121 - accuracy: 0.8610\n",
      "Epoch 22/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1130 - accuracy: 0.8556\n",
      "Epoch 23/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1053 - accuracy: 0.8770\n",
      "Epoch 24/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1049 - accuracy: 0.8877\n",
      "Epoch 25/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0951 - accuracy: 0.8717\n",
      "Epoch 26/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0976 - accuracy: 0.8984\n",
      "Epoch 27/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0939 - accuracy: 0.8930\n",
      "Epoch 28/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0922 - accuracy: 0.8824\n",
      "Epoch 29/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.0878 - accuracy: 0.9251\n",
      "Epoch 30/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0899 - accuracy: 0.8984\n",
      "Epoch 31/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0837 - accuracy: 0.9091\n",
      "Epoch 32/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0860 - accuracy: 0.9037\n",
      "Epoch 33/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0885 - accuracy: 0.8824\n",
      "Epoch 34/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0778 - accuracy: 0.9198\n",
      "Epoch 35/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0773 - accuracy: 0.9144\n",
      "Epoch 36/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0740 - accuracy: 0.9251\n",
      "Epoch 37/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.0751 - accuracy: 0.9037\n",
      "Epoch 38/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.0737 - accuracy: 0.9305\n",
      "Epoch 39/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0704 - accuracy: 0.9251\n",
      "Epoch 40/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0695 - accuracy: 0.9412\n",
      "Epoch 41/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0665 - accuracy: 0.9305\n",
      "Epoch 42/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0656 - accuracy: 0.9572\n",
      "Epoch 43/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0643 - accuracy: 0.9305\n",
      "Epoch 44/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0637 - accuracy: 0.9358\n",
      "Epoch 45/100\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.0617 - accuracy: 0.9412: 0s - loss: 0.0620 - accuracy: 0.\n",
      "Epoch 46/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.0630 - accuracy: 0.9358\n",
      "Epoch 47/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.0582 - accuracy: 0.9519\n",
      "Epoch 48/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.0550 - accuracy: 0.9519: 0s - loss: 0.0519 - accuracy: 0.95\n",
      "Epoch 49/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.0546 - accuracy: 0.9572\n",
      "Epoch 50/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0589 - accuracy: 0.9305: 0s - loss: 0.0589 - accuracy: 0.93\n",
      "Epoch 51/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.0617 - accuracy: 0.9305\n",
      "Epoch 52/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0501 - accuracy: 0.9572\n",
      "Epoch 53/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.0552 - accuracy: 0.9251\n",
      "Epoch 54/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.0482 - accuracy: 0.9572\n",
      "Epoch 55/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.0496 - accuracy: 0.9519\n",
      "Epoch 56/100\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.0587 - accuracy: 0.9144\n",
      "Epoch 57/100\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.0499 - accuracy: 0.9519\n",
      "Epoch 58/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.0428 - accuracy: 0.9572\n",
      "Epoch 59/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0431 - accuracy: 0.9626\n",
      "Epoch 60/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0421 - accuracy: 0.9572\n",
      "Epoch 61/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0412 - accuracy: 0.9572\n",
      "Epoch 62/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.0397 - accuracy: 0.9626\n",
      "Epoch 63/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.0389 - accuracy: 0.9572\n",
      "Epoch 64/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.0420 - accuracy: 0.9626\n",
      "Epoch 65/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0432 - accuracy: 0.9519\n",
      "Epoch 66/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0414 - accuracy: 0.9626\n",
      "Epoch 67/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0369 - accuracy: 0.9626\n",
      "Epoch 68/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.0353 - accuracy: 0.9679\n",
      "Epoch 69/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0368 - accuracy: 0.9626\n",
      "Epoch 70/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0317 - accuracy: 0.9679\n",
      "Epoch 71/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0342 - accuracy: 0.9733\n",
      "Epoch 72/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0308 - accuracy: 0.9733\n",
      "Epoch 73/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0305 - accuracy: 0.9679\n",
      "Epoch 74/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0288 - accuracy: 0.9733\n",
      "Epoch 75/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0291 - accuracy: 0.9733\n",
      "Epoch 76/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0276 - accuracy: 0.9786\n",
      "Epoch 77/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.0280 - accuracy: 0.9840\n",
      "Epoch 78/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0256 - accuracy: 0.9733\n",
      "Epoch 79/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.0264 - accuracy: 0.9786\n",
      "Epoch 80/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.0243 - accuracy: 0.9733\n",
      "Epoch 81/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0243 - accuracy: 0.9786\n",
      "Epoch 82/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0219 - accuracy: 0.9840\n",
      "Epoch 83/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0227 - accuracy: 0.9786\n",
      "Epoch 84/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0228 - accuracy: 0.9840\n",
      "Epoch 85/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0246 - accuracy: 0.9840\n",
      "Epoch 86/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0200 - accuracy: 0.9893\n",
      "Epoch 87/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0200 - accuracy: 0.9893\n",
      "Epoch 88/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0206 - accuracy: 0.9840\n",
      "Epoch 89/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0199 - accuracy: 0.9893\n",
      "Epoch 90/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0180 - accuracy: 0.9840\n",
      "Epoch 91/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0153 - accuracy: 0.9947\n",
      "Epoch 92/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.0191 - accuracy: 0.9947\n",
      "Epoch 93/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0182 - accuracy: 0.9840\n",
      "Epoch 94/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0176 - accuracy: 0.9893\n",
      "Epoch 95/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0164 - accuracy: 0.9893\n",
      "Epoch 96/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0155 - accuracy: 0.9893\n",
      "Epoch 97/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0145 - accuracy: 0.9840\n",
      "Epoch 98/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0146 - accuracy: 0.9893\n",
      "Epoch 99/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0139 - accuracy: 0.9893\n",
      "Epoch 100/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0132 - accuracy: 0.9893\n",
      "WARNING:tensorflow:10 out of the last 17 calls to <function Model.make_test_function.<locals>.test_function at 0x0000028519DA2048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1527 - accuracy: 0.7619\n",
      "Epoch 1/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2491 - accuracy: 0.5319\n",
      "Epoch 2/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2406 - accuracy: 0.5319\n",
      "Epoch 3/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2367 - accuracy: 0.5319\n",
      "Epoch 4/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2330 - accuracy: 0.5319\n",
      "Epoch 5/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2281 - accuracy: 0.5426\n",
      "Epoch 6/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2240 - accuracy: 0.6170\n",
      "Epoch 7/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2184 - accuracy: 0.6649\n",
      "Epoch 8/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2125 - accuracy: 0.6489\n",
      "Epoch 9/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2065 - accuracy: 0.7021\n",
      "Epoch 10/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1987 - accuracy: 0.7234\n",
      "Epoch 11/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1897 - accuracy: 0.7394\n",
      "Epoch 12/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1787 - accuracy: 0.7500\n",
      "Epoch 13/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1738 - accuracy: 0.7660\n",
      "Epoch 14/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1667 - accuracy: 0.7500\n",
      "Epoch 15/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1610 - accuracy: 0.7606\n",
      "Epoch 16/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1536 - accuracy: 0.7926\n",
      "Epoch 17/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1527 - accuracy: 0.7500\n",
      "Epoch 18/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1436 - accuracy: 0.7979\n",
      "Epoch 19/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.1426 - accuracy: 0.8245\n",
      "Epoch 20/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.1357 - accuracy: 0.8138\n",
      "Epoch 21/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.1318 - accuracy: 0.8404\n",
      "Epoch 22/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1314 - accuracy: 0.8085\n",
      "Epoch 23/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1244 - accuracy: 0.8298\n",
      "Epoch 24/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1242 - accuracy: 0.8511\n",
      "Epoch 25/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1240 - accuracy: 0.8191\n",
      "Epoch 26/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1175 - accuracy: 0.8404\n",
      "Epoch 27/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1140 - accuracy: 0.8511\n",
      "Epoch 28/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1129 - accuracy: 0.8564\n",
      "Epoch 29/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1225 - accuracy: 0.8404\n",
      "Epoch 30/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1114 - accuracy: 0.8511\n",
      "Epoch 31/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1089 - accuracy: 0.8511\n",
      "Epoch 32/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1112 - accuracy: 0.8245\n",
      "Epoch 33/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1163 - accuracy: 0.8298\n",
      "Epoch 34/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1017 - accuracy: 0.8564\n",
      "Epoch 35/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1014 - accuracy: 0.8723\n",
      "Epoch 36/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1030 - accuracy: 0.8617\n",
      "Epoch 37/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1008 - accuracy: 0.8564\n",
      "Epoch 38/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0978 - accuracy: 0.8564\n",
      "Epoch 39/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0958 - accuracy: 0.8617\n",
      "Epoch 40/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0944 - accuracy: 0.8777\n",
      "Epoch 41/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0945 - accuracy: 0.8777\n",
      "Epoch 42/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0917 - accuracy: 0.8830\n",
      "Epoch 43/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.0902 - accuracy: 0.8936\n",
      "Epoch 44/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0876 - accuracy: 0.8830\n",
      "Epoch 45/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0856 - accuracy: 0.9043\n",
      "Epoch 46/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0878 - accuracy: 0.8936\n",
      "Epoch 47/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0836 - accuracy: 0.8883\n",
      "Epoch 48/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0919 - accuracy: 0.8936\n",
      "Epoch 49/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0876 - accuracy: 0.8830\n",
      "Epoch 50/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0814 - accuracy: 0.9043\n",
      "Epoch 51/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0793 - accuracy: 0.9096\n",
      "Epoch 52/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0785 - accuracy: 0.8989\n",
      "Epoch 53/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0773 - accuracy: 0.9096\n",
      "Epoch 54/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0776 - accuracy: 0.9255\n",
      "Epoch 55/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0760 - accuracy: 0.9043\n",
      "Epoch 56/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0738 - accuracy: 0.9149\n",
      "Epoch 57/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0751 - accuracy: 0.9096\n",
      "Epoch 58/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0722 - accuracy: 0.9255\n",
      "Epoch 59/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0717 - accuracy: 0.9202\n",
      "Epoch 60/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0731 - accuracy: 0.9096\n",
      "Epoch 61/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0649 - accuracy: 0.9309\n",
      "Epoch 62/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0697 - accuracy: 0.9096\n",
      "Epoch 63/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0665 - accuracy: 0.9309\n",
      "Epoch 64/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0652 - accuracy: 0.9255\n",
      "Epoch 65/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0626 - accuracy: 0.9362\n",
      "Epoch 66/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0645 - accuracy: 0.9202\n",
      "Epoch 67/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0619 - accuracy: 0.9415\n",
      "Epoch 68/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.0604 - accuracy: 0.9309\n",
      "Epoch 69/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0609 - accuracy: 0.9415\n",
      "Epoch 70/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0617 - accuracy: 0.9362\n",
      "Epoch 71/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0577 - accuracy: 0.9309\n",
      "Epoch 72/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0569 - accuracy: 0.9415\n",
      "Epoch 73/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0567 - accuracy: 0.9415\n",
      "Epoch 74/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0577 - accuracy: 0.9415\n",
      "Epoch 75/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0564 - accuracy: 0.9309\n",
      "Epoch 76/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0540 - accuracy: 0.9468\n",
      "Epoch 77/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0518 - accuracy: 0.9521\n",
      "Epoch 78/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0489 - accuracy: 0.9521\n",
      "Epoch 79/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0569 - accuracy: 0.9202\n",
      "Epoch 80/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0494 - accuracy: 0.9521\n",
      "Epoch 81/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0454 - accuracy: 0.9681\n",
      "Epoch 82/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0473 - accuracy: 0.9574\n",
      "Epoch 83/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0508 - accuracy: 0.9362\n",
      "Epoch 84/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0482 - accuracy: 0.9521\n",
      "Epoch 85/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0458 - accuracy: 0.9574\n",
      "Epoch 86/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0454 - accuracy: 0.9574\n",
      "Epoch 87/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0470 - accuracy: 0.9521\n",
      "Epoch 88/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0409 - accuracy: 0.9628\n",
      "Epoch 89/100\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.0423 - accuracy: 0.9681\n",
      "Epoch 90/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0414 - accuracy: 0.9574\n",
      "Epoch 91/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0386 - accuracy: 0.9628\n",
      "Epoch 92/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0363 - accuracy: 0.9787\n",
      "Epoch 93/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0357 - accuracy: 0.9734\n",
      "Epoch 94/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0388 - accuracy: 0.9681\n",
      "Epoch 95/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0368 - accuracy: 0.9734\n",
      "Epoch 96/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0382 - accuracy: 0.9628\n",
      "Epoch 97/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0377 - accuracy: 0.9574\n",
      "Epoch 98/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0365 - accuracy: 0.9734\n",
      "Epoch 99/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0344 - accuracy: 0.9787\n",
      "Epoch 100/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0360 - accuracy: 0.9840\n",
      "WARNING:tensorflow:10 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x000002851E2D4168> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1322 - accuracy: 0.7500\n",
      "Epoch 1/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2480 - accuracy: 0.5319\n",
      "Epoch 2/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2346 - accuracy: 0.6330\n",
      "Epoch 3/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2275 - accuracy: 0.7074\n",
      "Epoch 4/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2174 - accuracy: 0.6489\n",
      "Epoch 5/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2083 - accuracy: 0.6915\n",
      "Epoch 6/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1993 - accuracy: 0.7394\n",
      "Epoch 7/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1874 - accuracy: 0.7926\n",
      "Epoch 8/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1745 - accuracy: 0.7766\n",
      "Epoch 9/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1699 - accuracy: 0.7926\n",
      "Epoch 10/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1600 - accuracy: 0.8085\n",
      "Epoch 11/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1517 - accuracy: 0.7979\n",
      "Epoch 12/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1453 - accuracy: 0.8032\n",
      "Epoch 13/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.1413 - accuracy: 0.8245\n",
      "Epoch 14/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1385 - accuracy: 0.8085\n",
      "Epoch 15/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1352 - accuracy: 0.8085\n",
      "Epoch 16/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1303 - accuracy: 0.8351\n",
      "Epoch 17/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.1259 - accuracy: 0.8351\n",
      "Epoch 18/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1244 - accuracy: 0.8670\n",
      "Epoch 19/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1221 - accuracy: 0.8457\n",
      "Epoch 20/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1167 - accuracy: 0.8511\n",
      "Epoch 21/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1143 - accuracy: 0.8511\n",
      "Epoch 22/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1166 - accuracy: 0.8564\n",
      "Epoch 23/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1133 - accuracy: 0.8404\n",
      "Epoch 24/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1065 - accuracy: 0.8404\n",
      "Epoch 25/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1080 - accuracy: 0.8351\n",
      "Epoch 26/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1012 - accuracy: 0.8723\n",
      "Epoch 27/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1030 - accuracy: 0.8617\n",
      "Epoch 28/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0977 - accuracy: 0.8830\n",
      "Epoch 29/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1041 - accuracy: 0.8351\n",
      "Epoch 30/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0939 - accuracy: 0.8830\n",
      "Epoch 31/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0929 - accuracy: 0.8883\n",
      "Epoch 32/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0891 - accuracy: 0.8777\n",
      "Epoch 33/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0929 - accuracy: 0.8883\n",
      "Epoch 34/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0826 - accuracy: 0.9043\n",
      "Epoch 35/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.0817 - accuracy: 0.8936\n",
      "Epoch 36/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.0814 - accuracy: 0.9096\n",
      "Epoch 37/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0779 - accuracy: 0.9043\n",
      "Epoch 38/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.0745 - accuracy: 0.9255\n",
      "Epoch 39/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.0748 - accuracy: 0.9096\n",
      "Epoch 40/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0735 - accuracy: 0.9149\n",
      "Epoch 41/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0784 - accuracy: 0.9149\n",
      "Epoch 42/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0730 - accuracy: 0.9096\n",
      "Epoch 43/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0658 - accuracy: 0.9149\n",
      "Epoch 44/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0633 - accuracy: 0.9309\n",
      "Epoch 45/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0592 - accuracy: 0.9415\n",
      "Epoch 46/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.0672 - accuracy: 0.9043\n",
      "Epoch 47/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.0589 - accuracy: 0.9255\n",
      "Epoch 48/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0540 - accuracy: 0.9362\n",
      "Epoch 49/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0559 - accuracy: 0.9255\n",
      "Epoch 50/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.0496 - accuracy: 0.9521\n",
      "Epoch 51/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0484 - accuracy: 0.9415\n",
      "Epoch 52/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.0459 - accuracy: 0.9468\n",
      "Epoch 53/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.0442 - accuracy: 0.9574\n",
      "Epoch 54/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.0410 - accuracy: 0.9521\n",
      "Epoch 55/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.0388 - accuracy: 0.9787\n",
      "Epoch 56/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0395 - accuracy: 0.9628\n",
      "Epoch 57/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0379 - accuracy: 0.9787\n",
      "Epoch 58/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0347 - accuracy: 0.9734\n",
      "Epoch 59/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0356 - accuracy: 0.9628\n",
      "Epoch 60/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.0332 - accuracy: 0.9734\n",
      "Epoch 61/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.0383 - accuracy: 0.9681\n",
      "Epoch 62/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0305 - accuracy: 0.9787\n",
      "Epoch 63/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0289 - accuracy: 0.9840\n",
      "Epoch 64/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0307 - accuracy: 0.9734\n",
      "Epoch 65/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0297 - accuracy: 0.9734\n",
      "Epoch 66/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0302 - accuracy: 0.9787\n",
      "Epoch 67/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0245 - accuracy: 0.9894\n",
      "Epoch 68/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0244 - accuracy: 0.9947\n",
      "Epoch 69/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0233 - accuracy: 0.9840\n",
      "Epoch 70/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0224 - accuracy: 0.9840\n",
      "Epoch 71/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0199 - accuracy: 0.9947\n",
      "Epoch 72/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0221 - accuracy: 0.9840\n",
      "Epoch 73/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0202 - accuracy: 0.9947\n",
      "Epoch 74/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0210 - accuracy: 0.9894\n",
      "Epoch 75/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0180 - accuracy: 0.9947\n",
      "Epoch 76/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0178 - accuracy: 0.9840\n",
      "Epoch 77/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0204 - accuracy: 0.9947\n",
      "Epoch 78/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.0155 - accuracy: 1.0000\n",
      "Epoch 79/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.0138 - accuracy: 1.0000\n",
      "Epoch 80/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0132 - accuracy: 1.0000\n",
      "Epoch 81/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0137 - accuracy: 1.0000\n",
      "Epoch 82/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0138 - accuracy: 0.9947\n",
      "Epoch 83/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0131 - accuracy: 0.9947\n",
      "Epoch 84/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0135 - accuracy: 0.9947\n",
      "Epoch 85/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0118 - accuracy: 1.0000\n",
      "Epoch 86/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0107 - accuracy: 1.0000\n",
      "Epoch 87/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0116 - accuracy: 1.0000\n",
      "Epoch 88/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0100 - accuracy: 0.9947\n",
      "Epoch 89/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0106 - accuracy: 1.0000\n",
      "Epoch 90/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0116 - accuracy: 1.0000\n",
      "Epoch 91/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0089 - accuracy: 1.0000\n",
      "Epoch 92/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0080 - accuracy: 1.0000\n",
      "Epoch 93/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0082 - accuracy: 1.0000\n",
      "Epoch 94/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.0095 - accuracy: 0.9947\n",
      "Epoch 95/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0068 - accuracy: 1.0000\n",
      "Epoch 96/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0077 - accuracy: 1.0000\n",
      "Epoch 97/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0080 - accuracy: 1.0000\n",
      "Epoch 98/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0064 - accuracy: 1.0000\n",
      "Epoch 99/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0058 - accuracy: 1.0000\n",
      "Epoch 100/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.0055 - accuracy: 1.0000\n",
      "WARNING:tensorflow:11 out of the last 12 calls to <function Model.make_test_function.<locals>.test_function at 0x000002852499F048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2023 - accuracy: 0.7500\n",
      "\n",
      " 10 fold accuracy: ['0.8095', '0.8571', '0.9048', '0.7143', '0.8571', '0.8571', '0.9524', '0.7619', '0.7500', '0.7500']\n"
     ]
    }
   ],
   "source": [
    "# 빈 accuracy 배열\n",
    "accuracy = []\n",
    "\n",
    "\n",
    "# 모델의 설정, 컴파일, 실행\n",
    "for train, test in skf.split(X, Y):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(24, input_dim=60, activation='relu'))\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='mean_squared_error',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    model.fit(X[train], Y[train], epochs=100, batch_size=5)\n",
    "    k_accuracy = \"%.4f\" % (model.evaluate(X[test], Y[test])[1])\n",
    "    accuracy.append(k_accuracy)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"\\n %.f fold accuracy:\" % n_fold, accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 베스트 모델 만들기\n",
    "  - 와인의 종류 예측"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxgAAAGiCAYAAABks2BeAAAgAElEQVR4Aey9/2sbV77/v3/b/GItCQKz5nrxNuDucnUp8Q2hJnzW6aVu2ai9xJRGpDTv8q7TS+uwjbsYl1ycEOSl14VEZb3KmxThcBUaBCmCgEtatQWD4fnhzMxLnnnpjDSSbM+x5xlwNWfmzDmv8zjq66XnnC/zm59//hny1+l0YP5++ukn/Pjjj/jhhx/w4sULfP/992g2m6jX67h//z74jwRIgARIgARIgARIgARI4OQSML/pzW978xvf/NY3v/nNb3+jAYwWEF0gOsF8/vLLL6n+fhO9SQqiwDi5XxZaTgIkQAIkQAIkQAIkQAKDCFBgDCLE6yRAAiRAAiRAAiRAAiRAAqkJUGCkRsWMJEACJEACJEACJEACJEACgwhQYAwixOskQAIkQAIkQAIkQAIkQAKpCVBgpEbFjCRAAiRAAiRAAiRAAiRAAoMIUGAMIsTrJEACJEACJEACJEACJEACqQlQYKRGxYwkQAIkQAIkQAIkQAIkQAKDCFBgDCLE6yRAAiRAAiRAAiRAAiRAAqkJUGCkRsWMJEACJEACJEACJEACJEACgwhQYAwixOskQAIkQAIkQAIkQAIkQAKpCVBgpEbFjCRAAiRAAiRAAiRAAiRAAoMIUGAMIsTrJEACJEACJEACJEACJEACqQlQYKRGxYwkQAIkQAIkQAIkQAIkQAKDCFBgDCLE6yRAAiRAAiRAAiRAAiRAAqkJUGCkRsWMJEACJEACJEACJEACJEACgwicKoHxt7/9Da7+DeoIXicBEiCBPBCgj85DL7ONJEACeSdAgXFMouQ3v/kN+EcG/A7wO5D374CrAiPv/cL20zfxO8DvQPQ7MK5AOpUCo9VqwZU/CabjdhTvJwESIIHTQEB8In30aehNtoEESOA0EjBCY9x/FBhHLEYkmI7bUbyfBEiABE4DAfGJFBinoTfZBhIggdNIgAJD9aprgcsEULFJmcokCZAACeSSgPhECoxcdj8bTQIkcAIIUGCoTnItcFFgqA5ikgRIIPcEXPPTYk/uO4YASIAESCAkQIGhvgoSKFx5MkaBoTqISRIggdwTcM1Piz257xgCIAESIIGQAAWG+ipIoKDAUGCYJAESIAFHCLjmp8UeR/DQDBIgARLInAAFhuoCCRQUGAoMkyRAAiTgCAHX/LTY4wgemkECJEACmROgwFBdIIGCAkOBYZIESIAEHCHgmp8WexzBQzNIgARIIHMCFBiqCyRQUGAoMEySAAmQgCMEXPPTYo8jeGgGCZAACWROgAJDdYEECgoMBYZJEiABEnCEgGt+WuxxBA/NIAESIIHMCVBgqC6QQEGBocAwSQIkQAKOEHDNT4s9juChGSRAAiSQOQEKDNUFEigoMBQYJkmABEjAEQKu+WmxxxE8NIMESIAEMidAgaG6QAIFBYYCwyQJkAAJOELANT8t9jiCh2aQAAmQQOYEKDBUF0igoMBQYJgkARIgAUcIuOanxR5H8NAMEiABEsicAAWG6gIJFBQYCgyTJEACJOAIAdf8tNjjCB6aQQIkQAKZE6DAUF0ggYICQ4FhkgRIgAQcIeCanxZ7HMFDM0iABEggcwIUGKoLJFBQYCgwTJIACZCAIwRc89NijyN4aAYJkAAJZE6AAkN1gQQKCgwFhkkSIAEScISAa35a7HEED80gARIggcwJUGCoLpBAQYGhwDBJAiRAAo4QcM1Piz2O4KEZJEACJJA5AQoM1QUSKCgwFBgmSYAESMARAq75abHHETw0gwRIgAQyJ0CBobpAAsVYAqP2GS7/YQKe5/l/E3+4jNv/aGHUMsUmZSqTJEACJJBLAuITR/GpO/c+QvmdcsLfZ9h6OryvFnty2RlsNAmQAAlYCFBgKCgSKEYJXP49O2u4PGGExSTOX/sMn73zKiaM0Ji4jLX/HT5wmTLFJmUqkyRAAiSQSwLiE0fx049vXcL076fjf/9yNvTTl3C7MbyfFnty2RlsNAmQAAlYCFBgKCgSKEYJXOaeh//nFX/U4uybd7DTMoFqB2uXg9GMVz58ONIohtikTGWSBEiABHJJQHziqH5a3/f41vnAb/9/a3js++3hRIbYk8vOYKNJgARIwEKAAkNBkUChA1C69A5uXzSjF2dxeW2nKyae/u1yMF3q3z9j8FK8mSQBEiCBYQmM56e1eHiIj84Fo87le0+7fjudzw/KEnuGbQfzkwAJkMBpJUCBoXpWAsUwweUg7yauFgOBcfXvkSD24ANMm2lSxavY5NMxRZxJEiABEhiOwHh+OuKbjT+uXsVZ45//8AG2nqlrKf212DNcK5ibBEiABE4vAQoM1bcSKA5EwzAB5x7K/sLus4gJjH/eRIkCQ5FmkgRIgARGIzCen4769KdY+49wCuv1rZFGL0ysEHtGaw3vIgESIIHTR4ACQ/WpBAoKDAWGSRIgARJwhMB4fjoiMB5/hvPm4c9ECTf/GTmfcuRC4oTY4wgemkECJEACmROgwFBdIIFCAsdwn9v4aNYyRerv4RD8aZoitdtE7UEd7V8VwFTJXTQf1FBr7qbKbTLtveygs5c6OzOSAAmcYgLj+ekDIdHdlGPExd0SH8Sew0O+h/aj4Xzk4dXNkkiABEhgfAIUGIqhBAoJHMN9PsXaZSMwPJy/9bg73L69XAp2KLm8Fu4sdRDg0pQvNilTe5KNj2cwcy7t3yI2XvQU0T3RXpuD55Ww+rx7Kn6wXel7vX7Ng1daRTt+V5iqo2KeGl6tWa/2ngzyl76wl9abv/fM7pMaakbUpPl71Aa1TC9DniEBVwiIT0zjPxPzPJM1c5MYdXG3lC329OXzso7V6xVUEv7WG1Gv08b6nM1HdtAywiONHxvyIU5f23mRBEiABIYkQIGhgEmgkMAx7OfOXy8GO0b9601sm2H2Zw/xgb9DyVlc+tuB6BimXLFJmdqT3G32Bp7Vt4vwvHksf62v1dHq9BTRPXG6BEYYrGcXEoN7LOjfqiH92EoXGQ9IgASOiYD4xGH8qM77dO1y8O6LMRZ3S5liT9/mt9cx581g6b+1L97A0jkP8QcoSQIjeNhSvLCUzpfdbfY1iRdJgARI4KgIUGAoshIoJHAM/flsGzf/LVg0OFGcxnQxPP7jRxnsULKLjUtmRKWIyiPV0AHJ8QRGB9U/e/Cml2EPb8c9gpEUrAdA4GUSIAEnCYztp1uP8dm/G984gdJ/bXdHm4f29+FaDbGnLyxfYNhGhQP/NIzAiOftWysvkgAJkEAmBCgwFHYJFKMGGv++Z9u48+FlnDdvi/3jRbz14R1sj7j9oSlPbFKmDkx2tiuY8qYwM1OAN7eK9v7AW7oZ6tcKvjC58W14yp8SFUz/MlPAgj9bsDT5W1jxR23msWEdCqDA6ILmAQmQwNAExCeO7KefbmPzyzu48+U9PNwZbsqqrU6xp29DKDD64uFFEiCB00WAAkP1pwQKWxDJ6pzYpEztm2xvljHlFTD3RQvY3UJ52kPhwgqaL/veFlzcb+DGZCAiildr9vUI/dZgtFYxG4oQ+5O2UGCckylLVbT6mhXmPzOVbo3J2xtqihNHMPri5UUSOGEExCdm5ZN1vWJPX4wUGH3x8CIJkMDpIkCBofpTAoUOIFmmxSZlqjW526zixmtm9GEKi2tNdGTU4sUWlsxIRmEGi5/X0Y6uJ1QlmelRBW8Olevms4ilbyyLNRIFRgdbVwrwpitYvjYFz5uzLBQPBUN3TcQGmmKnsqWb/LWDjtlJqvtX9d85UrrVjJyT67pxFBhdjjwggVNAQHxiln45WrfY0xctBUZfPLxIAiRwughQYKj+lEARDR5ZH4tNytRIcg/NtUXMThoBUcTcexto7Oof2QD299B+sIyFWSNACiheqKCmpjB1vjHTqjxMfdgA0EHdFwlTKG+qHZwSBEYgTqZQMaJkv431CwV4kwtYjw1RDDtFKtLU7mFQhn2EpJspPKDA0ESYJoGTTEB8Yta+WeoXe/oy9QWGh8JU705/xcJwi7zT+b2+1vAiCZAACRwpAQoMhVcChQQOFz7FJmVqPNlpod5opX5XxN5uE43oeyj2O2h+Po+CERfvViPrNTq+eDGio1Bawvq3oSLRAmN/F7XrJRRkWpZY12li/U0zkjEVGTnJSGC8voytVNs7NtUUK2kMP0mABFwgID7RBf9sbBB7Do9N0kORwHdOXV1Pt1Utt9w+vC5hSSRAAkMRoMBQuCRQuBK4jiZ4qUbvNbFsXhBopk9Fp1VFs+02sH5lCRsykBERGHvNVcz7oyczWLwbG6roltB+UEFppoKavwYkpcAIn/gdLCqXxeUpP7vv2eig/rnef34Jc2adSXeaVvR6iilb3ZbxgARI4LgJuOanxZ7D5LDX6aDT83bRFqo979FY8Ne8Wbeu5Zbbh9klLIsESGAIAhQYCpYEipMkMFr3oz+Ohz1eRd386N/fQ2eYt3J/u4yZcwtdwbH7pI7ogIjCqpIpBYa663CTSU8ID7cWlkYCJHD4BFzz02JPmpYGW4CnfEhyrZ6iyMCfctpUClTMQgIkcGwEKDAUagkUJ0lg2F6wJ2963XhvBp715U7ysif1wr1f26inmkZk7m9iVy/ODu/vLzZGFxhmalf9wQZWok/xbm2g1mhhT9ui+jaepMCI82CKBE4OAdf8tNhzeASH8U8UGIfHnSWRAAkcFgEKDEVSAsVJEhiqCbHkwBfmxXIDeFnHavTHe9LxG7PwPMt7MMJpTUvf6IKj6REERmsDZbM4/cwsFt5bxvqmCKQaNm5VUL5Q9Nd/lD6pH+ycFa2y53iYAN5zM0+QAAlkSMA1Py32HB6SYfwTBcbhcWdJJEACh0WAAkORlECRW4GheCQmI2swYnlSCQxgz2w32zO/OFbSQWK3ioVCuPjcsjmWZOw8WcFcwcPMJ/b3h0u+4HOYAB6/kykSIIFsCbjmp8WeNFQ6Xy2lep+P2VnK664j61cyBUY/OrxGAiSQDQEKDMVdAgUFhgKjk2MKDF1c37Rf1xSWn/TN5V+sXfXglVYRrEVvYPlc75aQM+E5P4AXisnBvueFfYPrZw4SIIGjJ+CanxZ70rRcRpVXnsh7e/p8ykOYFxtYTPRlU/7uf16/F5F+bLYd5z8SIAESOD4CFBiKtQQKCgwFRiePU2CEbwYvfW7foaprWqeGpUkPhStbkFcD+iMl3Zfz9Qnk1jx9hku6lfKABEjguAm45qfFnjQcRGCsPk+TO8xjNuGw+qiUPk2EyhBVMisJkAAJjEOAAkPRk0BBgaHA6OQAgVGYTB45kBEE87n8rS7Ynt410woKHgqvLWF9s4FWJNi2m3VsfLKA2TPm+nKwK5a9GJ4lARI4BQRc89NiTxq0gcAoonQlzY5/4S5/aQpmHhIgARJwiAAFhuoMCRSnRWB0Hq2icn2l543dqtnDJ7+rJpY71KjBMIME+2007q6g8t4CStHpAq+XUfl4HbXm7pA7SQ3fbN5BAiSQPQHX/LTYk4rMkKMRw+2Ol8oCZiIBEiCBIydAgaEQS6A4LQJDNY9JEiABEjjxBFzz02LPiQfLBpAACZDAIRGgwFAgJVBQYCgwTJIACZCAIwRc89NijyN4aAYJkAAJZE6AAkN1gQQKCgwFhkkSIAEScISAa35a7HEED80gARIggcwJUGCoLpBAQYGhwDBJAiRAAo4QcM1Piz2O4KEZJEACJJA5AQoM1QUSKCgwFBgmSYAESMARAq75abHHETw0gwRIgAQyJ0CBobpAAgUFhgLDJAmQAAk4QsA1Py32OIKHZpAACZBA5gQoMFQXSKCgwFBgmCQBEiABRwi45qfFHkfw0AwSIAESyJwABYbqAgkUFBgKDJMkQAIk4AgB1/y02OMIHppBAiRAApkToMBQXSCBggJDgWGSBEiABBwh4JqfFnscwUMzSIAESCBzAhQYqgskULj4qUxlkgRIgARyScBF/2xs4j8SIAESIIGAAAWG+ia4GriMXaaz+EcG/A7wO5D374Crfjrv/cL20zfxO8DvQPQ7oH5iD528f/8+6vU6ms0mvv/+e7x48QI//PADfvzxR/z000/odDr+388//wz5++WXX5Dm7zdyg/mUgkyhpnBTianMVGoqN0YYY8b5J4GLU6TGoch7SYAESODoCLjmp8Weo2sxSyYBEiCBk0XACI1x/1FgtFo4SkHC4DXuV5T3kwAJnCYC4hOP0u8OU7bYc5oYsy0kQAIkMA4BCgxFTwLFMMHlqPOKTcpUJkmABEgglwTEJx61701bvtiTy85go0mABEjAQoACQ0GRQJE2sBxHPrFJmcokCZAACeSSgPjE4/C/aeoQe3LZGWw0CZAACVgIUGAoKBIo0gSV48ojNilTmSQBEiCBXBIQn3hcPnhQPWJPLjuDjSYBEiABCwEKDAVFAsWggHKc18UmZSqTJEACJJBLAuITj9MP96tL7MllZ7DRJEACJGAhQIGhoEig6BdMjvua2KRMZZIESIAEcklAfOJx++Kk+sSeXHYGG00CJEACFgIUGAqKBIqkQJLFebFJmcokCZAACeSSgPjELPyxrU6xJ5edwUaTAAmQgIUABYaCIoHCFkSyOic2KVOZJAESIIFcEhCfmJVP1vWKPbnsDDaaBEiABCwEKDAUFAkUOoBkmRablKlMkgAJkEAuCYhPzNIvR+sWe3LZGWw0CZAACVgIUGAoKBIoosEj62OxSZnKJAmQAAnkkoD4xKx9s9Qv9uSyM9hoEiABErAQoMBQUCRQSOBw4VNsUqYySQIkQAK5JCA+0QX/bGwQe3LZGWw0CZAACVgIUGAoKBIoXAlcDF6qg5gkARLIPQHX/LTYk/uOIQASIAESCAlQYKivggQKCgwFhkkSIAEScISAa35a7HEED80gARIggcwJUGCoLpBAQYGhwDBJAiRAAo4QcM1Piz2O4KEZJEACJJA5AQoM1QUSKCgwFBgmSYAESMARAq75abHHETw0gwRIgAQyJ0CBobpAAgUFhgLDJAmQAAk4QsA1Py32OIKHZpAACZBA5gQoMFQXSKCgwFBgmCQBEiABRwi45qfFHkfw0AwSIAESyJwABYbqAgkUFBgKDJMkQAIk4AgB1/y02OMIHppBAiRAApkToMBQXSCBggJDgWGSBEiABBwh4JqfFnscwUMzSIAESCBzAhQYqgskUIwtMJ7t4N5yGeV3yrj9oIVxyhOblKlMkgAJkEAuCYhPHNmvPtvGnQ8v4/zvpzHt/53HW7e2sNMazVeLPbnsDDaaBEiABCwEKDAUFAkUIwcuE6AaW7h5aRKe5/l/F/+6Q4GhODNJAiRAAqMSGMtPP9vGzX+b8H3zxB8vo/zOW7j4L2H64m08HkFkiD2jtof3kQAJkMBpI0CBoXpUAsXoAuMeyhNGWExgwv/0QIGhIDNJAiRAAmMQGMtP3ytjwjz8+cMH2HoWjljs3MFbReO3X8EHI4w4iz1jNIm3kgAJkMCpIkCBobpTAsXoAmMTH7z+Pu788yFu/itHMBReJkmABEhgbALj+Omna5eD0eWLtyNTorbx0azx12dx9e/DT5MSe8ZuGAsgARIggVNCgAJDdaQEitEFhgSnbQoMxZZJEiABEjgMAmP56X/eRMmfvvoK3v/7U3/66vbqZUyac797C3ca4sPTf4o9h9E2lkECJEACp4EABYbqRQkUFBgKDJMkQAIk4AiBcf30zr338erZYIRZ1sp5vzuPD6qjrZcTexzBQzNIgARIIHMCFBiqCyRQUGAoMEySAAmQgCMExvLTz7Zx29+EYwLTF9/yd/q7/Mdgkffk5TUu8nakj2kGCZDAySZAgaH6b6zAFdt9xN0pUrvNGmqP2thTbR8uuYf2oxpqzd3hbjuk3H4bHjSRtva9TgedXw+pchZDAiSQKYFx/LRZg2EWeU/8201syyLvZw/xwTkzojGB8r30U6PkQZTYkykUVk4CJEACDhGgwFCdIYFCAsfonxkIjP1d1D5ZQGmqAK9QxOwby6i1VQMB1K958EqrsFwKMr+sY/V6BdXv1L3tdcx5Hpa+MefbWJ/z4F2tRTLtoXV/GZXrlXR/91uRe9XhdgWeV8Lqc3U+TPpt8JZQ27dfj5+12RrPMTD1oonag1rKvzraFDMDkTIDCYxKYBw/vb1c8hd5n13ajG0ffu8vo2/KIfaM2p74fR3UP6+g0s8/JvnoeEFMkQAJkEBmBCgwFHoJFKMLC3n6ddwCo4P6tSl4Z+ax2mij025g/c0peN4c1pWSGCgwYkIiAih23vajPTw3u5BOYNxtRgpXh44JjPbaHDxvFgupxNMKammHVlSzmSQBEhhMYBw/LSMYXvEqNmUEI7pN7dfiw9N/ij2DLU+Tw+Zb1X0xX6yuMUkCJEACDhCgwFCdIIHixAmMb2+g6BWx9E3noEX7DdyY9OBd2YpNhzpygREb1TgwZ5ijzv0FeN4Ulp/Y7zruEYxAYKQdMbHbzLMkQAKHQ2AsP21etPfvwZoL77eT/pu8J38bTI969c01bMemuqYTGWLP4bSOAuNwOLIUEiCBLAlQYCj6EijGFxhPsX3vDu58eQf3/hFshThqmWKTMjWWrF8vwDu3jKaaMtT6dAaemko0UGD4owcyFSpSTeypmS0I2s5F7h/iMLDbw/xd+1AABcYQMJmVBE4ZAfGJo/rU1rOn2P7yA5TfuYzzf7yIt955Hzc3tmNTpoYpW+w5HMwp/GjMFx9OrSyFBEiABA6TAAWGoimBYpjgctR5xSZlaiQZBqR3o+shwsu+WJjFamS5w5ELjNeXsZVmvULiAvEWVv8UbiGZsFYkEBgzWLgWrPfoWS8SodNdL1IoYubcTIq/ZTRi9wMcwVBAmCSBDAmITzxq35u2fLEnLZK93SbqD2qoN9vYUw+Fuv6q30gwBUZa1MxHAiSQEQEKDAVeAkXawHIc+cQmZWokWUfF81D6PKIi5KolEPk/zqeXsB6KgPrz+H5Su1+a9QYeSl+0gfD+7l7xfRd5h0JnzDUYna/KKHhTqHxSwZTnYc7Yof4FAuNgTcRGn+Uc/q17HXReRv+aWCl58N6tqvNBHh30KTBUBzBJAhkSEJ94HP43TR1iTzKS0De+sYyVSwUUJudQvr6EhdmCv7brxreRqa3WDTRUyRa/rnIwSQIkQAKZEqDAUPglUKQJKseVR2xSpkaSocCw/BAXgRDs/BTc4v84jzzNX/jv+A/4+jUT9PQOUWbjqEG7SKUY2o9YbT18vo65goepazV09oORg4JXxMKXcfEUCIxx1kQMZysFhrW3eJIEMiEgPvG4fPCgesSeZBihv/EC37YbGbVofTGHQmwzjjDvlSp2Yw9FIg9Inqz4byOP+vXkunmFBEiABI6fAAWGYi6BYlBAOc7rYpMyNZKso2J+lH9ieYwfEwXBLf6P84SpR5CF4UZgFCuoRwLhUQuM3W8qKBU8FC6sotWtt4Pm2qI/kjH15ipktCUbgTGP5a/TbVWbOPsr0ms8JAESGI2A+MTj9MP96hJ7klsTioaC5aFI6HOLH8rEzDDvmank6ZwzRRS6o8nJtfIKCZAACWRFgAJDkZdA0S+YHPc1sUmZGkmGASlxDUYp9RqMva/N9KSiPz2p6HkofxWZPhUTK2GdsXnC4bl+T96iT+Q6Ydl7Tay+bgJmATNvb0TERbSJNVRKM6iEu2SlExjByE50etdQxxER1nm02rP17tKFYuLWtQOnbEWaxkMSIIHhCIhPPG5fnFSf2JPcij4+uufdRDbfqkqO+WJ1jUkSIAEScIAABYbqBAkUSYEki/NikzI1lmx8VOwdcQDg78aknpoljmCYJ2nTHrxL69jd30X1zwV4k0uoyfTgWFCzBcFdbLydZhF1mOeNjYOX/b1ooj7EY/90AiOG6NATnDZ16EhZIAmkIiA+MQt/bKtT7Ek23uYvD3K3vyjBK94IN5fon9e/K+aLD8rhEQmQAAm4QoACQ/WEBApbEMnqnNikTI0nGzf8aUSLm6IGAHRqWJr0ULxej+W1C4zwRX3e3MFohwliBQ/eXDhlKRbUUgTBWK1pE3toP6qhNkBsjCwwfm2h8aCK9Y8jbxv/eB3VR03sRgZr0lhLgZGGEvOQwOETEJ+YlU/W9Yo9yS3t7y/jPrl/Xr+OmC9OrpVXSIAESCArAhQYirwECh1AskyLTcpUlRSBMIvKZhOtZhWVUgHedAW1l/Gs8WAGYH8XW++at34XMPdFfDE1WuuYP+Oh8No6WrGgliIIxqtNmUpX7tAC42UdyxeKKBSKmLtSwcrdg7UU1bVlLL0x23+KlsV6CgwLFJ4igWMgID4xS78crVvsSW566Neml9G7Ui689mYVweOhFD4w5ouTa+UVEiABEsiKAAWGIi+BIho8sj4Wm5SplmQHrc1lLLwWTEGaf28DDct76rTA6DRWMH9mCot3lbiQGswWr+bpfiyopQiCcv9QnynL/dW+nay1qnDqV+HCipVH9552FWUzRawb6LtXrAcUGFYsPEkCR05AfGLWvlnqF3uSGx76Nc9DbJTZ3PDdCma9AspfyehzCh8Y88XJtfIKCZAACWRFgAJDkZdAIYHDhU+xSZk6clILjNQFxYJaNAge3mLqVC+ZSm1wmDG0e+G+BPDkAvy50JE3nzc+7rOmZNJs51tAcSYpzyI2XiTXxSskQAKjERCf6IJ/NjaIPcmtCf1loeC/42dxrYH2yw7ajXUsTnuYerOKdnfnvKhvTSgx5osT8vA0CZAACWRIgAJDwZdA4UrgShe8VCMGJA9fYAyocKjLKYLrUOUB+HUL5YKH4ru1cApCQgH7bazOefD+tHKwk1U4UhJ/SV9kP/rorliWY/3CvoSaeZoESGAIAq75abEnuQkHfq39TTDKXDxTxMy5eSzdbSD6Xgwg3Czjo/jauVjZLzaweG4GNx7FzjJBAiRAAs4QoMBQXSGBggJDgTHJWFA7CJiWnGOcCsuNvAhw5lzSCMEMZj6WvWDkvPkAACAASURBVOP7V9l5shqsJZlZwPLdOprtiEhoNVBdW8LcZAGFmUVsJMwU618Dr5IACRwXAdf8tNiT3P6j8pfJNfIKCZAACWRJgAJD0ZdAcZoFRut+BZVbNViWZyga/ZId1D+voHK3d8liv7tSXRtm1EDepZGm4P0OWg/WsXy9jPmoaHltAUvXV7DRaIEjDmlAMg8JZEvANT8t9iRTocBIZsMrJEACp5EABYbqVQkUp1lgqCYzSQIkQAInioBrflrsSYZIgZHMhldIgAROIwEKDNWrEigoMBQYJkmABEjAEQKu+WmxJxlP+H6fJ+ONGyeXzyskQAIk4BYBCgzVHxIoKDAUGCZJgARIwBECrvlpsccRPDSDBEiABDInQIGhukACBQWGAsMkCZAACThCwDU/LfY4godmkAAJkEDmBCgwVBdIoKDAUGCYJAESIAFHCLjmp8UeR/DQDBIgARLInAAFhuoCCRQUGAoMkyRAAiTgCAHX/LTY4wgemkECJEACmROgwFBdIIGCAkOBYZIESIAEHCHgmp8WexzBQzNIgARIIHMCFBiqCyRQUGAoMEySAAmQgCMEXPPTYo8jeGgGCZAACWROgAJDdYEECgoMBYZJEiABEnCEgGt+WuxxBA/NIAESIIHMCVBgqC6QQOHipzKVSRIgARLIJQEX/bOxif9IgARIgAQCAhQY6pvgauAydpnO4h8Z8DvA70DevwOu+um89wvbT9/E7wC/A9HvgPqJPXTy/v37qNfraDab+P777/HixQv88MMP+PHHH/HTTz+h0+n4fz///DPk75dffkGav9/IDeZTCjKFmsJNJaYyU6mp3BhhjBnnnwQuTpEahyLvJQESIIGjI+CanxZ7jq7FLJkESIAEThYBIzTG/UeB0WrhKAUJg9e4X1HeTwIkcJoIiE88Sr87TNliz2lizLaQAAmQwDgEKDAUPQkUwwSXo84rNilTmSQBEiCBXBIQn3jUvjdt+WJPLjuDjSYBEiABCwEKDAVFAkXawHIc+cQmZSqTJEACJJBLAuITj8P/pqlD7MllZ7DRJEACJGAhQIGhoEigSBNUjiuP2KRMZZIESIAEcklAfOJx+eBB9Yg9uewMNpoESIAELAQoMBQUCRSDAspxXheblKlMkgAJkEAuCYhPPE4/3K8usSeXncFGkwAJkICFAAWGgiKBol8wOe5rYpMylUkSIAESyCUB8YnH7YuT6hN7ctkZbDQJkAAJWAhQYCgoEiiSAkkW58UmZSqTJEACJJBLAuITs/DHtjrFnlx2BhtNAiRAAhYCFBgKigQKWxDJ6pzYpExlkgRIgARySUB8YlY+Wdcr9uSyM9hoEiABErAQoMBQUCRQ6ACSZVpsUqYySQIkQAK5JCA+MUu/HK1b7MllZ7DRJEACJGAhQIGhoEigiAaPrI/FJmUqkyRAAiSQSwLiE7P2zVK/2JPLzmCjSYAESMBCgAJDQZFAIYHDhU+xSZnKJAmQAAnkkoD4RBf8s7FB7MllZ7DRJEACJGAhQIGhoEigcCVwMXipDmKSBEgg9wRc89NiT+47hgBIgARIICRAgaG+ChIoKDAUGCZJgARIwBECrvlpsccRPDSDBEiABDInQIGhukACBQWGAsMkCZAACThCwDU/LfY4godmkAAJkEDmBCgwVBdIoKDAUGCYJAESIAFHCLjmp8UeR/DQDBIgARLInAAFhuoCCRQUGAoMkyRAAiTgCAHX/LTY4wgemkECJEACmROgwFBdIIGCAkOBYZIESIAEHCHgmp8WexzBQzNIgARIIHMCFBiqCyRQUGAoMEySAAmQgCMEXPPTYo8jeGgGCZAACWROgAJDdYEECgoMBYZJEiABEnCEgGt+WuxxBA/NIAESIIHMCVBgqC6QQDGOwNip3cb7r7+K6d9PY/r3r+LitTt4/KyFUcsUm5SpTJIACZBALgmITxzVp7ae7WDr1ls47/voaUyffwu3a/TRufwysdEkQAJHQoACQ2EdO3D94yZKEx68iWlcfLuMy3+cgOd58P7wAR62RgtgYpMylUkSIAESyCUB8YmjCYwd3PvLpO+XJ/5wEW+9fRHTxmd7E7j418cjPQgSe3LZGWw0CZAACVgIUGAoKBIoRgtcT7H2H4GgKP3f7SBQPdvC+783wessrlYpMBRuJkmABEhgaAJj+em/X8VZ89Dnd2Xc2wl98oMP8Io5N1HGvREeBIk9QzeEN5AACZDAKSVAgaE6VgLFaAJjB/f+Txnld8q4/UDExDZu/msgMC6v7fDpmOLNJAmQAAkMS2AcP729XPJHL87+52bEHz/EB/6DoEm8/z/iu9N/ij3DtoP5SYAESOC0EqDAUD0rgWI0gWEJSP+8iZL/ZOwibj+2XE/xtExsUqYySQIkQAK5JCA+cRQ/bRcY2/ho1jwI8nBpdfgHQWJPLjuDjSYBEiABCwEKDAVFAsUogavnnmfbuPlvwZSpV5Y28TSFmOgpo9WC2KRMZZIESIAEcklAfKLNXw48J1OkilexKZtvyBQpz8PFv1Jg5PJLxUaTAAkcKgEKDIVzrMClBMTD668ECwlnr2JT5vqqPAODIQWG6iEmSYAE8k5gPD+9010rN1Es4XJ3kXew0JsjGHn/drH9JEACh0GAAkNRHC9wHUyBMsPwE/5Cwku4/Y+D82kEhc4jNilTjzS51+mg83IvuY7dJmoP6mj/mpwl+coumg9qqDV3k7OoK3svO+j0MUdlZ5IESOAUExCfqH1l6rS/Te37uHzebFF7GeUPb+OjS0ZgHMMajF/bqBv/F/171EbMvYV5hnCR8d4O768/j5Uaz9M3tYf2oxpq2q6+94x30Y85I8WT8erl3SRAAkdDgAJDcR07cLVa2LlTxmQoLm7+z/DD7TpIik3KVJXsoP55BZXraf9WUX+pikAHzbUyZs8Ec5H97XW9Akqf1NHZV3m3K/C8Elafq/Nhsn7Ng1daRdt6uY6K4XO1Zr3aezLIX/rCXlpv/t4zu09UQI8Gd318jEG111KeIQESGERAfKL2lanSj7dw58s7uPN1dEvaTVwtevCKl7HWGP6BkNgzyG7/+ss6VrWfvlVD7HFLex1znoelb5JKbGN9ro8PDe/XPrPzaLVPjFjGVtfFhuVbfHh7bc4fmQ/iQzRWWI7n1hNigG7XgPbo7Lb0C/PQK62fH/XhmK1iniMBErARoMBQVCRQpApUlulOhy0ujB1ikzJVJcMnTmkc7KfzVnHQ+nQWnjeF8mYLe6Gg2G2sYK7goXh1C20ziiB/m2VrGWKUWwIjDF6zC32Ca0SY6WAvjeInCZCAEwTEJ47kp7sbb1zG2v8GYkJGnCffuTfSWjmxpy+cvYj/FD9q+fR97xEJjL3ndeuP8OqHZmeteWx0VU6ywOjbxsjF/jEgktE/HF9gBMJnFgtavFnTK6h126ptYZoESOAwCFBgKIoSKEYKXK17KJsn8wl/Z5ei2yKmf0omNilTR07ufmmeQJVRi45KvKxiwfMw+0mzp9zOV2UUPA+FqRnMnAv/pgp9BEYH1T978KaX0VuaKf64RzDGD149UHiCBEggMwLiE0fz009x753gRXvebycx/S9n/emsE2OslRN7+gHpP3ogDzjCH74DBUYDNyY9eG9W0bFVmjCCYcsKdFB904N3aR273ZjQR2CkFErVd/uNYmtLxvfRgcBYisc1XQ3TJEACx0aAAkOhlkAxWuB6jC0z9J70FxuSz05gtL8owStUUO8GEwD+lKcZLD9RQEwyFB9za93x8zB/0hSpFlbOGaEVfSIWLZcCI0qDxyRAAsMRGM9Ph6MWX36A8tsXcf71t/w1GA9HmBolcULsSdWK/T10Wo3ISEIdzV21VmKQwPh1K3iYVbyBhq1Sq8DoHeWuri2j8t48prwilr6JSpXwB3+xhLI/AnAwpTb4IV/AlDxs6vd5pRqf+mWz1T9HgZGIhhdI4IQSoMBQHSeBQgKHC59ikzJ15KRt6Hrvq35TnixrIPqtwWitYjYcxdFzgAOjQ4FxTqYsVdHq25ow/5mpgxGUfkHt7Q0V1MYPXn3N40USIIFjJSA+0QX/bGwQewZCeF7F4kwBhZkFLF1fxvraCirvLfjr3gqlG2jIb/wBAmPv62BU2fOFgRInxojw/uJr5WBa6Od1dHCwTm95reoLnK27S76vnrpWU+vsQp95bgkb/rTbOlqhbUczUhDWVyim8/HnlnuE1dHYNbBHmYEESCCBAAWGAiOBwpXANVTwUm2xJ8PpS++qBdaNGyh6HspfWYLVdyuYMdOnrgdByV9Il7COwwy3b10pwJuuYPnaFDxvzrIQPBQM3TURG2hGR1Nshv+q5y9X/Sd4pVvNg3Uh3fnMug0UGDakPEcCJ5WAa35a7OnPcw9b/rShFbS0v+tUseh56I4ShwKhMBlOSY0+NNlvYeVPHgp/voEbFwrwJpdQE2EiBojAuLAUCAzLurLOk3UsTnsoXFrvtQehz0xc5D2P5a/TLahOvRNWz9SrJlZKHrx3qxYf3+muE+w22V98zilSwoOfJJA1AQoM1QMSKE6vwAimLxU/1APrLaz+ycyZVcFvv4OqEQz6yVLCGgzzFKngTaFihtv321j3A+AC1mNDFKHASL2LlOokPxmUYR8h0fkpMDQRpkngJBNwzU+LPYOYtr80/nEWlc1md4vvvZct1D9fxJQ3hRvfhiWEAqG8KQ9W5KFJB3Xz4KYwh1XjUzt1VKY9eLM34rsChvdb/ePLFsyi7oLZIfB6DW0tdnwTkgXGXmM93WYZ4eLqDftCvEGozDBM/52yVAkcwVBAmCSBjAlQYKgOkEBxogRGz9N9CUqWz/aG/6Rs9tNG96mQ7BgFM3xvgtX0PG6YIfS7Kyi/VoB3poTlR+oRmZ4itb+L2vUgaM19EVETnSbW3zQjGVNY/LyOth8nMxIYry9jK80uWw+aaoqV+pIwSQIkkCkB1/y02JMGSnt7HTfeKGHGf0hTQHFmBvNXVlCLvrMiFAixbWojvtR/gCOVvaihUjIPgUq+cPEXalsERufJBm68MRts2PHaEjYa/bZRShYYUu3Rf44iMI5gZOXoG8oaSOBUEqDAUN0qgeIkCQx/TUXCzlVJO1odnFcLtffbaNxd6T6hWl6ro2V7+VFEYOw1VzE/aQLcDBbvRsRFhG37QQWlmQpq/rs3UgqMMEge2Jq8Q5c1T3eE5GDu8cF7QpYwZ3Zh6U7Tkl1czGeKKVuRtvGQBEjgeAm45qfFnqEoRHxoz31KYOx+vRS8n8j2sMfcvG/eYbSImQurwXQni8AwI8pbt9ZRb8loSE+tkRPyor1Wd5eqxseRXQT7rYGzXlvExotI8d3DMBaMGr8iU7hsu3QtXSjC8+xb144+stI1ngckQAJ9CFBgKDgSKE6SwFBNiCVtC7pjGUZNqOC4+8TshJK2sJQCI21xI+Ub7unYSFXwJhIggSMh4JqfFnsSG/tdtfvQpvuQ4w3z3qEiSlcqKL8uP96n/BGG0tUl9aK9DlqPGuEIcGItBxdsAqN7tYWq9d0Q0Ycs4bG/ODy8UY+UP1lByazb607jCkbMg+1pV9DsrokLzndHyrt2HP0Bp00dPWPWQAJJBCgwFBkJFPkVGOGTq0FqQQmMLsZf26g/qA0QG6MLjL3dJuoPNrASDZC3NlBrHLwcsGtL3wMKjL54eJEEHCbgmp8WexKR7fZ/y3S92Y5PWVUjGLFy/bIGvIm6r8DYRXPgVNENLJmtxiMjBDEbTCLBxrEfav3aQuNBFesfRwTPx+uoPmpC7+bbY5M6QYGhgDBJAsdIgAJDwZZAkV+BkfKHd5LASAg6ccwjCIzWBsqzZj3ILBbeW8b65sEOJhu3KihfKAaLFj+pq+0W4zUfpFK28+AGHpEACThCwDU/LfYMg2fPvAtjcx3LkYcl/vax5of0r3uB4LDNZkryvdHK+wqMaMak49A/WgRGdyrS1Tl/58GZP0eEwPUKFmY9eN33ZwzagjxS/8s6lo0fLxQxd6WClbsHPt68r2PJXz9SwMzbG5ZdryLlRA4pMCIweEgCx0yAAkMBl0BBgaG2sVWcYF4W9bJ3q8Ckp1r69j0zfN6xRU+dE8BuFQsFD1PvVvtOEeg8WcFcwcOM5W3kvaVSYPQy4RkSOBkEXPPTYk8aep1vlzE3WUBhcg7l6yvheyaCH9Pmh3T5dTNNqoDSRwkPS9IIjNA/d2zr59DCendKlkzN6v0sFuwjGMGP9nmsDBgF2XhvBp6XctvY/QZumC1zL6yg/9rzKspmI5KkN5irDqDAUECYJIFjJECBoWBLoKDAGCAwFLduMtUIRjd3ugM/oE7Z3zKuSqhdjQbFBpatCw6DYOoHUL39bjR/dO95VQ+TJEAC2RFwzU+LPQOJmDdwFzwUr27B3+0p6YYny/4L8ObvWha2pREYSeX654MR5Jn3NiJvEz8YLfDfcyTiodHuKSntj/a0+fwKwrixcF/tVthTO9D+ohQTLn0Xn5vNR7xgp66ZqG/vHictPrdUzFMkQAJDEaDAULgkUFBgOCQwwjeDlz6371DV7cJODUuTHgpXtro7n/gjJWqxoRl5SfeXcoSlawAPSIAEjoOAa35a7BnY9pdVLHgpRlnlxXtf9P7AxyEJDOs7MgY2AEgrHNLm86sU4fVureu7rabst7E658H7U+R9TXrxeWr/nt3ic2vbeJIEThkBCgzVoRIoci8w+j3Z7z79mcHMx+qFfeGTqO4baKN5LcfL8mIp1Q86ufvVEmYKHgqvLWF9s4FWJIi0m3VsfLLgb+NYeG05/sIpXRDTJEACJ56Aa35a7EkDtv3lfDAF6u0VbDxqoq18WXVtCaUzxtcl+DJfYHgoTPVOa+p9Sm97Qh+MYBRfK/fubBVZDxLsdrXa408D4TCDhWvxtRfd3bHCMoItYlNOkTLvDHyyinnT7pkFLN+to9mOPAhqNWC4+FPLZhaxMeBZU5p+YB4SIIGjJUCBofhKoDgtAqN1v4LKrdpwL44b5omQZR3FUKMGwwwSyDs63ltAKSpWXi+j8vE6as1dZLEVovoKMUkCJHDEBFzz02JP6mabnaDWllG5Mo+YKHhtAUuDfJmsr4gIk34jsjafOIyP7rn/Rf8dsWJTrB61Um66EZLb76D1wCx8L2M+6uMNF7NeZejdAlP3CDOSAAkcMgEKDAVUAsVpERiqeUySAAmQwIkn4JqfFntOPFg2gARIgAQOiQAFhgIpgYICQ4FhkgRIgAQcIeCanxZ7HMFDM0iABEggcwIUGKoLJFBQYCgwTJIACZCAIwRc89NijyN4aAYJkAAJZE6AAkN1gQQKCgwFhkkSIAEScISAa35a7HEED80gARIggcwJUGCoLpBAQYGhwDBJAiRAAo4QcM1Piz2O4KEZJEACJJA5AQoM1QUSKCgwFBgmSYAESMARAq75abHHETw0gwRIgAQyJ0CBobpAAgUFhgLDJAmQAAk4QsA1Py32OIKHZpAACZBA5gQoMFQXSKCgwFBgmCQBEiABRwi45qfFHkfw0AwSIAESyJwABYbqAgkUFBgKDJMkQAIk4AgB1/y02OMIHppBAiRAApkToMBQXSCBwsVPZSqTJEACJJBLAi76Z2MT/5EACZAACQQEKDDUN8HVwGXsMp3FPzLgd4Dfgbx/B1z103nvF7afvonfAX4Hot8B9RN76OT9+/dRr9fRbDbx/fff48WLF/jhhx/w448/4qeffkKn0/H/fv75Z8jfL7/8gjR/v5EbzKcUZAo1hZtKTGWmUlO5McIYM84/CVycIjUORd5LAiRAAkdHwDU/LfYcXYtZMgmQAAmcLAJGaIz7jwKj1cJRChIGr3G/oryfBEjgNBEQn3iUfneYssWe08SYbSEBEiCBcQhQYCh6EiiGCS5HnVdsUqYySQIkQAK5JCA+8ah9b9ryxZ5cdgYbTQIkQAIWAhQYCooEirSB5TjyiU3KVCZJgARIIJcExCceh/9NU4fYk8vOYKNJgARIwEKAAkNBkUCRJqgcVx6xSZnKJAmQAAnkkoD4xOPywYPqEXty2RlsNAmQAAlYCFBgKCgSKAYFlOO8LjYpU5kkARIggVwSEJ94nH64X11iTy47g40mARIgAQsBCgwFRQJFv2By3NfEJmUqkyRAAiSQSwLiE4/bFyfVJ/bksjPYaBIgARKwEKDAUFAkUCQFkizOi03KVCZJgARIIJcExCdm4Y9tdYo9uewMNpoESIAELAQoMBQUCRS2IJLVObFJmcokCZAACeSSgPjErHyyrlfsyWVnsNEkQAIkYCFAgaGgSKDQASTLtNikTGWSBEiABHJJQHxiln45WrfYk8vOYKNJgARIwEKAAkNBkUARDR5ZH4tNylQmSYAESCCXBMQnZu2bpX6xJ5edwUaTAAmQgIUABYaCIoFCAocLn2KTMpVJEiABEsglAfGJLvhnY4PYk8vOYKNJgARIwEKAAkNBkUDhSuBi8FIdxCQJkEDuCbjmp8We3HcMAZAACZBASIACQ30VJFBQYCgwTJIACZCAIwRc89NijyN4aAYJkAAJZE6AAkN1gQQKCgwFhkkSIAEScISAa35a7HEED80gARIggcwJUGCoLpBAQYGhwDBJAiRAAo4QcM1Piz2O4KEZJEACJJA5AQoM1QUSKCgwFBgmSYAESMARAq75abHHETw0gwRIgAQyJ0CBobpAAgUFhgLDJAmQAAk4QsA1Py32OIKHZpAACZBA5gQoMFQXSKCgwFBgmCQBEiABRwi45qfFHkfw0AwSIAESyJwABYbqAgkUFBgKDJMkQAIk4AgB1/y02OMIHppBAiRAApkToMBQXSCBYhyB8fjeTbx1fhrTvw/+zr99Gw+ftTBqmWKTMpVJEiABEsglAfGJw/rUnXs3UX6njPJfH/b4451776NUnIDnefC8CZz9t/exuZPOb4s9uewMNpoESIAELAQoMBQUCRTDBi7Jv/1fJUyYAPXbV3H5nTIu/zEMWL8t496IIkNsUqYySQIkQAK5JCA+UfzuwM9nO9havoRJXzx48C7exk4rIh4efIBXzLWJV/HW8mf46NJkIDT+8AEeRvMlHIs9uewMNpoESIAELAQoMBQUCRQDA5Y10GziatE8/ZpEeWMnfEL2GJ/9uznn4dKqnIsENms58etikzKVSRIgARLIJQHxiWn99L13ggc9ExPhA5+YwHiKtf8Izpf+73bgt589xAfnAr99ee1pz2iHrlfsyWVnsNEkQAIkYCFAgaGgSKDQASRV+ukWPjPD7+98hHuNA5Fw7y9BoLr4VwoMhZtJEiABEhiawLB+evPDi3j/y208XC4FIxMxgSEPhl7BBw8O/PbD69N+3rP/uUmBMXQP8QYSIIG8E6DAUN+AYQNXX+Hx7Cke//0DlCaCofcPvj4IXn3vU6MaYpMylUkSIAESyCUB8YnD+FGTd9smMBq3cdGfOnURtyMPhp7+7bJFjNh9uNiTy85go0mABEjAQoACQ0GRQDFs4NL5u4HM83D23GXc/J/RRi9MuWKTMpVJEiABEsglAfGJ2u8OSnf9cnQE4583UbIIjNZGmQIjl98uNpoESOAwCFBgKIqjBi4d2J7+4x7urN7E1ddfCRd9X8Ltx/anX/penRablKlMkgAJkEAuCYhP1L5yUJoCI5dfFzaaBEggAwIUGAr6qIGrX2DbuhbO5V0aPJfXVo7YpEwdPflrG/UHNTR3Ry/iSO7cbaL2oI72r72l773soLPXe16fSZvPv29/D52XHezt61KYJgEScJmA+ESbv+x3ziownt3BW5YRjJ2/Xjy6EYy9Tl/fs/e8nugLD61f9vfQbpp6arG/Ros+8dAYsyASyDEBCgzV+aMGLj+o/b/PcMm8++JfP8JWZB3Fzuql1IHKFhzFJmVqPPldFZXrlb5/1e/CW9rrmPM8LH0TLwJoYPncDGZS/S1i44W+35J+voXlfnbdbx3ctF2B55Ww+vzgVHBUR8XzUPqirS+odNp84W2J9aliE5MdtB7Fg7MO1rG0c4ousWG8QAJOExCfaPOX/c5ZBUbrIT74vdmI4yyuVg9Gme/9JdhZ6pUPe9+ZoesQewZB6zxawcJMAZ5XQOFM8Dnz9jqanfid7bU5uy8MHw7F/IoSCNFr9ZYq2FTTaWL9yiwKXgFTr5dRub6CDVPG5jqWr5cxPxXYVfqkjk7qhy9H4QvbWJ/z4F2txeEwRQIkcCIIUGCobpJAoQNIqvSzeyibBd2xQLWDtcvpA5WtHrFJmRpP+k//5cfuBpbMFovnloLAEQagpgiCRIEB+CMAZrSg39+mmZtsEwJxk0wqCJTzWEkKgk8iwyiJP/jTCoe0+UI7E+vrbYf9TFBf8cJSX2HXFX53m/ZieJYESGAoAuITbf6y3zm7wGhhc+ms/xBo4i/3gh2jdtZw2fflryDN5hxiT79GdLYrmPIKmPu0gV354b5bx41SAd7kDTTkXNdvWnzsyzpW+z2w6V5bwtyk7cd5G+sXCvCmK6hFXK+2u/NkBXMFD8UPG/pSQvoofGEdlYIH79IG+piaYA9PkwAJZE2AAkP1gASKfkGq37XtT88Hay68CUya0YzJQFxMzF5N/VZYXb7YpEztkwyf/JRWYX3m30dg9Cn04NIQP8wDgbGEWiR4HhSkjhLLDYKXd2ZqwOjKFAqpRjrCehPrU3YlJocUNInl8AIJkMAwBMQnal85KJ0kMFo791D+XbCd+MTkNCZ/a44nMP3mGh5HRqOTyhd7ktsQ+uQ3q72jAq1VzHoe5u8e/IxOHMFIrkBdaWO15KFwrR4/v7eFckofWXvXg5cUQ+KlAjgCX/jtDRT9qWtzWJeHYz318gQJkICrBCgwVM9IoEgKJGnO7zy4jQ/euYzzv5/G+ctlvL98D9sjvsXb1Cc2KVP7JBu4YZ5eFW/A+vwpUWCkHOb+dH7IEYxhBEYQ5M2LCQ+mRKUNXmnzhegoMPp8h3iJua4+HgAAIABJREFUBNwlID4xjT+O5vE33/jyDu7c28ZTLRwaW/jsPy/iVfNg6PxlvH9rCzsp/bbYk0ysn28K/XVEDAQCw+YLk2uIX0mqbxfVP5sRkzKq1qdPQSntBxWUCgWUPk076ppUX9yq1KlOHZVpD4U/L6E87cGbW0U7zUOq1BUwIwmQwFEToMBQhCVQRINS1sdikzI1ORk+EfO8IiqPLNlCgTH/8VawuO9RG8H66SBIpJvys46GZXqvru0wRzAOBIeuRdJDBjlfYHgoTKVbd7IYecIY1DhkfWImP0mABMYiID4xa98s9Ys9yY0KfMXUJ7Yf7KHfjUxHkhGMlSfhdFXLxhfJdQH4NRipiI6KdPPvt1H7cA7FgofC5BzK3WlVFVTeW8DsZME/v3S32Tva0i1EHxyeL+w8WceiERczFdReAni+jvkzHrzZCmp9RJG2iGkSIIFsCVBgKP4SKCRwuPApNilTE5Id1K4W4f2pjCUz1/bcMpr6yU8oMLo/rN/YCKdSHV6QEOMOU2AUXzMLEvstZF/wpxoMFiJinWXNSbi+pBvYo2tReoL84fM6sIxHJEACSQTEJ7rgn40NYk+SvcAett61++PON0soqodBIjD0hhe7dxf7ThM1osGM/gYLyG0becQt3HvZRvNRuGbv0gpqjRY6L1Ns1xcvZvwpUmZHq8YGblwo+ovPZ97eQCtqxss6ll8LF8X/eRnV5i53/+vpA54gAbcIUGCo/pBA4UrgShe8DhrR+cYsJCxi6UHHf/JjFuoVLqyiFRUZiVOkDv8HcxAoy6juRhaOtxrhtogbWDGC4co8ZqbmsPpF0i5Su2gmLRK3nLfunHKAqP/RUNOmDp9Xf+N4lQRIwBBwzU+LPX17R6b9lCrY8H/It1D/fBEzBQ9T12qx0YIkgYFwe1vrJhzRByDixyKb9CXbdhi7NY3hC/eaWDEL3QtFzF1ZQS2mLOJW7zY2sPR6sNZOM4vnZIoESCBrAhQYqgckUJxEgdG6u+jvUjJzPRKsnlf94WZvehHrT8I5Tf0ERvgEzDwFG/iXYvvAzldL9idury1gyYiLj9dRfVBHs93B3oMkgaE66SiTEph7tsq1VRoE1amr67F95KPbRMaOu1PRbGXxHAmQQFoCrvlpsWeg/WZ60q0y5sOtwEtv3MD6drvnaXyiwBhYQZAhuH8R1e401sBXDfTpSX5/4GLvMX3h/l4Pg75NHTZ/38J4kQRI4CgIUGAoqhIoTpLA2Gtv4UY4fDz/uWXebKeF6oclFKYXsWGeaCUKDAXjuJP6x33qLRkTpk1F37ERaUsQfFMIqIRge/D+kBaqPVO2gmla1nUst2rcbjHSDzwkgVEJuOanxZ5R26Pvkxft2V5jEcsbvhej/jw6nwioXzPblFumx8ZuPswEfeFh0mRZJHAaCFBgqF6UQHGSBAb2O6ivraCmgoxq2kHSVYHx7TJmzi1gQxby9X2pVGTesGWalD9yEH3HxkHrj/hojKkCR2wZiyeB00LANT8t9qTh27qf8ECk52HFKupmkXO/f6Evj687i2yJ23PvHtrH9nLQNL7wqEdWegDwBAmQwDERoMBQoCVQnCiBEW1DOEd3L7rmInrdHGuBEaZHHj5PMVXKN2G/jcbdFZRfD3ZtmjojOzjNo3xrAw0RFtpea/ow5g2HBe/vYbdZR+3uSmwR+crdGhp95gNbzTqK/eDtFfEsCeSWgGt+WuxJ0yG7TXkhavLnxnsz6bYCtwqMYDRh+SubQ00rMMIHOGl9u7XhaQSGZaON7sYaZm2GB+/dap8Xv8ZHbqxm8CQJkEAmBCgwFHYJFCdVYKSau6sFhmIgL03ydHAZeF9PQQcnwq0GC68tYf1BE20TRPaAvU4H7WYN6++V/JfkzX7YQHfacPfuBpbPzSC+TezhCIzW3TJmjdCZXcCSvx5Egr5ZgF7G3GQB3pkSlh/1WtU1L3aQLqjGbmGCBEhgKAKu+WmxZ6hG9Mmcyo+b+60Co0/BqS8dhn8d1xcehg2pG8yMJEACh0yAAkMBlUBxqgXG/l7wRCjx4U84bH1oAqOF1T9ZdrNS7DtflVHwirjxrbpgHRUYP/js3l9AwZtCebN3kWXXgv0Omp/OoeDNYPlJ92yfg3GDap+ieYkESMAn4JqfFnvSdE9rbd6+8UW48HvGfJoHG14JepvanvKHFhgd1D9PM0VrCXPmZa06BvQY0O/EuL5wfB/fzzpeIwESOFoCFBiKrwSKUy0wVJt7k4csMDpVLMbezN1bo38mMVjaAlU41D/GOgt/IeR0ioWQ+zUsRe1/sYHF6I+B2HGwhaJ3Zir5R8TH1verJ0DhaRIgAU3ANT8t9mg7belgAfYSNpLWjnXPNw7eXp3kc2bMeyM89PU3xj91fU74o92886Jbj4za9n7Wvxswcptkl+8Tx/WFFBi27w/PkcBJIUCBoXpKAgUFhuXp1chTpIIRDK+0En8fh2JvpgUUvFmsfKcuWEcwdJ7h063PZ4OnhAP2ig9ehFVA+asw2MoIUHeucOQdH2nOmblh/EcCJDAyAdf8tNiTpkG+wBi47asq6dB8ziH/aD80u1R7/eQh22qrgudIgASOjAAFhkIrgeKkCozdL+eCN7lOBQup/eH22BP2yPm3NxK2TT3kEQzD+MUWlmbMy5RKWFqrotGSH+UtNDbXseS/wXUKi3dtv/ZDe/qNCsTauIiNF6pjbcn9XWxdnfHfHFt6bx1V/+VXYlfwhtvlN2aD65/UYy/CshXHcyRAAsdDwDU/Lfakab0vMIollHt2jeqdurSaeu1XmppNnvBH+7mF2IYWlURbqrB55LS1jZePAmM8frybBLIlQIGh+EugOKkCA0M9UUp6kh6+OVtPPwrfS7HRVNDSJvf30G4Ei6flRVNGAM1fqWB5rY5W9E20qkyzGNz69tqEEYO+u2ipstFuYONWBUtvlGLTmgK7amjuJnHSBTFNAiRwHARc89NiT6q2/zqEL+vjE1PVZcl0pL7UUt/op8L1IndHDTij18w7SYAExidAgaEYSqA4sQJDtYdJEiABEjhtBFzz02LPaePM9pAACZDAqAQoMBQ5CRQUGAoMkyRAAiTgCAHX/LTY4wgemkECJEACmROgwFBdIIGCAkOBYZIESIAEHCHgmp8WexzBQzNIgARIIHMCFBiqCyRQUGAoMEySAAmQgCMEXPPTYo8jeGgGCZAACWROgAJDdYEECgoMBYZJEiABEnCEgGt+WuxxBA/NIAESIIHMCVBgqC6QQEGBocAwSQIkQAKOEHDNT4s9juChGSRAAiSQOQEKDNUFEigoMBQYJkmABEjAEQKu+WmxxxE8NIMESIAEMidAgaG6QAIFBYYCwyQJkAAJOELANT8t9jiCh2aQAAmQQOYEKDBUF0igoMBQYJgkARIgAUcIuOanxR5H8NAMEiABEsicAAWG6gIJFC5+KlOZJAESIIFcEnDRPxub+I8ESIAESCAgQIGhvgmuBi5jl+ks/pEBvwP8DuT9O+Cqn857v7D99E38DvA7EP0OqJ/YQyfv37+Per2OZrOJ77//Hi9evMAPP/yAH3/8ET/99BM6nY7/9/PPP0P+fvnlF6T5+43cYD6lIFOoKdxUYiozlZrKjRHGmHH+SeDiFKlxKPJeEiABEjg6Aq75abHn6FrMkkmABEjgZBEwQmPcfxQYrRaOUpAweI37FeX9JEACp4mA+MSj9LvDlC32nCbGbAsJkAAJjEOAAkPRk0AxTHA56rxikzKVSRIgARLIJQHxiUfte9OWL/bksjPYaBIgARKwEKDAUFAkUKQNLMeRT2xSpjJJAiRAArkkID7xOPxvmjrEnlx2BhtNAiRAAhYCFBgKigSKNEHluPKITcpUJkmABEgglwTEJx6XDx5Uj9iTy85go0mABEjAQoACQ0GRQDEooBzndbFJmcokCZAACeSSgPjE4/TD/eoSe3LZGWw0CZAACVgIUGAoKBIo+gWT474mNilTmSQBEiCBXBIQn3jcvjipPrEnl53BRpMACZCAhQAFhoIigSIpkGRxXmxSpjJJAiRAArkkID4xC39sq1PsyWVnsNEkQAIkYCFAgaGgSKCwBZGszolNylQmSYAESCCXBMQnZuWTdb1iTy47g40mARIgAQsBCgwFRQKFDiBZpsUmZSqTJEACJJBLAuITs/TL0brFnlx2BhtNAiRAAhYCFBgKigSKaPDI+lhsUqYySQIkQAK5JCA+MWvfLPWLPbnsDDaaBEiABCwEKDAUFAkUEjhc+BSblKlMkgAJkEAuCYhPdME/GxvEnlx2BhtNAiRAAhYCFBgKigQKVwIXg5fqICZJgARyT8A1Py325L5jCIAESIAEQgIUGOqrIIGCAkOBYZIESIAEHCHgmp8WexzBQzNIgARIIHMCFBiqCyRQUGAoMEySAAmQgCMEXPPTYo8jeGgGCZAACWROgAJDdYEECgoMBYZJEiABEnCEgGt+WuxxBA/NIAESIIHMCVBgqC6QQEGBocAwSQIkQAKOEHDNT4s9juChGSRAAiSQOQEKDNUFEigoMBQYJkmABEjAEQKu+WmxxxE8NIMESIAEMidAgaG6QAIFBYYCwyQJkAAJOELANT8t9jiCh2aQAAmQQOYEKDBUF0igoMBQYJgkARIgAUcIuOanxR5H8NAMEiABEsicAAWG6gIJFIciMJ5t4aPz05j+/TTO/9dDjFqm2KRMZZIESIAEcklAfOKwPnXn3k2U3ymj/NcEf/zsIW6/V0b5nY9wr9FK7bPFnlx2BhtNAiRAAhYCFBgKigSKYQOXLf/D66/A8zz/7+zSZupgpcsSm5SpTJIACZBALgmIT9S+MjH9bAdby5cwGfpj7+Jt7LSUgPjnHZRnJ0KffRZX/66u6/yRtNiTy85go0mABEjAQoACQ0GRQJEYqCJBpW+ef95ESYKZ54ECQ4FmkgRIgARGJDCsn773TiAcJiZCAaEFxv8L/fXEBCZ8v02BMWLX8DYSIAES8AlQYKgvwrCByy4yHuP2RRPIJvHq7FmOYCjGTJIACZDAOASG9dObH17E+19u4+FyKRih6BEYn6H89mfYatxBmQJjnK7hvSRAAiTgE6DAUF+EYQOXTWDs/O2y/xRs4t9u4s7/DQIaRzAUaCZJgARIYEQCo/rp7SSB0R2ZvkeBMWKf8DYSIAESiBKgwIjSADBq4OoKjf+9h/LvPHgTr+KDr1uQgEaBoUAzSQIkQAIjEhjVT4s/tq7B8EUGBcaIXcLbSIAESCBGgAIjhmN8gbG5NAnPm8Cr1zbxtEWBofAySQIkQAJjE6DAGBuhuwX82kFnz13znLJsfw8dwnKqS2jMAQEKjAMW/tGogcsfwah9hFfM/N3fvYW1f+xgp7GDrQ9fDdZg/OUOdhpPR9pJSmxSpiYnjdN52c9J76H9qIZacze5jL5Xxr1/F80HNdSfn6AostdB5+UJsrdv//EiCZxsAuITuyPH3SlO/Xd+OikjGHvP66g9aGJUD+337m4TtQd1tH8dta+Pwk8HZSbGnufrmDuzgI0Xo9qcs/v221i/MIWlbzo5azibexIIUGCoXho1cPmBbqMcbnEYbE0rW9QefF7E7SH2VpfgKTYpU3uTL+tYeWMWhcjuVYXZMjZaOmsb63MevKs1fQFAeC1ShrF/6ZtoVtv9HdQ/r6DyeR2DXV0dFc9D6Yt2tFD/eO+7KpavV1BJ9VdFT9O2KwP6INo3FdR7LLCfaK/NwfOWUNu3Xx98toOWEXUPUv6NLP4GW8IcJHDSCYhPFB+Z9tMFgbH7aB1Lr89gZqqAwtQMSm/cwMaTuNfs52/aD25gbrJw4OfOzKK81uz1u74vLGH1uaW399uory2jbOw4N4/y9RXUeh74JPvpeIltbH3cx2d/2cDBo5mgTGvs2W/gxnQBC3fjsqrzZB3l16bCuFbA7BvLqB2GANkfIHYApK179+4iZs4Zlmn+ltGIA0Tj4zT3HeRZ+jryffluBbPenL2fVT1MksBxEqDAULRHDVx+gHv21B+1MCMX8tcdwXjHjGDs4Omz/k/YbIFSbFKmxpOdOirTHgoXVtDYDd35XhvVd6fgeXO4sRn9YbuBpXNJAiNerD1lExjhudIqemWDLiU5cAWBdRYLqQTGBpoj/uDvF8C1tSY9bP7eMoI2Fy8spRNPd5u9RfAMCZCAT0B8os1f9juXrcDYQ+vzORS8KSx+XkOz3UGn3UT1w5L/43ku8sAlyd+Y8/79aw34bn5/D+0HFZQKHorX6pEf8gCSBMbzdcyf8VAoLWHdjwtVrL9nbChgbi3qvZP9dPxrGOSbeW+j5wHKyiUPXiwmBHltAqP16Sy8Py3HfHpnu4Ipr4CZq1W0zKh8q4ZKyYirOaxHTY0bNDjVaWL1UijSrA/agGHqDvqrjKqxMcXfno5bZlpYivs6T1b87e/jD/z2UL9WhDe3niL2DkbDHCRwWAQoMBTJUQNXUlCTgHbUi7ybn0zBKxgHpxq038LKnzx4k3NY6v5oX8LcpBYY4dOcAU/Yg2lNRy0wxhgpkOlhA5x185bZ3St9PUkBX9Huk0wbrPsUwUskQAI+gVH9tPjjTBZ5t1Yx63mY+7xn3BXtL8wI6cFog9XfvKxiseBh6kP9/BvYvTsPz5vByneRL4hVYIS+e24VLfUjNxAv0TLS+qzkfPVrKQXG7gbmPQ/zX0ZHL5pYnvbgaVs7WygXPHhXtuKCKtL0foe739xAyQisS0soJz5oG65ua3/1M2LUa89XUfJmsPxEFRB+t8pfHYwVqRxMksCxE6DAUMhHDVxJAuPpP+7hzpd3cOfrxyOtvzDlik3K1EgyDBqXNqxzdttf6B/TNoHQQrUrQJKHu1e+MQHAdn94Lva0KmJi7DA5II3tqP2gWkBx5mA4OXHY+rUVNFSQjZkZSYxtF5LbHKmGhyRAAikIiE9M8rtJ57v++N62vwlHb77H2DL++ss72HqcfrRZ7OlneuBDFlGNzG7p5n9ZxYIRH+EIgtXf+L7N8uPSFKLu98u1CYwX65gzP+TVNCQ//34NS56HqU9k9DStz0rOlygwrlSx6z8ECn4Qtz6f9R+QbUXZNG6g6HlY3IyeDIg1P5mB581jI6pHgksD/mvWLMxg0YwAmfULSVOFh6zb2l8DLBnpst+ntnbvYsOMFl1Yt/4GGKku3kQCYxKgwFAAJVD0Bp70weaw7xWblKmRZOgo36z2zsM1ciCVwIgUZ4bdm2aRoZlWVUdTplx1s9gc82EKjHksfx2d0pV0bFkEaQuqXbtHPwgCSErhcm4Gy9/qupKDsM7JNAmQQH8C4hMP29eOWp7Y08/qwIeU7eu4wh/3si7N9oO1s7kYG+WI12XxLzZfGP5wjk+xkZLaWC1FR7YtZUrW2GdyvkSBcWYqXK9wA3Wz9sKMqKv4tfulGdVJmAr1qOJPKys/iBkyZMIWx4Iihq076K+0cWv0hfd+LC9UULc8GAtsiI5ADYmD2UngkAlQYCigEihGDTRHcZ/YpEyNJRsfFeEVl1Dr2TEkfLJRLGHxmoxM2KZIBcV1vl0Oho+nzMK/CipX5jHlz9e9gUb3QZLNMR+mwBhjDYYfVKew9GWSKImfT72TlWXqVTDNKmHebc9IdXIQjnUkEyRAAgMJiE88Cn87SpliT1/Dt82P4iIqj3pz7W1XUPSKuBE+mLAJDPQTB9+tYEY/7bcJjHCkQ4RMzJJOFYtmFKW7FiStz0rOlygwouseQttl9EZs8u9NmsbaDkZirO2QAgZ+2uJYcNOwdXceraZbW+fPElhBbeiRl8Cu2lUPXsJMBSRwHIiBGUjgiAhQYCiwEihGCTJHdY/YpEyNJ43DNQv9rtbQiTzdaH0RLAosr0V/WCcs8v41mNs6dS1eBsIF5AdzXkPH3B3m7mBPhpuHmCJ1sLvWgLnH8Zb2T7W3htiFqoJgylf/IpOuWn8EJGXmFKlEMrxAAsMSEJ94VD532HLFnv7tMNNzCvAmy6hGFijvfbeBRX+DjnW0Q98tT6MXwodCq486gL/LUrAmQfIF9XVQu1rsnWJkExjYQ/36FLziIqrRnZj2O6hdM+v4olvEJguHeDuT86URGHtfBbsv6lEV/95z8UXf3XrViA/2d/1dsSrXV1D9rvskDMAeGvc30Iye6hYyQGCkrbtb3vAHw+0+NYOiWXtSKHZ3q4qNlJv4bXZ/jIq34U3iHSRwaAQoMBRKCRTDBpijzC82KVN7k60NLM4UDhyQ2crwTAk3HkSimX9XgmMNnwppR29uiQeK8P7IVrZL34TnhhAYtqdPw/1o70VwnGeGszUIwlNX13t2WrFuXfuoPdICxuNsP+sigawIiE88Sr87TNliz0Ae+23UPpzzfygWJmcwY3x0oYi5D2toR0Y9A99yMJLbfRBi3hNR8FCYWcDy3Rpqm+tYes2UMYOKfheCVWAA2G9h4+0Zf9eo0hUzql0Otr09M4/V2Ha5ycIh3s46KuaHbyQexI5juxsFZUZ/BAdtPRi9kbLjMUfOyme0nEC4Tb25iurdYAH31LtVX6x1vqlgKiaa5H7zmRAHe+Jd9B5zfFD3bjP64G7Y4zraAzYjGbizVOQ707UraYRDN4NpEjhiAhQYCrAEimGCy1HnFZuUqYnJvc7Blnc92+H5dyU5Vtk5YxnNyG5UZms8E9RmP5XdT2z3H6bAKKO6e9CGfk5W2jfskyC98Dv2JChC1g9ySYFz4PmDkRnAtoh+wd9Vxrp17a0aF+tF+oGHJBAlID7xqH1v2vLFnqiNfY/397D1rtlhaaVnNydzX9+HF3vBOyyW3ihh5vUyKreqsL42J0lghIbt7TZRl10DGy2ILz2wO63AOLhj8NHBj3PJ21dgTC9DlpxL/uAztM3syNXewEIpMtLxso7lC8XgvRlnSlg2oz/Wf7Y4FpZudr9KUXfrvkw7HuVz9KlS1uaI8KHAsOPh2WMnQIGhkEugSBtYjiOf2KRMHSvpi5BO7PFHUN6LGm6Eztk8YTPrL7wzU5g3T9i6U69sjvlwBMZwQmEBGzI4479pOypKqv5wcelWM7a/eOK6CQuKsQCnuvkoAniqipmJBE40AfGJx+F/09Qh9gwDtd8T+r4CI20lAwTG4GIOwT/573eIOtdegdF3itQJWYMxmOWAHN8uY+bc4uA3mPfLxylSAyDz8nEToMBQxCVQpAkqx5VHbFKm2pO+A0qxRat54+jHvfupdwuVOa63ZNSieyVhaFkJjOgP/nbwpKy6tuwvGl+8u574Ju9oLeMd24Pj2IF7fw+tRg1+W7rb+i5jfdO229agFthtHHQXr5NA3gmITzwuHzyoHrFnmH4ZXWDsof2ohpp12CJigQMCIxgBju6c1SswZHHyrHo/SOCrs9lFauS693bRfFTFunqz+crdGhqtqNCK9JM5TNtX/fJxkbeCymTWBCgwVA9IoBgUUI7zutikTLUnQwe08iT6NF8fN7ES247QVlTyj9898xbUnv3Dd7F+wT4PtzBlBE8JC+9VsLxWRb1VPTqB8V013M0jnH70Wjm2u8fShaL/Qqpg8eQq6pGpYDYK3XP7HdQ/MXOnCyheMNMSIm+t3VzH8nsLmDW7bc0sYsOmyboFRQ+SGUdz8ZgESCBOQHzicfrhfnWJPXErI6noA5dw3n3VTJGaXsK6TFMyfiTcuW/hktmiNelFoLYR5Ehdcqh/jHZ9o306jz/lyjx4mgmnF5VK/lujbWvlpIr+n6GdCTtnHdxr36ZWds2yvQej9emo78E4qLXfGoyh61bxwcS5g7V1G1gJ44M3nRAf/L4qIlgTY++fivluvDGbuFVxIIpm4y9bjDaXxyRwzAQoMBRwCRT9gslxXxOblKn2pA4q1lw6QO36i/7i6xKmwjms4X7lZjGirDk4M4e5gQLFWnF48gh/WPvtn8HSfw9YcPepefNtdI1EP3uBxkdmh5U5rDT67C+430b13Sl4XsLLtHqqOEIOPXXxBAmcHgLiE4/bFyfVJ/YkEU6e+hk8ePF/PF4PHsCYdw9tfaxfjhotWfvv6LXIcbi1dndthU1gRB+UPKih3mwHU0r96bPj+afON0soelOYCt/GfTDFNmJjeOi/aE/7Tdk1q6TeOh6+ybvw5+qY69T6cByy7sD+OSz3jQ+7qF4xD7jK2NLbyY8dt8Lt6C+tY7c7lbmXM8+QwHESoMBQtCVQJAWSLM6LTcpUe3IkgREOucuTNP+zgZbe4aK7ZqOPY7Zbpc72Bq5DW0ydqv1DDEn7loftfcP+IsNY456v+k/9ujtxvdjAonkqaP1TIs6Wp980tljFTJBAfgiIT8zCH9vqFHsOqwf6T+Uc1/+mtbLXT6e9s/PoBkoFD2bL891WuIW62dY86cfvbhULBQ9zX8Yf4HS2K5gybxd/txrEo1YNlZLZNWsOq6lHipOs7s8xfd1hOWl2UPTjk4dufBDTxo1brVXMegWUv0pa0C4V8ZMEjo8ABYZiLYHCFkSyOic2KVPtyVSOqr9jtRccPTvu/aMHrqgV1uNU7R9WYJgdXwrwimXUBvjv9hdmakNkmNrygr5+u2L1XOuKOmtreZIEcklAfGJWPlnXK/YcVmecWIGxZ7bhLQVb4H5U776TqfNkFfNmw5DZinpPxQGx1qez8CZvoKFEyO43y1j4Uzhty7wD4vUbqD3vs57hoMgBRx3UP6+gcte+T5W5OW3d/gi3GYEZMOW2+YmZ4jSP9eg7SExFY8WtPdSvFVG4cPAelQEN52USOBYCFBgKswQKHUCyTItNylR70ndUg+ZyJr/J216oPuu6wBjU/v5zWXVr/bRZd3Ip2G9+4ZONg6kE/ihPCw2zH73Zfaswg8W7Yz9as5rAkyRAAgEB8YlZ+uVo3WLPYfVPKoEReeGafYQ0HDkdeRR0iAdBuzUsvzHrT6s169DWbVOFXjaxfkXyLGDlW/W0Zr+FlT950G/0PiymR1qOmR4/EOOtAAAgAElEQVT7ppkeO4X5pPgQvq9kaVO2PoxYlCpuJ8St71YweygjOhF7eEgCh0CAAkNBlEARDR5ZH4tNylR7stM62Ns8NuWpd01CPfbGU3tx9rODn/zY75OzwTshui+PktOH8TlE+2sPGpGtd9NV3vmu5u8QUn49Ou0pmEe9creBlp5bm65Y5iIBEhiCgPjErH2z1C/2DNGEvlk7j1ZRub6Bpnqa373J3/5Vb96RkB55FHQ4P91p1VFv7lrep9G1Ojjw37+R4Hv9Fwkm7BylinEx2fmujo1bFSTFh+jLFGP2jxq39s1LBi0vWYwVzgQJZEOAAkNxl0AhgcOFT7FJmcokCZAACeSSgPhEF/yzsUHsyWVnHHajjXjig5p0VP0puIcxXSxddcxFAsMQoMBQtCRQuBK4GLxUBzFJAiSQewKu+WmxJ/cdQwAkQAIkEBKgwFBfBQkUFBgKDJMkQAIk4AgB1/y02OMIHppBAiRAApkToMBQXSCBggJDgWGSBEiABBwh4JqfFnscwUMzSIAESCBzAhQYqgskUFBgKDBMkgAJkIAjBFzz02KPI3hoBgmQAAlkToACQ3WBBAoKDAWGSRIgARJwhIBrflrscQQPzSABEiCBzAlQYKgukEBBgaHAMEkCJEACjhBwzU+LPY7goRkkQAIkkDkBCgzVBRIoKDAUGCZJgARIwBECrvlpsccRPDSDBEiABDInQIGhukACBQWGAsMkCZAACThCwDU/LfY4godmkAAJkEDmBCgwVBdIoHDxU5nKJAmQAAnkkoCL/tnYxH8kQAIkQAIBAQoM9U1wNXAZu0xn8Y8M+B3gdyDv3wFX/XTe+4Xtp2/id4Dfgeh3QP3EHjp5//591Ot1NJtNfP/993jx4gV++OEH/Pjjj/jpp5/Q6XT8v59//hny98svvyDN32/kBvMpBZlCTeGmElOZqdRUbowwxozzTwIXp0iNQ5H3kgAJkMDREXDNT4s9R9dilkwCJEACJ4uAERrj/qPAaLVwlIKEwWvcryjvJwESOE0ExCcepd8dpmyx5zQxZltIgARIYBwCFBiKngSKYYLLUecVm5SpTJIACZBALgmITzxq35u2fLEnl53BRpMACZCAhQAFhoIigSJtYDmOfGKTMpVJEiABEsglAfGJx+F/09Qh9uSyM9hoEiABErAQoMBQUCRQpAkqx5VHbFKmMkkCJEACuSQgPvG4fPCgesSeXHYGG00CJEACFgIUGAqKBIpBAeU4r4tNylQmSYAESCCXBMQnHqcf7leX2JPLzmCjSYAESMBCgAJDQZFA0S+YHPc1sUmZyiQJkAAJ5JKA+MTj9sVJ9Yk9uewMNpoESIAELAQoMBQUCRRJgSSL82KTMpVJEiABEsglAfGJWfhjW51iTy47g40mARIgAQsBCgwFRQKFLYhkdU5sUqYySQIkQAK5JCA+MSufrOsVe3LZGWw0CZAACVgIUGAoKBIodADJMi02KVOZJAESIIFcEhCfmKVfjtYt9uSyM9hoEiABErAQoMBQUCRQRINH1sdikzKVSRIgARLIJQHxiVn7Zqlf7MllZ7DRJEACJGAhQIGhoEigkMDhwqfYpExlkgRIgARySUB8ogv+2dgg9uSyM9hoEiABErAQoMBQUCRQuBK4GLxUBzFJAiSQewKu+WmxJ/cdQwAkQAIkEBKgwFBfBQkUFBgKDJMkQAIk4AgB1/y02OMIHppBAiRAApkToMBQXSCBggJDgWGSBEiABBwh4JqfFnscwUMzSIAESCBzAhQYqgskUFBgKDBMkgAJkIAjBFzz02KPI3hoBgmQAAlkToACQ3WBBAoKDAWGSRIgARJwhIBrflrscQQPzSABEiCBzAlQYKgukEBBgaHAMEkCJEACjhBwzU+LPY7goRkkQAIkkDkBCgzVBRIoKDAUGCZJgARIwBECrvlpsccRPDSDBEiABDInQIGhukACBQWGAsMkCZAACThCwDU/LfY4godmkAAJkEDmBCgwVBdIoBhZYHz9GcrvlK1/n/3PU4xSrtikTGWSBEiABHJJQHzisP50597NwDf/9WGPL3587ybeOj+N6d+bv/N469YWdlqtnny2OsWeXHYGG00CJEACFgIUGAqKBApbEElzbnu5BM/zMFGUQHXw+dbfdlIFK12P2KRMZZIESIAEcklAfKL2lYnpZzvYWr6ESc/z/bN38XZMPOxslINrv30Vl995Cxf/ZSLw45fX8DSFyBB7ctkZbDQJkAAJWAhQYCgoEigSA9WAYCMC4+zS5khiwlav2KRMZZIESIAEcklAfKLNX9rO3XsnFAwTwWdcYDzER+eM8DiLt+6ED4F2buOSL0bO47PHg0cxxJ5cdgYbTQIkQAIWAhQYCooECluQSnOOAkMBZZIESIAEDpnAsH5688OLeP/LbTwMR5i1wLjtT2v9CPcaIibuoewLjFfwwQM5l/wp9hxyM1kcCZAACZxYAhQYquskUKQRE7Y8IjC8305i8rfBcPxEsYT3q6NNjzJ1iE3KVCZJgARIIJcExCfafHC/c13/rKZI6XuebpRx1giM313F5rNkYSH3iT257Aw2mgRIgAQsBCgwFBQJFBI4hv3sBrCJaVy89hnu3Crj1Ylg+P1qdXCgstUnNilTmSQBEiCBXBIQn2jzl/3Odf1zP4Gxcw/l3wU+++Jft1NNdRV7ctkZbDQJkAAJWAhQYCgoEij6Bal+1x5++Iq/OHDynXvdxYGPb50PFhZeii8s7FdO9JrYpExlkgRIgARySUB8YtRPpjkeLDB2sHY5WKcxeek2tlOMXph6xZ68dMZep4O9/by0lu0kgdEJ7L3M7/8rFBjqeyOBIk2wSp3nHx/hVTPcXryKzQGLxG1lik3K1MHJ/T10XnaCv87e4PxD5Nh7XkftQR3tX4e4aUDW3WYNtUdtJFnqX39QQy3y19yNFzqojHhundpF05StC9XZImnjPA4ZbaR0HpIACfz/7Z3/ixtHmv/3b+tf0uAgGBhuFt0aZgknMBlM8BCwzh8yCWtdDouwEQ6eCxetIZ8xH+8EBgUfsjGaJUzAkdk5+SCImFPIIkgQGBScKAkIBt4fqrufVnV1daslzYx6rPfAoK7u6qqnXiU9Tz311UZAdKJNX6bdS3cwvsGjP635u0dt3sLhN9lHnEUem6yv2r3RcQ3rG7vo5sjB8O1RD4Y5ODf0p5H/Yrbr3IqaktEYg2ez2c9YYr8N0HnSxgwmOJbEQjeGvcXbNUYZBgdbWL/VxmghwS7my3QwjHoTQ5FmpJKffYNH/6HOwPgzDv6uGae/3fLn8/7zR/jqrB2MkwE6B1VsrblwHBfrl4soqv8gXLxeR+s7+1fdV5LRBvykMR91JtSPxnFK2P/eAGgLngzRvltGad2F4xaweaOO9iAesfOhA6e0D8sjL3L/cQ2129H/Zi+azrQ00p93UFOO4K12NNHEkB+/9FmSxIkvhg+Gz5N4W+6nOF9hgrwggRUgMK+eTnMwvvrPEl5TW4xvVvDgfzT9nUFnizyLo++ifrmInYfLaipPKcGwhbK7jt2v7fFG/VPqeAoaaRP74+vDTn9iu3Rd7tujKtoRp2eEvmrwah1SqdcJrdrhswaq14oorrtw14so3dhF8/lEDkXCnj+AkwHad7ZQcP31mGoLe3ezgobxvkpDL4+d7vS7o+cNVK6sw/U2KHB9W/ti+nvTYwzQ2JqUQZVD/Vef6m8GcWz2U28DqHbJtV1rGwCDBrZi6QLj71qoG7bfbAtMwi30dbGC61G/hfqNEorFAty1IorXKth7Ytju41piu8Zju6naVQEHt4CtO20MI9859WUwynDSx94bDrYWaCdYinMhbtHBMKpJDEWyE5FueA6rr/s9YH96FMzd/QYP3vHv6dOmZklfZDJEjQdHHdSKLpyNHTS6g/gQ9rCH1u0SXMfF1oHxwwIwerYfa8B7P9pbWygYzkR2B2OEzofrcC5tY787wGjQReOddTjOFhqGCEkK1hslkJGYpM9gJCUpDYGV/vy8HYxAIW+W7dxNhXqvvbQeOuHHTxLIAwHRibPoURU3ycFYxLlQ6Yo8i7NZvNNicRmSUhjh6KYL92YLI7NRBWD4RRVFryGdseMpKRt1/2UH+xH9pzrNop0/ui63N/B9loWr1Wz69aHRW4Ux+ve34Drr2LnfRm8wwmjQQ+uOsqHRBqM1/5MBGlfFHg99ezweoO3Z4AJqx9Gxer08aWiSnnkjS46L4q0W+spO9tuolVSDOG5rk9JY7H6Sg2G2AXpovu+3AWIdlGbjPBDI57uJcuQ7Ee1snDgYTfSM7+fwywrWHRel2010+2pWRx+d+zve97Wgjy4kOBjCtnS7jX7Q1hh2G9jZcOBsNaKdopYyjI9rKJxbPSxWi6f5Nh0Mg6YYilkNVxg/XCDo4LW1DWz80+tBr9hsQ+5hejMYr+6dAhy3gtZLo1CRYPBjd7bRzNpJ5v1gokYjs4Px9S4KTgHVp1qPz0kXu8pY3DyKTIeyK9g+WlmUSmAc7GkIgBFa1x04G3WYpsSPsSQHw9bjIyLzkwRIIEZgXj1tdTD++y8oSa+k5fOt/zd9B0CRJybozDdy7GAEurz2zCjUqIfGe0W4bhHV97cTe4CNt2YMxhuvuq63NvCxIMv+PjaVI3E/3h8++Cw6gm/Lf3S44zkn8dGeIZpvO3Au70V62vXyzAgHQA91r7G7j77euB4doaKcPsPWZk8/mDY8ZRSo871yluJ15OUjbYAnehvA79WPyWVpnKs0bHwzl+E3n4HnSOhsVKfq02rQPgmcPauD4bN1rzfjoxVe2RxUvtCcRWsZ+th/Y5F6yFzaXEWkg2FUhxgKvYE/8/U/jvHgTgX/+uYG/njtXVTu/BVfhfurp4+A2PISmQxRjWDw477amN7L7f2IzOFNIzk9uICD0bntwrlcj/Uo9D8twnGiQ9pTFexvA/T0Ie9uPzZKk55GH3vegVpJzhUdDL3aeU0CeSUgOtGmL9Puffv3R3jw+QM8eHQcbsLR/99jPFL3Ev4f/f3bqTtJiTyL81qwUby4AAkpjHH0vgvnjb1oA1bFPq7BvbKLtmpkWhtokmRgowy9L0/TP+ONV13X2xugi7H009xBS2sXhzK+bKGsnI9gJoAtf08+i+1TaYwel2MjC3p5wnyyXnRVR56DncO4sL27ytbqNm+WesjWwbf3VPVWxutIiZ/UBrDytTbOF3QwvO9kEfXnNphBZ+eHHf+h7fsbyGRjCxjvq1QSyjB8qJzvLTROZcqarSz5u0cHw6gTMRRpRuq8n4lMhqhGMDAAhRo6hpduRIR80WPDk2ZECc/tYAQK533Lmgbvh7yJfa1zKFnBjtC960/tKt2soXavgcYnNVSuFvzh68eTuVbJaQAIeqTUHEr7uonAwbgsU5bsczkFC4IeMufSur/ORda7JH2+1zScP7tCnqTPKxIgARsB0YnnrYuT8hN5bLLG7qnNNwY9dHsDjGKbZCQ0ir0NO/rodvsYvdR6S43E1e5Og17Xn86TFC1jWpGkgwZ18a597DeMa2ughQ9H6NxX01riU1jCKIkXcV2p63pbA1/0s13XJ2YUPvDTrBjrOoLHJ21UNTsSz3+E1jspawotnPTyhEJkvBh+rkZUEqZCPat5U7oqTySxOevhZIxBT62xUetaOugNzS9YvI6SnA5PEs8pKkTX8yQ0zn2+26h/mWVNjbHY39LWEBJKvv2SNvXOUi94Xse6ZV2In4alzAllEMej+KnW6JkI8kpe0cEwqlUMRZIhWcZ9kckQNR78voEt10HhegN987cfxB4e76LkOlj/sB2fRzsOdpwy1zk830NprjUYgbG0DDHLj01fJOYp2I0qGsFwrD/sOnEMKl/GC+X3ztQQ9D/4C+W0xe31cDGiP3/Y2aihrtaEOFuWBeqBgxGuichgCH8zmbW8E4BL93qTHbxCnqb8FuUUr1XeIQESMAiITlyGPrblKfIYYsaC/Ydq3reLQrGE0pWit/h3/Z0WBmGnUNzBGDyevFO+sY31S2qa5w5a+gYbox7233ahOjtKN8rYVgtZ1Xz8253I7jX+OgmV/zbKN0p+Wpe20dDTikkNjL+oeJuG1I4tD/Vbtgaa/nzua9GVNewGHTgeh2BTkHgDX2UUZzlT9mpkxikgNiUMgD+nftI4tuXf/bgQG6WX/P0R/OjoyCIOhvdu0shQ0OCd19FSMo++rqN0yYG7vo2KmrJ80/8euqVddMNBE6kjvUNxtjaArV2g8vf5zrkGIyj/1ueWOeHBs23ZVMH2/Q2cays/Nf3KcRBxvJMcDAQzKMw1G/KleAU/6WAYlSqGwmZElnVPZDJEtQcHbex6PfsuClcrk8VtH5SxqXaSurSJykEv7lyEP2Jt56lIT3wZzclAQfCDj67LiAuUouAtP0JPSbqFcDSg/F9Bhr91UNtQjtMe1C4i3v7rY9VT1/IWselzI30lvYeeNOqDNr1SUGqxXk2tBZHFd2tlNCKdCb682XeRipd4NqNmU8i2NHmPBEhAJyA6cVk62cxX5NFlNK9HX1R8HfREa+i8bPu67U43iG7oTNs6gJMe6sZ87v79TThrVbS19XcyvzxsIAdz0SNrCk4G2Fe7A02ZWus3iKfpe3+6VObdBU1AkXCgi401Ma7qGAt0e+v9yQiBrYE/my6OZB4EgkXaaxW0NNs3/q7pLe51rzZCx9DPv4jyh/7C4/1nI6C76/V8x3YPGrVRXXPg3jyKOH8LOxgJ07FgjLbYSpp6L/jexDol1aYyat1HuL4jsGc3WxgGdTQ+Mb7PekZBG6D8OPRQEqcX2etXTyzteozObb9Tsa7v3jXsYFctgt+ooSMieA5GAd5MCeVIPVYNhCFa113/9yXxguz8tTib2PtOy9/StvGfjnF0U+1AFZ0arr35yl3SwTCqVAyFaUCWGRaZDFHTg14DXIYz1bBi19tZIu1wpFl+xH7caQang5oaLbENq1t+hKkK9mUfrXsVbF/2e/28aUlXyth92I0svIqlobbHk52zPtO8CbUw0dvNSu0Q0sHAc0SW5GBcq+NoyiI6f1jaGPpN/wbwKQm8sgREJy5TL+t5izzJwP3ey8iONUHkwWclOI6MwkYbZONB15uO0tccB/Va9J1gi9OYk+Av0A23dvV0brxHPjzDIRxFiZfC06uRefzxON4dWw9wQtT02z4HfYTbjK/rervt8tNYv9XItlWtbRtwbZtZb2tT1UkXbE/q2wxfKj//SQ+7vybB73n3RpKu19F80kbroOqPBBRrEWdQpaKXxyzrtHD6uwvaNYutFnmi+QYOhuYUVp/6eW9mnMWQPoKxQMP8ZITeQQWbavRPTWlWW+Y7LjZvNtDV/H1/DVEBW7eCHapkZzFxpi6VUD1oof2kifr1ojdKuH2gtSsUmGm8svyOBPAF/6SDYVSgGArdeCz7WmQyRD31oF1J27PJ5mAECidxDUYp4xoMXYb0Xn9d4Y17+9j2DILaV95QAkGSgyc1lEJln1ERBwok3A9bU6iZ7oW7RslcWH27vWA7xnCalv4sw5QtHRWvSeAVJSA6cdm6WfIXeRJxf7+PkuMa5wbYYvs6yD4dYzIds3dPOSWTBtfwcdmbzlP+tBNuoxlLXe3ep3qcN2to9YJtU2OR7Dc8vZqlYZQrByPbAuVwe9O0bcBP1BpHNWJiWeQejv5P6kOnqBy4xidVlK8UsX2zhr3DXqRTTOLqtkvuZf303p2yO2LJ1sjPlIHsUFVHT3N0R8/3vCnZm+GaApttTmkDzLwGo4LWcPIbkJEs22daR2rf++1UcGTOWFYs0r6/J0P0Dve86WHFK2VUP2kgnMatc6SDEdKggxGi8C/EUIjhyMOnyGSIGga7nwSH6UWmNGW9t4NmsKvB6TsYgDcP1bLw3Btyd6MKOa5gUxrgaiH2B2WUpMzeQYJVVG9Nhs0VoOFztRgtRDXlIqODMSWVxR7blPRiKfJtEnjVCIhOzIN+VjKIPImc0xoukZfiDsbw6a5/cKq+mYTXAxvVn4Mne6hcCw5Zu7SJ8h1LA+hlD807wXRZx59Gm9Tg1cVa1hSpxUYw9BIsfh23T5M0Z7Gdk7eiV2npR2PGQ967msMZiRE0eK1OayRiSuCFTL12vEPqvPUvl9axfacdThNLWtCd2AZQ0/rcCo70aUcJjfOhWrsktn7qZ3Q6t1mq1LrK/Ds1U9XCCWUAOEVKo5T58vHjx+h0Ouj1evjhhx/w4sUL/Pjjj/jpp5/w888/YzQaef+//PIL5P/XX39Flv/fyQvqUxJSiarEVSYqM5WpylwJoYRZ5E8MRV4MVxbjNewZOyt8qrZDK6L6X9H7e97e21VvqHZyomkHcjhq6g9PQfV2H1FbxXbQ/jS6D3gi82AeamSLt2AOauG2LM32344r2DEG+ra0sWlE/rQvrwdj2kF7wcmw6c7G/A7GeNhD50kTe/qZHfeaaFu20k1k5T2gg5HOh09JAGGDPi96WuxGYt14DZfornn2uFEHY/ylv26jchjdktvfNSjqYITpKT3d76LpTQ2dbKUaPpcLte33k33/sLC1GjqxHa0koizydjDZiWjyLHJ1Gg00L0GfQ5KDMR5HpxRNtV0RIecLxO3TJJ3U/DPZnmh5Jilnu/Lzz7qLVLY0rbFkPcc924yABNtlawOodZVrDmJTBhMb51Zp5rqZWldTvr9eW8s2lU6XJLEMXOStY8p6TQejP/vZFrMYxqnGy6yphB9JmoJUSfg/PLXDiDH64fWWqcVJfu9F8UoVe3cyOhiQg/02UTvsoR8szFYLq/QFiSr/afJ5xRwPvbMwmvf0qUN7ntPU7Y8x9nZ1sox9Jv7odXhzOBj9Jiqb/uL58gd1NA4nTp2S0d9K10Xpbse6sF7P3b9OUNLxiLxDAitLQHTiLHr0LOOKPIkV8qKBLcdBuFuNFjFcA+Hd0x2MoMfz7UZsSo0/opDgYIRpT9kqVeI9r6OYuA1nECnYSce6nk7SUZ8JtsePMkTzPWVb6pAl7fqr0Wufg7fuQe+x9kaq/W1F2x9ORqtTG43RhLOFLDsqeovKtV0O24cN1IMdlcpvK3uYUB+ZbE9G+5ckfco5GP53RT8HY5Z6MDPUv5/RZ2O1k5naMCCcAizPpQ2wjsrD9DZA2voFSW2mT69TNDqtyp9eqG95G3QMflBG9Xp6uyZTGyWpvoP7kyllM5XkQkbmFCmj2sRQnKUxmjVtkckQNTmYoOSn/jhe9LTFcB1/L3XZjcnYe91X6NMWeYuII/QP694cVDXMuf1BM7qwKoiWKt/JAK2bm96iqs0bVdS9hVbSmG9iT3bJ2thB09q54hv4pB4xkXSsyjuyOCgSQf8ctlBWi9jfbwWLxPWHk2uZqxrZym7y2Liig2EAYZAEYgREJ86qS88qvsgTEzS8Iac3m4eOBucXbe0HU030Bpw4GMb5ObKWQhq0Qa9yfCFt9H1/YbjlXIfv9lB0JluuhiJHLgI5E+f5B5ETbI//NNBtInck/XjA08Wh/Zk0EmV+vW4vTtvBSJ6SU0L5g0nnlm+HOjj6JLomJlKapAZnJBKApM4xM54tLN+Jkv0kb/d6SzuDKWs9iCOidzjKFLzg7Cdx+NQ6xEtb2NLPlIjIma0NcOoOxtf1xKlVaj3MZP1N02v7dD9X2zEnt2v071ykeHpAnBqjGeGfP7bNg/Z0VhmuOYLxioxgZKjrMMpsDkb4WupF2o/X24ZRnV3xnfGr1VM8GaF104VT2I33kGVV8np60649Y7qecEJo9OV2ZG1IF3W9V864LriOt1NJ4pzT2IF90bwYIoFXnYA06M/KYZg1XZEnlXt4TtE+ukM12jpA9/72ZPts72XdwQBk8XblcR+q32M87Aa73+nbXWrbcHYH/jbeKu2DHayrrbmPg0nuSgeqc5JuttBXiXlTqdr+Vt/alquJZfCmusR3oYrET3UwIjEXDuj2IuJgBLo+04Ybtg06Yj3w00WN5G9Gn9v2iJ0oYc96CnU0o9FxzdsWV3V49ZVj1vfr1nG3IpupRN9KC9mmKGvTksX5CzvkTqFzzGDl1bGtjjLdS3YUEks95furf+cS07A+6GP/jfjWxNaor9BNjmAYlSmGYlYDc5bxRSZD1ORgwo9k/h9HPKvzdjD8Q4uiBxPFpQK8g/eMxeNePENx2d6d+V6wR/3U3Tks+54n9c7ZdsSI30txsmYuBF8ggYtHQHTiWerdWdIWeaaSHLRRlwPu1IGg16poRvbJ9BuV4RlAGKH3sIrtYJqqu15C+W4b3YcVFC/X0JZ1E2obThXPO2Av2IozlraaBxvNXx34p9LTt1xNLkNwWOnbxoiK/sJ3LdRu76GdeWMN/eXZrnV7ltrAny3ZuWKn5j+37VHfBTVi4GLqAYeB1MOndZTfUIcsBp1U13bR/v687MXpOxhzVcYiLyW0nSRJ/Tsn97J8egczultTD7TMktZFikMHw6gtMRSzGJezjisyGaImBxN+JPP+OGwZnbeDAbUPdVEdirON3YMWuv3JkPnoZR9d2ZfaTdiSNlDysTm9xuiBjBpMTgC3lX5yzz8Z14F7pYrGYbR3Z9DroHm37O297V6po6Nt8TdJgVckQAKzEhCdeNa6N2v6Is+s5bhw8UdHqLgF1I7Pq9GaTMjbPfFGE+oMvNQGfnISp/YkNf9FbI93UrS+fuLURD6DhFbEwVAdAwntBmk/eJ8y0+Ckj703XGzp53CdAf08JkkHw6gVMRRZDct5xBOZDFGTg0O1lmKyO5RE7D+uoZa217dEzPA5erZ/6j1VU+U7GWPQVQuy/D3F9R+zt7/4w27yHvAAZho1mMV+ngzQfbgX3TZXKaBrFdQ+aaA9457zGfAzCgmsNAHRieehf7PkIfKsQqV4U3Gm7Dp13hx8e7S8c4Km5T+f7Rmj+3EJ2/d7kRO/z5tt9vyCbeXlcLrsL05ivuxg/3YNzdwahsYAABPCSURBVN7k1rleTRuBsyz+j88wkM5PvxGhnM+iOoE+5SDLcy3jOWZGB8OALYYii1E5rzgikyEqgyRAAiSwkgREJ56XDp6Wj8izKpUxVnvPz9IJsypgWE4SMAgo51I2JjAevfJBOhhGFYuhmGZQzvO5yGSIyiAJkAAJrCQB0YnnqYfT8hJ5VrIyWGgSIAESsBCgg2FAEUORZkzO+5nIZIjKIAmQAAmsJAHRieeti5PyE3lWsjJYaBIgARKwEKCDYUARQ5FkSJZxX2QyRGWQBEiABFaSgOjEZehjW54iz0pWBgtNAiRAAhYCdDAMKGIobEZkWfdEJkNUBkmABEhgJQmITlyWTjbzFXlWsjJYaBIgARKwEKCDYUARQ2EakGWGRSZDVAZJgARIYCUJiE5cpl7W8xZ5VrIyWGgSIAESsBCgg2FAEUOhG49lX4tMhqgMkgAJkMBKEhCduGzdLPmLPCtZGSw0CZAACVgI0MEwoIihEMORh0+RyRCVQRIgARJYSQKiE/Ogn5UMIs9KVgYLTQIkQAIWAnQwDChiKPJiuGi8jApikARIYOUJ5E1PizwrXzEEQAIkQAIBAToYxldBDEUePw1RGSQBEiCBlSSQR/2sZOIfCZAACZCAT4AOhvFNyKvhUnKpyuI/GfA7wO/Aqn8H8qqnV71eWH7qJn4H+B3QvwNGE3vm4OPHj9HpdNDr9fDDDz/gxYsX+PHHH/HTTz/h559/xmg08v5/+eUXyP+vv/6KLP+/kxfUpySkElWJq0xUZipTlbkSQgmzyJ8YLk6RWoQi3yUBEiCBsyOQNz0t8pxdiZkyCZAACVwsAsrRWPSPDka/j7N0SGi8Fv2K8n0SIIFXiYDoxLPUu7OkLfK8SoxZFhIgARJYhAAdDIOeGIpZjMtZxxWZDFEZJAESIIGVJCA68ax1b9b0RZ6VrAwWmgRIgAQsBOhgGFDEUGQ1LOcRT2QyRGWQBEiABFaSgOjE89C/WfIQeVayMlhoEiABErAQoINhQBFDkcWonFcckckQlUESIAESWEkCohPPSwdPy0fkWcnKYKFJgARIwEKADoYBRQzFNINyns9FJkNUBkmABEhgJQmITjxPPZyWl8izkpXBQpMACZCAhQAdDAOKGIo0Y3Lez0QmQ1QGSYAESGAlCYhOPG9dnJSfyLOSlcFCkwAJkICFAB0MA4oYiiRDsoz7IpMhKoMkQAIksJIERCcuQx/b8hR5VrIyWGgSIAESsBCgg2FAEUNhMyLLuicyGaIySAIkQAIrSUB04rJ0spmvyLOSlcFCkwAJkICFAB0MA4oYCtOALDMsMhmiMkgCJEACK0lAdOIy9bKet8izkpXBQpMACZCAhQAdDAOKGArdeCz7WmQyRGWQBEiABFaSgOjEZetmyV/kWcnKYKFJgARIwEKADoYBRQyFGI48fIpMhqgMkgAJkMBKEhCdmAf9rGQQeVayMlhoEiABErAQoINhQBFDkRfDReNlVBCDJEACK08gb3pa5Fn5iiEAEiABEggI0MEwvgpiKOhgGGAYJAESIIGcEMibnhZ5coKHYpAACZDA0gnQwTCqQAwFHQwDDIMkQAIkkBMCedPTIk9O8FAMEiABElg6AToYRhWIoaCDYYBhkARIgARyQiBvelrkyQkeikECJEACSydAB8OoAjEUdDAMMAySAAmQQE4I5E1Pizw5wUMxSIAESGDpBOhgGFUghoIOhgGGQRIgARLICYG86WmRJyd4KAYJkAAJLJ0AHQyjCsRQ0MEwwDBIAiRAAjkhkDc9LfLkBA/FIAESIIGlE6CDYVSBGAo6GAYYBkmABEggJwTypqdFnpzgoRgkQAIksHQCdDCMKhBDsZCD8Y9vcHTvXbz5+w1s/H4Db773f3H0TR/zpikyGaIySAIkQAIrSUB04rw69bTfE3lWsjJYaBIgARKwEKCDYUARQzG/AfoGj/60Bsd5DRtvvYt339rAa44D57W38dfufE6GyGSIyiAJkAAJrCQB0Ynz6+n5dHFSfiLPSlYGC00CJEACFgJ0MAwoYiiSDMm0+982K3jdcbD29l9x/A/fiB3+++twHAd/uPPVXKMYIpMhKoMkQAIksJIERCdO08fn9VzkWcnKYKFJgARIwEKADoYBRQzFfIbpWxz8n9fgOH/AR19qPWT/c4QHnz/Ag0fH+Lav3c94LTIZojJIAiRAAitJQHTifHp6dh08LR+RZyUrg4UmARIgAQsBOhgGFDEU0wyK/flX+Oj3DpzX38Kf/3OyBmPjzXdx8Pf5jZrIZIjKIAmQAAmsJAHRiXY9PL+unTc9kWclK4OFJgESIAELAToYBhQxFPMZmkeoqPUW3pqLDbz17x/ho/f+6K/BcEr4y3/PZ/hEJkNUBkmABEhgJQmITpxPT8+nh9PyEnlWsjJYaBIgARKwEKCDYUARQ5FmTJKfiYPxGkr143C9xVe3/+A5Ha/926PwXnIaceMnMhmiMkgCJEACK0lAdOIsevQs44o8K1kZLDQJkAAJWAjQwTCgiKGYzxiJg/E6bv1NcxT+dstb+O3880f4KuO6Cz1/kckQNSU4RO9JG23Lf2+Y8tpCj/w8O9+P7an8NkDHlOfZAJHYQZy5ZQzeT5TBlOxkjNHLEcYn5gOGSYAE8kxAdKKuJ5d5LfLkmRllIwESIIHzJEAHw6AthmI+Y3WMjzfVFKnXcaulORiPKv40qX/5C47PxcHoo3W7hprlv9kzCpwl+KKN+o0S1i85cNc2Ub7bxiDWKO+g5jgofTawp/iyg31TnnttRPydQQNbjoPqU3sSwACNLQfOrbY9QvB+ogzmW8c1OE4J+9+bD7KGR+g/sztyNueuPbfnlFUexiOB1SCwmJ7WdPMc+thmG0Se1aDPUpIACZDAdAJ0MAxGYihsRiTLva/+I5gO9SeZDvUNHrwTbFN7++jiTZEadVDbcOC+vY/uYIRBt4GdDQfOVgNRVyLBwRiPvFECNVKQ9u+NIlw4B8Mvc+Fq1erMxRy8h/N4d8YXlEESIAEsqqez6PJZ4og8rBoSIAESIAGfAB0M45sghmIW4xKJ+7+PcOuf1Fa1Dl5b28DGWnC9eQuHc57mLTIZohrBIZrvFVG8nPV/B80XRhKWYPdOAc5aFe2R9vDrXRQcB5Uv9AlOdgdj9Gw/Q+N7D201lDHVwehid82B804LujihZOc+gmEvcygPL0iABM6EgOjEiO49pdGIedIUec6ksEyUBEiABC4gAToYRqWJoZjHyITv/OMYj+p/xr++uYE/XnsXlTsPcPzt/MPyIpMhaiw4HhmjBMM2dtVog+Og/PkgNoIwfe1BBzXXQfGu2fPex95lc6rSlMa2Wu/Q72rrQjroDXUHJYOD8duRv0tXYRfdWOkn75/fFKkpZbbJyHskQAILExCdGOrcJToXSgaRZ+GCMQESIAESeEUI0MEwKlIMRV4M19zG62SA1vvrcC5tY/f2FlxnE7vPrP3+BgEtGIwIVJ5o94LLzocOnDf20Q8fpTS2v29hp+jCLZZRvV1H42APtQ/K2FRrOkq76IpYU0Ywxl9W4HrbABdQfWo4J0qO4P3ClYo/anK/Yx/pEJm9NRgO3PVsoz47DyMrRgCklFny4CcJkMCpE8ibnhZ5Tr2gTJAESIAELigBOhhGxYmhuKgOxrjfRfNu0Hi/sou2t1BihN7BDoquA/dKFY0nPZiDBwYGPyiLoCdeRBhtcLAFx6miHS729hvb67cawShFB4PfVPQxjt534JT20A/jBsmMWthxHGwdBKs5AgfBXQsa/O81J4vAT/rYe8OBe30Xu1fd+LQtlaQ4GLImwlxEHko/uRiba0MOK97C773nxmiQiueVZ/IuHQydBa9J4PwI5E1PizznR4A5kQAJkEC+CdDBMOpHDMXFcjDGGDzc8dZFqMb59s09tHpDb/tVfw1Eyx9pGA/QOahiu1jwRwI2d9GR0QODgxcUB8Oyy1KSgxE6B5fLaIrf8Lk/glI77AVOBzB+2Ufn/g7WnXXsfh1kHjgIlUNp3MsoxQidD9fhuFvYV85OsPDcUfK/1AQP3s88RUp7NbxMKXMYJ7zgCEaIghckcI4E8qanRZ5zRMCsSIAESCDXBOhgGNUjhuJiORh+IcYjaZBPChV3BIJnak1ErEd+8p539awG11lH/blxXw0WJIxgJDXuB8cN7N4oobjuwnFcFIq+I9TWz80IHITINrWjHhrvrMNx1lF7qnlDL9qolVw4bgnKcRmq0ZElORiTUZspW9aa537EsfIOCZBABgJ509MiTwbRGYUESIAEVoIAHQyjmsVQXCgH4zfp8Y9/9u6V4DgVtIbxZ7JtbOJibxlRSFqDUdrXpj1l7M1PGyEwHIzhl1VvnYZzqYS6bf3ISTD162ogR0YHw3eO/MXvagH8rP8TB8h23kgZm44D69a1GaZsGV9HBkmABCwE8qanRR6LqLxFAiRAAitJgA6GUe1iKC6Sg9H9JNsiZfsWtpOpTAYKAP62sIXbHeORv4uUGznwzuJgfNeKb1F7YxOOU0DpZg2VayL3ujdlq3Srahy0pw6y62IQH5gx5AmCGR0M+8unddfC4bSSZjokQAIegbzpaZGH1UMCJEACJOAToINhfBPEUFwkB8MowqkGux+r6Uk7aGmzk0ZPqyg4BdSe6VlZGtbDnrYtbXz6UKc32Tp36kF7XlqycFzPV7ue18E4GWPY66D9cC/iEO09bKPbz+rdiBwWDvKInyRAAqdCIG96WuQ5lcIxERIgARJ4BQjQwTAqUQzFhXUwxgO071VRvlJEwY1O//EXgNfR7JrbrRoQ9GC4oLqGVq+P3mENJdfB+odtjCK7QmVrWKtdrtqHDdRv18LGfP2ghfazHoa/jf2zOmxt+rSpVSLvHA5G/2HF3y53s4zqJw20nogj1MTe7Qq21lwkTtGSfCOf2ThEXmGABEhgJgJ509Miz0yFYGQSIAESeIUJ0MEwKlcMxUV0MMbf7WNLNf6v1dFSowNGQ3086nu7SCkHoXDzaLIFrMEgFhz10bpbRsk7JXwb1Yddf1F1JGJ6w3r0dd1rrLtrW6jc3kMzbMi30Tqoo3JNTZNyUfq4YzguQSZZHAy1cN26nWxE0DAwfFz2FrFXDgfejlvhA/1CrfP4VO2CVbQudtej+tfpHOLxeYcESGBWAnnT0yLPrOVgfBIgARJ4VQnQwTBqVgzFxXMwhmhcdeBs7WMQGVkwCqhOpvhCnfXgoHYcfzb/nZSGtTqBWzk1t44sjomW4/O6t0B6O3agHYAsDoaWVJZL77DAjTp6U3jhpI2q4yDcIetFEzuesyVrSPRPfz2Jc2kd9jUvRRQ/sZ5DnkVkxiEBEgDCk7PzoqfFbrBySIAESIAEfAJ0MIxvghiKvBguJYfIZIhqBEdovePC2dhFd0qDefj5trft6+6ptnNTHIyXLZQdB8W7PUNmIygH730WHKChPz4DB6N/Xy04L/lna+h5Gdf+mhMXlS+ChSgyUmIe0pc1bA4tGfkxSAIkkE5AdGJe9LTIky41n5IACZDA6hCgg2HUtRiKvBiu7A4GgBdHqBb9NQPVgxa6fX1r2gF6z5qoXy96U5G27/fsU5EMHtmDKQ6GOqLi821/CtR7e2g+62GgNcYHvQ5aB1WULqmTxuvRw/NEAM/BcOCu66MFSdc7aL6QF1M+T4Y4uuXzKH3QQKvb96dYebIFvG5s+nLfTZi6lZI8H5EACZwNgbzpaZHnbErLVEmABEjg4hGgg2HUmRiKC+lgqLKcjDHoqgXK/kJvfZrO9s0a6gdt9IbG4gyDwXxB/0yIvacpC8jVTlAHddRubkenD13xF1i3g9PHrfnPOGqQeLaHLfFBF817NVTVQYDa1Kez5WUThPdIgASyEMibnhZ5ssjOOCRAAiSwCgToYBi1LIbiwjoYRnkYJAESIIFXjUDe9LTI86pxZnlIgARIYF4CdDAMcmIo6GAYYBgkARIggZwQyJueFnlygodikAAJkMDSCdDBMKpADAUdDAMMgyRAAiSQEwJ509MiT07wUAwSIAESWDoBOhhGFYihoINhgGGQBEiABHJCIG96WuTJCR6KQQIkQAJLJ0AHw6gCMRR0MAwwDJIACZBATgjkTU+LPDnBQzFIgARIYOkE6GAYVSCGgg6GAYZBEiABEsgJgbzpaZEnJ3goBgmQAAksnQAdDKMKxFDQwTDAMEgCJEACOSGQNz0t8uQED8UgARIggaUToINhVIEYCjoYBhgGSYAESCAnBPKmp0WenOChGCRAAiSwdAJ0MIwqEENBB8MAwyAJkAAJ5IRA3vS0yJMTPBSDBEiABJZOgA6GUQViKPL4aYjKIAmQAAmsJIE86mclE/9IgARIgAR8AnQwjG9CXg2XkktVFv/JgN8BfgdW/TuQVz296vXC8lM38TvA74D+HTCa2DMHHz9+jE6ng16vhx9++AEvXrzAjz/+iJ9++gk///wzRqOR9//LL79A/n/99Vdk+f+dvKA+JSGVqEpcZaIyU5mqzJUQShj+kQAJkAAJkAAJkAAJkAAJXFwCdDAubt1RchIgARIgARIgARIgARLIHQE6GLmrEgpEAiRAAiRAAiRAAiRAAheXAB2Mi1t3lJwESIAESIAESIAESIAEckeADkbuqoQCkQAJkAAJkAAJkAAJkMDFJUAH4+LWHSUnARIgARIgARIgARIggdwRoIORuyqhQCRAAiRAAiRAAiRAAiRwcQnQwbi4dUfJSYAESIAESIAESIAESCB3BM7Swfj/TGLtwrX6ND4AAAAASUVORK5CYII="
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 데이터 적용1\n",
    "df_pre = pd.read_csv('example/080228-master/deeplearning/dataset/wine.csv', header=None)\n",
    "df = df_pre.sample(frac=1) # tansorflow.sample : 샘플을 섞어 가져오기, frac = 1 : smaple을 전체를 가져온다, \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0     1     2     3      4     5      6        7     8     9     10  \\\n",
      "5744  6.8  0.26  0.29  11.9  0.052  54.0  160.0  0.99546  3.03  0.58  10.4   \n",
      "5022  7.1  0.18  0.39  14.5  0.051  48.0  156.0  0.99947  3.35  0.78   9.1   \n",
      "1379  7.5  0.57  0.02   2.6  0.077  11.0   35.0  0.99557  3.36  0.62  10.8   \n",
      "313   8.6  0.47  0.30   3.0  0.076  30.0  135.0  0.99760  3.30  0.53   9.4   \n",
      "646   7.3  0.67  0.05   3.6  0.107   6.0   20.0  0.99720  3.40  0.63  10.1   \n",
      "\n",
      "      11  12  \n",
      "5744   6   0  \n",
      "5022   5   0  \n",
      "1379   6   1  \n",
      "313    5   1  \n",
      "646    5   1  \n"
     ]
    }
   ],
   "source": [
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = df.values\n",
    "X = dataset[:,0:12].astype(float)\n",
    "Y = dataset[:,12]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 6497 entries, 5744 to 5659\n",
      "Data columns (total 13 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   0       6497 non-null   float64\n",
      " 1   1       6497 non-null   float64\n",
      " 2   2       6497 non-null   float64\n",
      " 3   3       6497 non-null   float64\n",
      " 4   4       6497 non-null   float64\n",
      " 5   5       6497 non-null   float64\n",
      " 6   6       6497 non-null   float64\n",
      " 7   7       6497 non-null   float64\n",
      " 8   8       6497 non-null   float64\n",
      " 9   9       6497 non-null   float64\n",
      " 10  10      6497 non-null   float64\n",
      " 11  11      6497 non-null   int64  \n",
      " 12  12      6497 non-null   int64  \n",
      "dtypes: float64(11), int64(2)\n",
      "memory usage: 710.6 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 모델 설정\n",
    "model = Sequential()\n",
    "model.add(Dense(30,  input_dim=12, activation='relu'))\n",
    "model.add(Dense(12, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "#모델 컴파일\n",
    "model.compile(loss='binary_crossentropy',\n",
    "          optimizer='adam',\n",
    "          metrics=['accuracy'])\n",
    "\n",
    "import os\n",
    "# 모델 저장 폴더 설정\n",
    "MODEL_DIR = './model/'\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "   os.mkdir(MODEL_DIR)\n",
    "\n",
    "# 모델 저장 조건 설정\n",
    "modelpath=\"./model/{epoch:02d}-{val_loss:.4f}.hdf5\"\n",
    "checkpointer = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=1, save_best_only=True) #filepath = 모델의 저장 이름을 정해놓은 것 # verbose = 1이면 값을 보여주겠다, 0이면 보여주지 않는다, # save_best_only=True : 모델이 앞서 저장한 모델보다 나아졌을 때만 저장하게끔 하기 위함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1층 모델 : 가중치 파라미터 12*30 = 360개/ 바이오스 파라미터 30 개, 합 = 390개\n",
    "# 2층 모델 : 가중치 파라미터 30*12 = 360개/ 바이오스 파라미터 12 개, 합 = 372개\n",
    "# 3층 모델 : 가중치 파라미터 12*8 = 96개 / 바이오스 파라미터 8개, 합 = 104개\n",
    "# 4층 모델 : 가중치 파라미터 8*1 = 8개 / 바이오스 파라미터 1개, 합 = 9개\n",
    "\n",
    "# 4개의 모델 총 390 + 372 + 104 + 9 = 875개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_48 (Dense)             (None, 30)                390       \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 12)                372       \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 8)                 104       \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 875\n",
      "Trainable params: 875\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000028525D66678> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.29318, saving model to ./model\\01-0.2932.hdf5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.29318 to 0.23743, saving model to ./model\\02-0.2374.hdf5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.23743 to 0.21581, saving model to ./model\\03-0.2158.hdf5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.21581 to 0.20553, saving model to ./model\\04-0.2055.hdf5\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.20553 to 0.19845, saving model to ./model\\05-0.1984.hdf5\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.19845 to 0.19248, saving model to ./model\\06-0.1925.hdf5\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.19248 to 0.18801, saving model to ./model\\07-0.1880.hdf5\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.18801 to 0.18392, saving model to ./model\\08-0.1839.hdf5\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.18392 to 0.17947, saving model to ./model\\09-0.1795.hdf5\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.17947\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.17947 to 0.17526, saving model to ./model\\11-0.1753.hdf5\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.17526 to 0.17462, saving model to ./model\\12-0.1746.hdf5\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.17462 to 0.16711, saving model to ./model\\13-0.1671.hdf5\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.16711 to 0.16128, saving model to ./model\\14-0.1613.hdf5\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.16128 to 0.15861, saving model to ./model\\15-0.1586.hdf5\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.15861 to 0.15519, saving model to ./model\\16-0.1552.hdf5\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.15519 to 0.14942, saving model to ./model\\17-0.1494.hdf5\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.14942 to 0.14181, saving model to ./model\\18-0.1418.hdf5\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.14181 to 0.13880, saving model to ./model\\19-0.1388.hdf5\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.13880 to 0.13524, saving model to ./model\\20-0.1352.hdf5\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.13524 to 0.13393, saving model to ./model\\21-0.1339.hdf5\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.13393\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.13393 to 0.12676, saving model to ./model\\23-0.1268.hdf5\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.12676 to 0.12263, saving model to ./model\\24-0.1226.hdf5\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.12263 to 0.11977, saving model to ./model\\25-0.1198.hdf5\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.11977\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.11977 to 0.11738, saving model to ./model\\27-0.1174.hdf5\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.11738\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.11738\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.11738 to 0.11222, saving model to ./model\\30-0.1122.hdf5\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.11222 to 0.10717, saving model to ./model\\31-0.1072.hdf5\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.10717\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.10717 to 0.10668, saving model to ./model\\33-0.1067.hdf5\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.10668 to 0.10534, saving model to ./model\\34-0.1053.hdf5\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.10534 to 0.10148, saving model to ./model\\35-0.1015.hdf5\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.10148\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.10148\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.10148 to 0.10092, saving model to ./model\\38-0.1009.hdf5\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.10092 to 0.09796, saving model to ./model\\39-0.0980.hdf5\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.09796 to 0.09770, saving model to ./model\\40-0.0977.hdf5\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.09770\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.09770 to 0.09212, saving model to ./model\\42-0.0921.hdf5\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.09212\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.09212\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.09212 to 0.08835, saving model to ./model\\45-0.0883.hdf5\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.08835\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.08835\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.08835 to 0.08657, saving model to ./model\\48-0.0866.hdf5\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.08657\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.08657\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.08657 to 0.08544, saving model to ./model\\51-0.0854.hdf5\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.08544\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.08544 to 0.08447, saving model to ./model\\53-0.0845.hdf5\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.08447\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.08447\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.08447\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.08447\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.08447 to 0.08423, saving model to ./model\\58-0.0842.hdf5\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.08423 to 0.07993, saving model to ./model\\59-0.0799.hdf5\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.07993 to 0.07858, saving model to ./model\\60-0.0786.hdf5\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.07858\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.07858\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.07858 to 0.07811, saving model to ./model\\63-0.0781.hdf5\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.07811\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.07811\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.07811\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.07811 to 0.07681, saving model to ./model\\67-0.0768.hdf5\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.07681 to 0.07651, saving model to ./model\\68-0.0765.hdf5\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.07651 to 0.07610, saving model to ./model\\69-0.0761.hdf5\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.07610 to 0.07587, saving model to ./model\\70-0.0759.hdf5\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.07587 to 0.07473, saving model to ./model\\71-0.0747.hdf5\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.07473\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.07473 to 0.07399, saving model to ./model\\73-0.0740.hdf5\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.07399 to 0.07383, saving model to ./model\\74-0.0738.hdf5\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.07383 to 0.07307, saving model to ./model\\75-0.0731.hdf5\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.07307\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.07307\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.07307\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.07307 to 0.07171, saving model to ./model\\79-0.0717.hdf5\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.07171\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.07171\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.07171 to 0.07149, saving model to ./model\\82-0.0715.hdf5\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.07149\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.07149 to 0.07011, saving model to ./model\\84-0.0701.hdf5\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.07011\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.07011\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.07011\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.07011\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.07011 to 0.06905, saving model to ./model\\89-0.0690.hdf5\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.06905\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.06905\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.06905\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.06905\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.06905\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.06905 to 0.06875, saving model to ./model\\95-0.0687.hdf5\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.06875 to 0.06755, saving model to ./model\\96-0.0676.hdf5\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.06755\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.06755\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00099: val_loss did not improve from 0.06755\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.06755\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.06755\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.06755\n",
      "\n",
      "Epoch 00103: val_loss improved from 0.06755 to 0.06564, saving model to ./model\\103-0.0656.hdf5\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.06564\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.06564\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.06564\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.06564\n",
      "\n",
      "Epoch 00108: val_loss improved from 0.06564 to 0.06427, saving model to ./model\\108-0.0643.hdf5\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.06427\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.06427\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.06427\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.06427\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.06427\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.06427\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.06427\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.06427\n",
      "\n",
      "Epoch 00117: val_loss improved from 0.06427 to 0.06424, saving model to ./model\\117-0.0642.hdf5\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.06424\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.06424\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.06424\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.06424\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.06424\n",
      "\n",
      "Epoch 00123: val_loss improved from 0.06424 to 0.06394, saving model to ./model\\123-0.0639.hdf5\n",
      "\n",
      "Epoch 00124: val_loss improved from 0.06394 to 0.06382, saving model to ./model\\124-0.0638.hdf5\n",
      "\n",
      "Epoch 00125: val_loss improved from 0.06382 to 0.06204, saving model to ./model\\125-0.0620.hdf5\n",
      "\n",
      "Epoch 00126: val_loss improved from 0.06204 to 0.06139, saving model to ./model\\126-0.0614.hdf5\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.06139\n",
      "\n",
      "Epoch 00128: val_loss improved from 0.06139 to 0.06109, saving model to ./model\\128-0.0611.hdf5\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.06109\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.06109\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.06109\n",
      "\n",
      "Epoch 00132: val_loss improved from 0.06109 to 0.06037, saving model to ./model\\132-0.0604.hdf5\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.06037\n",
      "\n",
      "Epoch 00134: val_loss improved from 0.06037 to 0.05917, saving model to ./model\\134-0.0592.hdf5\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.05917\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.05917\n",
      "\n",
      "Epoch 00137: val_loss improved from 0.05917 to 0.05786, saving model to ./model\\137-0.0579.hdf5\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.05786\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.05786\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.05786\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.05786\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.05786\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.05786\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.05786\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.05786\n",
      "\n",
      "Epoch 00146: val_loss improved from 0.05786 to 0.05784, saving model to ./model\\146-0.0578.hdf5\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.05784\n",
      "\n",
      "Epoch 00148: val_loss improved from 0.05784 to 0.05567, saving model to ./model\\148-0.0557.hdf5\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.05567\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.05567\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.05567\n",
      "\n",
      "Epoch 00152: val_loss improved from 0.05567 to 0.05465, saving model to ./model\\152-0.0547.hdf5\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.05465\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.05465\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.05465\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.05465\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.05465\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.05465\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.05465\n",
      "\n",
      "Epoch 00160: val_loss improved from 0.05465 to 0.05261, saving model to ./model\\160-0.0526.hdf5\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.05261\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.05261\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.05261\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.05261\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.05261\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.05261\n",
      "\n",
      "Epoch 00167: val_loss improved from 0.05261 to 0.05231, saving model to ./model\\167-0.0523.hdf5\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.05231\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.05231\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.05231\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.05231\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.05231\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.05231\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.05231\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.05231\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.05231\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.05231\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.05231\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.05231\n",
      "\n",
      "Epoch 00180: val_loss improved from 0.05231 to 0.04948, saving model to ./model\\180-0.0495.hdf5\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.04948\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.04948\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.04948\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.04948\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.04948\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.04948\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.04948\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.04948\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.04948\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.04948\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.04948\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.04948\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.04948\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.04948\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.04948\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.04948\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.04948\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.04948\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.04948\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.04948\n",
      "204/204 [==============================] - 1s 2ms/step - loss: 0.0377 - accuracy: 0.9895\n",
      "\n",
      " Accuracy: 0.9895\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 모델 실행\n",
    "model.fit(X, Y, validation_split=0.2, epochs=200, batch_size=200, verbose=0, callbacks=[checkpointer]) # checkpointer을 보기 위해 verbose = 0, #callbacks 해줄 때, ModelCheckpoint 함수에 의해 저장이 된다. \n",
    "\n",
    "# 결과 출력\n",
    "print(\"\\n Accuracy: %.4f\" % (model.evaluate(X, Y)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 그래프로 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 적용1\n",
    "df_pre = pd.read_csv('example/080228-master/deeplearning/dataset/wine.csv', header=None)\n",
    "df = df_pre.sample(frac=0.15)\n",
    "dataset = df.values\n",
    "X = dataset[:,0:12].astype(float)\n",
    "Y = dataset[:,12]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 모델의 설정\n",
    "model = Sequential()\n",
    "model.add(Dense(30,  input_dim=12, activation='relu'))\n",
    "model.add(Dense(12, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "          optimizer='adam',\n",
    "          metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장 폴더 설정\n",
    "MODEL_DIR = './model/'\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "   os.mkdir(MODEL_DIR)\n",
    "\n",
    "# 모델 저장 조건 설정\n",
    "modelpath=\"./model/{epoch:02d}-{val_loss:.4f}.hdf5\"\n",
    "checkpointer = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=1, save_best_only=True)\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=100) # 손실값이 좋아지지 않으면, 100번까지 기다렸다가 멈추어라."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1.1199 - accuracy: 0.3720\n",
      "Epoch 00001: val_loss improved from inf to 0.47823, saving model to ./model\\01-0.4782.hdf5\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.9899 - accuracy: 0.4334 - val_loss: 0.4782 - val_accuracy: 0.7422\n",
      "Epoch 2/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.4272 - accuracy: 0.7660\n",
      "Epoch 00002: val_loss did not improve from 0.47823\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.4381 - accuracy: 0.7611 - val_loss: 0.5427 - val_accuracy: 0.7453\n",
      "Epoch 3/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.4365 - accuracy: 0.7720\n",
      "Epoch 00003: val_loss did not improve from 0.47823\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.4677 - accuracy: 0.7657 - val_loss: 0.6154 - val_accuracy: 0.7453\n",
      "Epoch 4/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5090 - accuracy: 0.7660\n",
      "Epoch 00004: val_loss did not improve from 0.47823\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.5112 - accuracy: 0.7657 - val_loss: 0.6352 - val_accuracy: 0.7453\n",
      "Epoch 5/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5087 - accuracy: 0.7720\n",
      "Epoch 00005: val_loss did not improve from 0.47823\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.5183 - accuracy: 0.7657 - val_loss: 0.6137 - val_accuracy: 0.7453\n",
      "Epoch 6/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5002 - accuracy: 0.7620\n",
      "Epoch 00006: val_loss did not improve from 0.47823\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.4956 - accuracy: 0.7657 - val_loss: 0.5632 - val_accuracy: 0.7453\n",
      "Epoch 7/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.4515 - accuracy: 0.7720\n",
      "Epoch 00007: val_loss did not improve from 0.47823\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.4523 - accuracy: 0.7657 - val_loss: 0.4975 - val_accuracy: 0.7453\n",
      "Epoch 8/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.4141 - accuracy: 0.7560\n",
      "Epoch 00008: val_loss improved from 0.47823 to 0.43215, saving model to ./model\\08-0.4321.hdf5\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.4021 - accuracy: 0.7657 - val_loss: 0.4321 - val_accuracy: 0.7453\n",
      "Epoch 9/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.3236 - accuracy: 0.7840\n",
      "Epoch 00009: val_loss improved from 0.43215 to 0.38658, saving model to ./model\\09-0.3866.hdf5\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.3519 - accuracy: 0.7657 - val_loss: 0.3866 - val_accuracy: 0.7484\n",
      "Epoch 10/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.3217 - accuracy: 0.7820\n",
      "Epoch 00010: val_loss improved from 0.38658 to 0.37571, saving model to ./model\\10-0.3757.hdf5\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.3281 - accuracy: 0.7779 - val_loss: 0.3757 - val_accuracy: 0.7857\n",
      "Epoch 11/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.3286 - accuracy: 0.8260\n",
      "Epoch 00011: val_loss did not improve from 0.37571\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.3350 - accuracy: 0.8285 - val_loss: 0.3787 - val_accuracy: 0.8416\n",
      "Epoch 12/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.3436 - accuracy: 0.8640\n",
      "Epoch 00012: val_loss improved from 0.37571 to 0.35804, saving model to ./model\\12-0.3580.hdf5\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.3416 - accuracy: 0.8698 - val_loss: 0.3580 - val_accuracy: 0.8634\n",
      "Epoch 13/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.3196 - accuracy: 0.8940\n",
      "Epoch 00013: val_loss improved from 0.35804 to 0.33430, saving model to ./model\\13-0.3343.hdf5\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.3145 - accuracy: 0.8959 - val_loss: 0.3343 - val_accuracy: 0.8602\n",
      "Epoch 14/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.2867 - accuracy: 0.8860\n",
      "Epoch 00014: val_loss improved from 0.33430 to 0.32948, saving model to ./model\\14-0.3295.hdf5\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.2850 - accuracy: 0.8851 - val_loss: 0.3295 - val_accuracy: 0.8602\n",
      "Epoch 15/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.2649 - accuracy: 0.8840\n",
      "Epoch 00015: val_loss did not improve from 0.32948\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.2727 - accuracy: 0.8775 - val_loss: 0.3336 - val_accuracy: 0.8540\n",
      "Epoch 16/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.2720 - accuracy: 0.8700\n",
      "Epoch 00016: val_loss did not improve from 0.32948\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.2687 - accuracy: 0.8698 - val_loss: 0.3343 - val_accuracy: 0.8571\n",
      "Epoch 17/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.2565 - accuracy: 0.8720\n",
      "Epoch 00017: val_loss improved from 0.32948 to 0.32758, saving model to ./model\\17-0.3276.hdf5\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.2652 - accuracy: 0.8744 - val_loss: 0.3276 - val_accuracy: 0.8727\n",
      "Epoch 18/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.2643 - accuracy: 0.8740\n",
      "Epoch 00018: val_loss improved from 0.32758 to 0.31469, saving model to ./model\\18-0.3147.hdf5\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.2577 - accuracy: 0.8867 - val_loss: 0.3147 - val_accuracy: 0.8727\n",
      "Epoch 19/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.2548 - accuracy: 0.8960\n",
      "Epoch 00019: val_loss improved from 0.31469 to 0.30130, saving model to ./model\\19-0.3013.hdf5\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.2476 - accuracy: 0.8959 - val_loss: 0.3013 - val_accuracy: 0.8820\n",
      "Epoch 20/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.2474 - accuracy: 0.9080\n",
      "Epoch 00020: val_loss improved from 0.30130 to 0.29068, saving model to ./model\\20-0.2907.hdf5\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.2393 - accuracy: 0.9142 - val_loss: 0.2907 - val_accuracy: 0.8882\n",
      "Epoch 21/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.2287 - accuracy: 0.9300\n",
      "Epoch 00021: val_loss improved from 0.29068 to 0.28393, saving model to ./model\\21-0.2839.hdf5\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.2330 - accuracy: 0.9280 - val_loss: 0.2839 - val_accuracy: 0.8944\n",
      "Epoch 22/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.2431 - accuracy: 0.9240\n",
      "Epoch 00022: val_loss improved from 0.28393 to 0.27957, saving model to ./model\\22-0.2796.hdf5\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.2300 - accuracy: 0.9342 - val_loss: 0.2796 - val_accuracy: 0.9006\n",
      "Epoch 23/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.2283 - accuracy: 0.9340\n",
      "Epoch 00023: val_loss improved from 0.27957 to 0.27623, saving model to ./model\\23-0.2762.hdf5\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.2265 - accuracy: 0.9372 - val_loss: 0.2762 - val_accuracy: 0.9006\n",
      "Epoch 24/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.2306 - accuracy: 0.9380\n",
      "Epoch 00024: val_loss improved from 0.27623 to 0.27462, saving model to ./model\\24-0.2746.hdf5\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.2215 - accuracy: 0.9387 - val_loss: 0.2746 - val_accuracy: 0.8944\n",
      "Epoch 25/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.2257 - accuracy: 0.9320\n",
      "Epoch 00025: val_loss did not improve from 0.27462\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2166 - accuracy: 0.9357 - val_loss: 0.2746 - val_accuracy: 0.8913\n",
      "Epoch 26/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.2075 - accuracy: 0.9380\n",
      "Epoch 00026: val_loss improved from 0.27462 to 0.27419, saving model to ./model\\26-0.2742.hdf5\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.2131 - accuracy: 0.9342 - val_loss: 0.2742 - val_accuracy: 0.8913\n",
      "Epoch 27/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.2172 - accuracy: 0.9280\n",
      "Epoch 00027: val_loss improved from 0.27419 to 0.27143, saving model to ./model\\27-0.2714.hdf5\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.2099 - accuracy: 0.9326 - val_loss: 0.2714 - val_accuracy: 0.8913\n",
      "Epoch 28/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.2029 - accuracy: 0.9360\n",
      "Epoch 00028: val_loss improved from 0.27143 to 0.26651, saving model to ./model\\28-0.2665.hdf5\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.2061 - accuracy: 0.9357 - val_loss: 0.2665 - val_accuracy: 0.8975\n",
      "Epoch 29/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.2035 - accuracy: 0.9340\n",
      "Epoch 00029: val_loss improved from 0.26651 to 0.26103, saving model to ./model\\29-0.2610.hdf5\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.2022 - accuracy: 0.9357 - val_loss: 0.2610 - val_accuracy: 0.9006\n",
      "Epoch 30/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1975 - accuracy: 0.9360\n",
      "Epoch 00030: val_loss improved from 0.26103 to 0.25657, saving model to ./model\\30-0.2566.hdf5\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.1983 - accuracy: 0.9403 - val_loss: 0.2566 - val_accuracy: 0.9068\n",
      "Epoch 31/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1880 - accuracy: 0.9380\n",
      "Epoch 00031: val_loss improved from 0.25657 to 0.25307, saving model to ./model\\31-0.2531.hdf5\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.1951 - accuracy: 0.9403 - val_loss: 0.2531 - val_accuracy: 0.9099\n",
      "Epoch 32/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1886 - accuracy: 0.9400\n",
      "Epoch 00032: val_loss improved from 0.25307 to 0.25023, saving model to ./model\\32-0.2502.hdf5\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.1922 - accuracy: 0.9403 - val_loss: 0.2502 - val_accuracy: 0.9130\n",
      "Epoch 33/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1904 - accuracy: 0.9360\n",
      "Epoch 00033: val_loss improved from 0.25023 to 0.24809, saving model to ./model\\33-0.2481.hdf5\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.1894 - accuracy: 0.9403 - val_loss: 0.2481 - val_accuracy: 0.9161\n",
      "Epoch 34/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1780 - accuracy: 0.9420\n",
      "Epoch 00034: val_loss improved from 0.24809 to 0.24650, saving model to ./model\\34-0.2465.hdf5\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.1869 - accuracy: 0.9403 - val_loss: 0.2465 - val_accuracy: 0.9161\n",
      "Epoch 35/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1940 - accuracy: 0.9380\n",
      "Epoch 00035: val_loss improved from 0.24650 to 0.24536, saving model to ./model\\35-0.2454.hdf5\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.1840 - accuracy: 0.9403 - val_loss: 0.2454 - val_accuracy: 0.9161\n",
      "Epoch 36/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1732 - accuracy: 0.9480\n",
      "Epoch 00036: val_loss improved from 0.24536 to 0.24451, saving model to ./model\\36-0.2445.hdf5\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.1815 - accuracy: 0.9418 - val_loss: 0.2445 - val_accuracy: 0.9130\n",
      "Epoch 37/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1675 - accuracy: 0.9440\n",
      "Epoch 00037: val_loss improved from 0.24451 to 0.24311, saving model to ./model\\37-0.2431.hdf5\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.1791 - accuracy: 0.9418 - val_loss: 0.2431 - val_accuracy: 0.9130\n",
      "Epoch 38/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1720 - accuracy: 0.9460\n",
      "Epoch 00038: val_loss improved from 0.24311 to 0.24062, saving model to ./model\\38-0.2406.hdf5\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.1765 - accuracy: 0.9418 - val_loss: 0.2406 - val_accuracy: 0.9130\n",
      "Epoch 39/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1836 - accuracy: 0.9340\n",
      "Epoch 00039: val_loss improved from 0.24062 to 0.23758, saving model to ./model\\39-0.2376.hdf5\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.1742 - accuracy: 0.9403 - val_loss: 0.2376 - val_accuracy: 0.9099\n",
      "Epoch 40/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1748 - accuracy: 0.9420\n",
      "Epoch 00040: val_loss improved from 0.23758 to 0.23550, saving model to ./model\\40-0.2355.hdf5\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.1719 - accuracy: 0.9433 - val_loss: 0.2355 - val_accuracy: 0.9130\n",
      "Epoch 41/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1789 - accuracy: 0.9360\n",
      "Epoch 00041: val_loss improved from 0.23550 to 0.23413, saving model to ./model\\41-0.2341.hdf5\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.1701 - accuracy: 0.9418 - val_loss: 0.2341 - val_accuracy: 0.9193\n",
      "Epoch 42/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1630 - accuracy: 0.9400\n",
      "Epoch 00042: val_loss improved from 0.23413 to 0.23401, saving model to ./model\\42-0.2340.hdf5\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.1682 - accuracy: 0.9403 - val_loss: 0.2340 - val_accuracy: 0.9224\n",
      "Epoch 43/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1718 - accuracy: 0.9400\n",
      "Epoch 00043: val_loss improved from 0.23401 to 0.23281, saving model to ./model\\43-0.2328.hdf5\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.1668 - accuracy: 0.9403 - val_loss: 0.2328 - val_accuracy: 0.9193\n",
      "Epoch 44/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1727 - accuracy: 0.9400\n",
      "Epoch 00044: val_loss improved from 0.23281 to 0.23077, saving model to ./model\\44-0.2308.hdf5\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.1655 - accuracy: 0.9403 - val_loss: 0.2308 - val_accuracy: 0.9193\n",
      "Epoch 45/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1578 - accuracy: 0.9420\n",
      "Epoch 00045: val_loss improved from 0.23077 to 0.22920, saving model to ./model\\45-0.2292.hdf5\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.1644 - accuracy: 0.9403 - val_loss: 0.2292 - val_accuracy: 0.9193\n",
      "Epoch 46/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1632 - accuracy: 0.9380\n",
      "Epoch 00046: val_loss improved from 0.22920 to 0.22805, saving model to ./model\\46-0.2280.hdf5\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.1637 - accuracy: 0.9418 - val_loss: 0.2280 - val_accuracy: 0.9161\n",
      "Epoch 47/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1679 - accuracy: 0.9440\n",
      "Epoch 00047: val_loss improved from 0.22805 to 0.22773, saving model to ./model\\47-0.2277.hdf5\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.1632 - accuracy: 0.9449 - val_loss: 0.2277 - val_accuracy: 0.9161\n",
      "Epoch 48/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1671 - accuracy: 0.9500\n",
      "Epoch 00048: val_loss did not improve from 0.22773\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1623 - accuracy: 0.9464 - val_loss: 0.2278 - val_accuracy: 0.9161\n",
      "Epoch 49/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1747 - accuracy: 0.9380\n",
      "Epoch 00049: val_loss did not improve from 0.22773\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1610 - accuracy: 0.9464 - val_loss: 0.2289 - val_accuracy: 0.9224\n",
      "Epoch 50/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1478 - accuracy: 0.9460\n",
      "Epoch 00050: val_loss did not improve from 0.22773\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.1601 - accuracy: 0.9433 - val_loss: 0.2310 - val_accuracy: 0.9224\n",
      "Epoch 51/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1579 - accuracy: 0.9440\n",
      "Epoch 00051: val_loss did not improve from 0.22773\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.1584 - accuracy: 0.9433 - val_loss: 0.2380 - val_accuracy: 0.9193\n",
      "Epoch 52/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1759 - accuracy: 0.9400\n",
      "Epoch 00052: val_loss did not improve from 0.22773\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.1606 - accuracy: 0.9433 - val_loss: 0.2350 - val_accuracy: 0.9193\n",
      "Epoch 53/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1537 - accuracy: 0.9440\n",
      "Epoch 00053: val_loss did not improve from 0.22773\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.1581 - accuracy: 0.9464 - val_loss: 0.2299 - val_accuracy: 0.9193\n",
      "Epoch 54/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1581 - accuracy: 0.9440\n",
      "Epoch 00054: val_loss improved from 0.22773 to 0.22410, saving model to ./model\\54-0.2241.hdf5\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.1555 - accuracy: 0.9479 - val_loss: 0.2241 - val_accuracy: 0.9161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1481 - accuracy: 0.9460\n",
      "Epoch 00055: val_loss improved from 0.22410 to 0.22050, saving model to ./model\\55-0.2205.hdf5\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.1539 - accuracy: 0.9464 - val_loss: 0.2205 - val_accuracy: 0.9130\n",
      "Epoch 56/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1492 - accuracy: 0.9460\n",
      "Epoch 00056: val_loss improved from 0.22050 to 0.21903, saving model to ./model\\56-0.2190.hdf5\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.1541 - accuracy: 0.9449 - val_loss: 0.2190 - val_accuracy: 0.9161\n",
      "Epoch 57/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1687 - accuracy: 0.9420\n",
      "Epoch 00057: val_loss improved from 0.21903 to 0.21861, saving model to ./model\\57-0.2186.hdf5\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.1546 - accuracy: 0.9464 - val_loss: 0.2186 - val_accuracy: 0.9130\n",
      "Epoch 58/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1529 - accuracy: 0.9480\n",
      "Epoch 00058: val_loss did not improve from 0.21861\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1537 - accuracy: 0.9464 - val_loss: 0.2199 - val_accuracy: 0.9161\n",
      "Epoch 59/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1501 - accuracy: 0.9480\n",
      "Epoch 00059: val_loss did not improve from 0.21861\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.1522 - accuracy: 0.9449 - val_loss: 0.2229 - val_accuracy: 0.9161\n",
      "Epoch 60/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1494 - accuracy: 0.9460\n",
      "Epoch 00060: val_loss did not improve from 0.21861\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.1518 - accuracy: 0.9464 - val_loss: 0.2250 - val_accuracy: 0.9161\n",
      "Epoch 61/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1589 - accuracy: 0.9460\n",
      "Epoch 00061: val_loss did not improve from 0.21861\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.1517 - accuracy: 0.9464 - val_loss: 0.2243 - val_accuracy: 0.9161\n",
      "Epoch 62/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1551 - accuracy: 0.9460\n",
      "Epoch 00062: val_loss did not improve from 0.21861\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1510 - accuracy: 0.9464 - val_loss: 0.2224 - val_accuracy: 0.9161\n",
      "Epoch 63/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1378 - accuracy: 0.9520\n",
      "Epoch 00063: val_loss did not improve from 0.21861\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.1502 - accuracy: 0.9464 - val_loss: 0.2206 - val_accuracy: 0.9161\n",
      "Epoch 64/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1502 - accuracy: 0.9460\n",
      "Epoch 00064: val_loss did not improve from 0.21861\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.1496 - accuracy: 0.9479 - val_loss: 0.2188 - val_accuracy: 0.9193\n",
      "Epoch 65/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1586 - accuracy: 0.9420\n",
      "Epoch 00065: val_loss improved from 0.21861 to 0.21815, saving model to ./model\\65-0.2182.hdf5\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.1493 - accuracy: 0.9449 - val_loss: 0.2182 - val_accuracy: 0.9161\n",
      "Epoch 66/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1508 - accuracy: 0.9460\n",
      "Epoch 00066: val_loss did not improve from 0.21815\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.1491 - accuracy: 0.9449 - val_loss: 0.2183 - val_accuracy: 0.9161\n",
      "Epoch 67/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1538 - accuracy: 0.9400\n",
      "Epoch 00067: val_loss did not improve from 0.21815\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.1488 - accuracy: 0.9449 - val_loss: 0.2183 - val_accuracy: 0.9161\n",
      "Epoch 68/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1625 - accuracy: 0.9420\n",
      "Epoch 00068: val_loss improved from 0.21815 to 0.21805, saving model to ./model\\68-0.2180.hdf5\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.1482 - accuracy: 0.9449 - val_loss: 0.2180 - val_accuracy: 0.9161\n",
      "Epoch 69/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1338 - accuracy: 0.9480\n",
      "Epoch 00069: val_loss did not improve from 0.21805\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.1478 - accuracy: 0.9449 - val_loss: 0.2189 - val_accuracy: 0.9161\n",
      "Epoch 70/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1495 - accuracy: 0.9440\n",
      "Epoch 00070: val_loss did not improve from 0.21805\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1475 - accuracy: 0.9464 - val_loss: 0.2199 - val_accuracy: 0.9161\n",
      "Epoch 71/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1494 - accuracy: 0.9440\n",
      "Epoch 00071: val_loss did not improve from 0.21805\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1472 - accuracy: 0.9464 - val_loss: 0.2199 - val_accuracy: 0.9161\n",
      "Epoch 72/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1450 - accuracy: 0.9500\n",
      "Epoch 00072: val_loss did not improve from 0.21805\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.1468 - accuracy: 0.9479 - val_loss: 0.2186 - val_accuracy: 0.9193\n",
      "Epoch 73/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1487 - accuracy: 0.9480\n",
      "Epoch 00073: val_loss improved from 0.21805 to 0.21671, saving model to ./model\\73-0.2167.hdf5\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.1460 - accuracy: 0.9449 - val_loss: 0.2167 - val_accuracy: 0.9193\n",
      "Epoch 74/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1516 - accuracy: 0.9420\n",
      "Epoch 00074: val_loss improved from 0.21671 to 0.21575, saving model to ./model\\74-0.2157.hdf5\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.1466 - accuracy: 0.9403 - val_loss: 0.2157 - val_accuracy: 0.9193\n",
      "Epoch 75/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1517 - accuracy: 0.9440\n",
      "Epoch 00075: val_loss improved from 0.21575 to 0.21491, saving model to ./model\\75-0.2149.hdf5\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.1467 - accuracy: 0.9418 - val_loss: 0.2149 - val_accuracy: 0.9193\n",
      "Epoch 76/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1555 - accuracy: 0.9400\n",
      "Epoch 00076: val_loss improved from 0.21491 to 0.21349, saving model to ./model\\76-0.2135.hdf5\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.1455 - accuracy: 0.9449 - val_loss: 0.2135 - val_accuracy: 0.9193\n",
      "Epoch 77/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1544 - accuracy: 0.9400\n",
      "Epoch 00077: val_loss did not improve from 0.21349\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1449 - accuracy: 0.9464 - val_loss: 0.2138 - val_accuracy: 0.9193\n",
      "Epoch 78/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1406 - accuracy: 0.9500\n",
      "Epoch 00078: val_loss did not improve from 0.21349\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.1444 - accuracy: 0.9510 - val_loss: 0.2148 - val_accuracy: 0.9161\n",
      "Epoch 79/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1461 - accuracy: 0.9560\n",
      "Epoch 00079: val_loss did not improve from 0.21349\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.1443 - accuracy: 0.9525 - val_loss: 0.2159 - val_accuracy: 0.9161\n",
      "Epoch 80/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1583 - accuracy: 0.9500\n",
      "Epoch 00080: val_loss did not improve from 0.21349\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.1444 - accuracy: 0.9510 - val_loss: 0.2160 - val_accuracy: 0.9161\n",
      "Epoch 81/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1466 - accuracy: 0.9500\n",
      "Epoch 00081: val_loss did not improve from 0.21349\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.1442 - accuracy: 0.9525 - val_loss: 0.2148 - val_accuracy: 0.9161\n",
      "Epoch 82/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1444 - accuracy: 0.9520\n",
      "Epoch 00082: val_loss improved from 0.21349 to 0.21234, saving model to ./model\\82-0.2123.hdf5\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.1436 - accuracy: 0.9525 - val_loss: 0.2123 - val_accuracy: 0.9193\n",
      "Epoch 83/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1436 - accuracy: 0.9540\n",
      "Epoch 00083: val_loss improved from 0.21234 to 0.20939, saving model to ./model\\83-0.2094.hdf5\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.1430 - accuracy: 0.9495 - val_loss: 0.2094 - val_accuracy: 0.9224\n",
      "Epoch 84/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1444 - accuracy: 0.9460\n",
      "Epoch 00084: val_loss improved from 0.20939 to 0.20876, saving model to ./model\\84-0.2088.hdf5\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.1430 - accuracy: 0.9479 - val_loss: 0.2088 - val_accuracy: 0.9224\n",
      "Epoch 85/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1461 - accuracy: 0.9440\n",
      "Epoch 00085: val_loss did not improve from 0.20876\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1429 - accuracy: 0.9449 - val_loss: 0.2088 - val_accuracy: 0.9224\n",
      "Epoch 86/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1405 - accuracy: 0.9440\n",
      "Epoch 00086: val_loss did not improve from 0.20876\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.1422 - accuracy: 0.9449 - val_loss: 0.2101 - val_accuracy: 0.9224\n",
      "Epoch 87/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1398 - accuracy: 0.9500\n",
      "Epoch 00087: val_loss did not improve from 0.20876\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.1413 - accuracy: 0.9495 - val_loss: 0.2129 - val_accuracy: 0.9193\n",
      "Epoch 88/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1399 - accuracy: 0.9500\n",
      "Epoch 00088: val_loss did not improve from 0.20876\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.1416 - accuracy: 0.9510 - val_loss: 0.2146 - val_accuracy: 0.9161\n",
      "Epoch 89/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1409 - accuracy: 0.9540\n",
      "Epoch 00089: val_loss did not improve from 0.20876\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.1413 - accuracy: 0.9525 - val_loss: 0.2120 - val_accuracy: 0.9193\n",
      "Epoch 90/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1300 - accuracy: 0.9560\n",
      "Epoch 00090: val_loss improved from 0.20876 to 0.20856, saving model to ./model\\90-0.2086.hdf5\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.1405 - accuracy: 0.9495 - val_loss: 0.2086 - val_accuracy: 0.9224\n",
      "Epoch 91/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1482 - accuracy: 0.9400\n",
      "Epoch 00091: val_loss improved from 0.20856 to 0.20611, saving model to ./model\\91-0.2061.hdf5\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.1402 - accuracy: 0.9464 - val_loss: 0.2061 - val_accuracy: 0.9255\n",
      "Epoch 92/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1336 - accuracy: 0.9460\n",
      "Epoch 00092: val_loss improved from 0.20611 to 0.20548, saving model to ./model\\92-0.2055.hdf5\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.1409 - accuracy: 0.9464 - val_loss: 0.2055 - val_accuracy: 0.9224\n",
      "Epoch 93/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1418 - accuracy: 0.9440\n",
      "Epoch 00093: val_loss did not improve from 0.20548\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.1407 - accuracy: 0.9464 - val_loss: 0.2063 - val_accuracy: 0.9224\n",
      "Epoch 94/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1315 - accuracy: 0.9520\n",
      "Epoch 00094: val_loss did not improve from 0.20548\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1395 - accuracy: 0.9464 - val_loss: 0.2083 - val_accuracy: 0.9224\n",
      "Epoch 95/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1394 - accuracy: 0.9460\n",
      "Epoch 00095: val_loss did not improve from 0.20548\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1391 - accuracy: 0.9479 - val_loss: 0.2094 - val_accuracy: 0.9193\n",
      "Epoch 96/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1403 - accuracy: 0.9520\n",
      "Epoch 00096: val_loss did not improve from 0.20548\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.1389 - accuracy: 0.9510 - val_loss: 0.2077 - val_accuracy: 0.9193\n",
      "Epoch 97/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1474 - accuracy: 0.9460\n",
      "Epoch 00097: val_loss did not improve from 0.20548\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.1387 - accuracy: 0.9495 - val_loss: 0.2055 - val_accuracy: 0.9224\n",
      "Epoch 98/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1354 - accuracy: 0.9480\n",
      "Epoch 00098: val_loss improved from 0.20548 to 0.20532, saving model to ./model\\98-0.2053.hdf5\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.1383 - accuracy: 0.9479 - val_loss: 0.2053 - val_accuracy: 0.9224\n",
      "Epoch 99/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1238 - accuracy: 0.9540\n",
      "Epoch 00099: val_loss improved from 0.20532 to 0.20518, saving model to ./model\\99-0.2052.hdf5\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.1380 - accuracy: 0.9479 - val_loss: 0.2052 - val_accuracy: 0.9224\n",
      "Epoch 100/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1359 - accuracy: 0.9500\n",
      "Epoch 00100: val_loss improved from 0.20518 to 0.20402, saving model to ./model\\100-0.2040.hdf5\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.1377 - accuracy: 0.9510 - val_loss: 0.2040 - val_accuracy: 0.9224\n",
      "Epoch 101/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1346 - accuracy: 0.9480\n",
      "Epoch 00101: val_loss improved from 0.20402 to 0.20294, saving model to ./model\\101-0.2029.hdf5\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.1375 - accuracy: 0.9510 - val_loss: 0.2029 - val_accuracy: 0.9224\n",
      "Epoch 102/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1477 - accuracy: 0.9480\n",
      "Epoch 00102: val_loss improved from 0.20294 to 0.20218, saving model to ./model\\102-0.2022.hdf5\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.1374 - accuracy: 0.9525 - val_loss: 0.2022 - val_accuracy: 0.9224\n",
      "Epoch 103/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1246 - accuracy: 0.9580\n",
      "Epoch 00103: val_loss improved from 0.20218 to 0.20092, saving model to ./model\\103-0.2009.hdf5\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.1371 - accuracy: 0.9510 - val_loss: 0.2009 - val_accuracy: 0.9224\n",
      "Epoch 104/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1392 - accuracy: 0.9480\n",
      "Epoch 00104: val_loss improved from 0.20092 to 0.20010, saving model to ./model\\104-0.2001.hdf5\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.1365 - accuracy: 0.9510 - val_loss: 0.2001 - val_accuracy: 0.9255\n",
      "Epoch 105/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1427 - accuracy: 0.9480\n",
      "Epoch 00105: val_loss did not improve from 0.20010\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.1367 - accuracy: 0.9479 - val_loss: 0.2007 - val_accuracy: 0.9224\n",
      "Epoch 106/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1346 - accuracy: 0.9540\n",
      "Epoch 00106: val_loss did not improve from 0.20010\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.1367 - accuracy: 0.9449 - val_loss: 0.2010 - val_accuracy: 0.9224\n",
      "Epoch 107/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1326 - accuracy: 0.9460\n",
      "Epoch 00107: val_loss did not improve from 0.20010\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.1360 - accuracy: 0.9449 - val_loss: 0.2005 - val_accuracy: 0.9255\n",
      "Epoch 108/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1467 - accuracy: 0.9400\n",
      "Epoch 00108: val_loss did not improve from 0.20010\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.1352 - accuracy: 0.9479 - val_loss: 0.2022 - val_accuracy: 0.9224\n",
      "Epoch 109/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1564 - accuracy: 0.9400\n",
      "Epoch 00109: val_loss did not improve from 0.20010\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1350 - accuracy: 0.9495 - val_loss: 0.2045 - val_accuracy: 0.9224\n",
      "Epoch 110/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1347 - accuracy: 0.9520\n",
      "Epoch 00110: val_loss did not improve from 0.20010\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1353 - accuracy: 0.9510 - val_loss: 0.2048 - val_accuracy: 0.9224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 111/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1093 - accuracy: 0.9540\n",
      "Epoch 00111: val_loss did not improve from 0.20010\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.1347 - accuracy: 0.9495 - val_loss: 0.2013 - val_accuracy: 0.9255\n",
      "Epoch 112/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1085 - accuracy: 0.9640\n",
      "Epoch 00112: val_loss improved from 0.20010 to 0.19829, saving model to ./model\\112-0.1983.hdf5\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.1338 - accuracy: 0.9495 - val_loss: 0.1983 - val_accuracy: 0.9224\n",
      "Epoch 113/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1425 - accuracy: 0.9480\n",
      "Epoch 00113: val_loss improved from 0.19829 to 0.19774, saving model to ./model\\113-0.1977.hdf5\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.1347 - accuracy: 0.9525 - val_loss: 0.1977 - val_accuracy: 0.9193\n",
      "Epoch 114/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1233 - accuracy: 0.9620\n",
      "Epoch 00114: val_loss did not improve from 0.19774\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1346 - accuracy: 0.9525 - val_loss: 0.1985 - val_accuracy: 0.9224\n",
      "Epoch 115/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1275 - accuracy: 0.9560\n",
      "Epoch 00115: val_loss did not improve from 0.19774\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.1337 - accuracy: 0.9525 - val_loss: 0.1988 - val_accuracy: 0.9224\n",
      "Epoch 116/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1380 - accuracy: 0.9480\n",
      "Epoch 00116: val_loss did not improve from 0.19774\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.1329 - accuracy: 0.9495 - val_loss: 0.1986 - val_accuracy: 0.9255\n",
      "Epoch 117/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1418 - accuracy: 0.9460\n",
      "Epoch 00117: val_loss did not improve from 0.19774\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.1324 - accuracy: 0.9510 - val_loss: 0.1983 - val_accuracy: 0.9255\n",
      "Epoch 118/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1482 - accuracy: 0.9420\n",
      "Epoch 00118: val_loss did not improve from 0.19774\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.1323 - accuracy: 0.9510 - val_loss: 0.1981 - val_accuracy: 0.9224\n",
      "Epoch 119/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1324 - accuracy: 0.9500\n",
      "Epoch 00119: val_loss did not improve from 0.19774\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.1321 - accuracy: 0.9525 - val_loss: 0.1989 - val_accuracy: 0.9224\n",
      "Epoch 120/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1232 - accuracy: 0.9600\n",
      "Epoch 00120: val_loss did not improve from 0.19774\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1321 - accuracy: 0.9525 - val_loss: 0.1994 - val_accuracy: 0.9255\n",
      "Epoch 121/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1322 - accuracy: 0.9500\n",
      "Epoch 00121: val_loss did not improve from 0.19774\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1319 - accuracy: 0.9525 - val_loss: 0.1979 - val_accuracy: 0.9224\n",
      "Epoch 122/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1352 - accuracy: 0.9540\n",
      "Epoch 00122: val_loss improved from 0.19774 to 0.19503, saving model to ./model\\122-0.1950.hdf5\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.1311 - accuracy: 0.9541 - val_loss: 0.1950 - val_accuracy: 0.9255\n",
      "Epoch 123/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1224 - accuracy: 0.9600\n",
      "Epoch 00123: val_loss improved from 0.19503 to 0.19343, saving model to ./model\\123-0.1934.hdf5\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.1305 - accuracy: 0.9556 - val_loss: 0.1934 - val_accuracy: 0.9193\n",
      "Epoch 124/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1326 - accuracy: 0.9560\n",
      "Epoch 00124: val_loss improved from 0.19343 to 0.19315, saving model to ./model\\124-0.1932.hdf5\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.1315 - accuracy: 0.9556 - val_loss: 0.1932 - val_accuracy: 0.9224\n",
      "Epoch 125/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1332 - accuracy: 0.9540\n",
      "Epoch 00125: val_loss did not improve from 0.19315\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.1320 - accuracy: 0.9541 - val_loss: 0.1933 - val_accuracy: 0.9193\n",
      "Epoch 126/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1218 - accuracy: 0.9580\n",
      "Epoch 00126: val_loss did not improve from 0.19315\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.1311 - accuracy: 0.9571 - val_loss: 0.1956 - val_accuracy: 0.9255\n",
      "Epoch 127/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1359 - accuracy: 0.9560\n",
      "Epoch 00127: val_loss did not improve from 0.19315\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.1299 - accuracy: 0.9556 - val_loss: 0.1970 - val_accuracy: 0.9255\n",
      "Epoch 128/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1235 - accuracy: 0.9540\n",
      "Epoch 00128: val_loss did not improve from 0.19315\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.1310 - accuracy: 0.9541 - val_loss: 0.1956 - val_accuracy: 0.9255\n",
      "Epoch 129/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1381 - accuracy: 0.9500\n",
      "Epoch 00129: val_loss improved from 0.19315 to 0.19158, saving model to ./model\\129-0.1916.hdf5\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.1299 - accuracy: 0.9556 - val_loss: 0.1916 - val_accuracy: 0.9193\n",
      "Epoch 130/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1169 - accuracy: 0.9620\n",
      "Epoch 00130: val_loss improved from 0.19158 to 0.19022, saving model to ./model\\130-0.1902.hdf5\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.1285 - accuracy: 0.9587 - val_loss: 0.1902 - val_accuracy: 0.9161\n",
      "Epoch 131/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1205 - accuracy: 0.9580\n",
      "Epoch 00131: val_loss improved from 0.19022 to 0.18968, saving model to ./model\\131-0.1897.hdf5\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.1287 - accuracy: 0.9556 - val_loss: 0.1897 - val_accuracy: 0.9193\n",
      "Epoch 132/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1284 - accuracy: 0.9620\n",
      "Epoch 00132: val_loss improved from 0.18968 to 0.18911, saving model to ./model\\132-0.1891.hdf5\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.1292 - accuracy: 0.9587 - val_loss: 0.1891 - val_accuracy: 0.9224\n",
      "Epoch 133/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1421 - accuracy: 0.9500\n",
      "Epoch 00133: val_loss improved from 0.18911 to 0.18862, saving model to ./model\\133-0.1886.hdf5\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.1287 - accuracy: 0.9587 - val_loss: 0.1886 - val_accuracy: 0.9161\n",
      "Epoch 134/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1199 - accuracy: 0.9560\n",
      "Epoch 00134: val_loss did not improve from 0.18862\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1278 - accuracy: 0.9571 - val_loss: 0.1896 - val_accuracy: 0.9224\n",
      "Epoch 135/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1315 - accuracy: 0.9560\n",
      "Epoch 00135: val_loss did not improve from 0.18862\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1271 - accuracy: 0.9571 - val_loss: 0.1900 - val_accuracy: 0.9255\n",
      "Epoch 136/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1205 - accuracy: 0.9580\n",
      "Epoch 00136: val_loss did not improve from 0.18862\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.1271 - accuracy: 0.9556 - val_loss: 0.1897 - val_accuracy: 0.9255\n",
      "Epoch 137/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1373 - accuracy: 0.9460\n",
      "Epoch 00137: val_loss improved from 0.18862 to 0.18831, saving model to ./model\\137-0.1883.hdf5\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.1271 - accuracy: 0.9556 - val_loss: 0.1883 - val_accuracy: 0.9255\n",
      "Epoch 138/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1407 - accuracy: 0.9500\n",
      "Epoch 00138: val_loss did not improve from 0.18831\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.1265 - accuracy: 0.9556 - val_loss: 0.1885 - val_accuracy: 0.9286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 139/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1177 - accuracy: 0.9580\n",
      "Epoch 00139: val_loss improved from 0.18831 to 0.18795, saving model to ./model\\139-0.1879.hdf5\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.1267 - accuracy: 0.9556 - val_loss: 0.1879 - val_accuracy: 0.9286\n",
      "Epoch 140/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1100 - accuracy: 0.9640\n",
      "Epoch 00140: val_loss improved from 0.18795 to 0.18464, saving model to ./model\\140-0.1846.hdf5\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.1260 - accuracy: 0.9556 - val_loss: 0.1846 - val_accuracy: 0.9224\n",
      "Epoch 141/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1216 - accuracy: 0.9560\n",
      "Epoch 00141: val_loss improved from 0.18464 to 0.18359, saving model to ./model\\141-0.1836.hdf5\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.1256 - accuracy: 0.9571 - val_loss: 0.1836 - val_accuracy: 0.9224\n",
      "Epoch 142/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1244 - accuracy: 0.9580\n",
      "Epoch 00142: val_loss improved from 0.18359 to 0.18337, saving model to ./model\\142-0.1834.hdf5\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.1278 - accuracy: 0.9571 - val_loss: 0.1834 - val_accuracy: 0.9224\n",
      "Epoch 143/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1198 - accuracy: 0.9660\n",
      "Epoch 00143: val_loss did not improve from 0.18337\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.1266 - accuracy: 0.9587 - val_loss: 0.1836 - val_accuracy: 0.9224\n",
      "Epoch 144/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1176 - accuracy: 0.9580\n",
      "Epoch 00144: val_loss did not improve from 0.18337\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.1248 - accuracy: 0.9587 - val_loss: 0.1858 - val_accuracy: 0.9286\n",
      "Epoch 145/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1127 - accuracy: 0.9580\n",
      "Epoch 00145: val_loss did not improve from 0.18337\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1251 - accuracy: 0.9556 - val_loss: 0.1860 - val_accuracy: 0.9255\n",
      "Epoch 146/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1341 - accuracy: 0.9560\n",
      "Epoch 00146: val_loss did not improve from 0.18337\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1251 - accuracy: 0.9587 - val_loss: 0.1838 - val_accuracy: 0.9255\n",
      "Epoch 147/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1190 - accuracy: 0.9560\n",
      "Epoch 00147: val_loss improved from 0.18337 to 0.18264, saving model to ./model\\147-0.1826.hdf5\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.1243 - accuracy: 0.9571 - val_loss: 0.1826 - val_accuracy: 0.9224\n",
      "Epoch 148/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1291 - accuracy: 0.9520\n",
      "Epoch 00148: val_loss improved from 0.18264 to 0.18098, saving model to ./model\\148-0.1810.hdf5\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.1241 - accuracy: 0.9571 - val_loss: 0.1810 - val_accuracy: 0.9224\n",
      "Epoch 149/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1258 - accuracy: 0.9580\n",
      "Epoch 00149: val_loss did not improve from 0.18098\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1241 - accuracy: 0.9587 - val_loss: 0.1812 - val_accuracy: 0.9224\n",
      "Epoch 150/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1206 - accuracy: 0.9600\n",
      "Epoch 00150: val_loss did not improve from 0.18098\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1236 - accuracy: 0.9571 - val_loss: 0.1825 - val_accuracy: 0.9224\n",
      "Epoch 151/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1298 - accuracy: 0.9520\n",
      "Epoch 00151: val_loss did not improve from 0.18098\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1238 - accuracy: 0.9541 - val_loss: 0.1815 - val_accuracy: 0.9224\n",
      "Epoch 152/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1202 - accuracy: 0.9540\n",
      "Epoch 00152: val_loss improved from 0.18098 to 0.18030, saving model to ./model\\152-0.1803.hdf5\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.1234 - accuracy: 0.9556 - val_loss: 0.1803 - val_accuracy: 0.9255\n",
      "Epoch 153/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1236 - accuracy: 0.9580\n",
      "Epoch 00153: val_loss improved from 0.18030 to 0.17868, saving model to ./model\\153-0.1787.hdf5\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.1228 - accuracy: 0.9571 - val_loss: 0.1787 - val_accuracy: 0.9255\n",
      "Epoch 154/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1236 - accuracy: 0.9600\n",
      "Epoch 00154: val_loss improved from 0.17868 to 0.17771, saving model to ./model\\154-0.1777.hdf5\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.1223 - accuracy: 0.9587 - val_loss: 0.1777 - val_accuracy: 0.9255\n",
      "Epoch 155/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1291 - accuracy: 0.9560\n",
      "Epoch 00155: val_loss improved from 0.17771 to 0.17709, saving model to ./model\\155-0.1771.hdf5\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.1221 - accuracy: 0.9587 - val_loss: 0.1771 - val_accuracy: 0.9317\n",
      "Epoch 156/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1254 - accuracy: 0.9600\n",
      "Epoch 00156: val_loss improved from 0.17709 to 0.17690, saving model to ./model\\156-0.1769.hdf5\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.1219 - accuracy: 0.9602 - val_loss: 0.1769 - val_accuracy: 0.9255\n",
      "Epoch 157/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1169 - accuracy: 0.9600\n",
      "Epoch 00157: val_loss did not improve from 0.17690\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.1211 - accuracy: 0.9602 - val_loss: 0.1778 - val_accuracy: 0.9255\n",
      "Epoch 158/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1264 - accuracy: 0.9560\n",
      "Epoch 00158: val_loss did not improve from 0.17690\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1207 - accuracy: 0.9587 - val_loss: 0.1790 - val_accuracy: 0.9255\n",
      "Epoch 159/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1318 - accuracy: 0.9560\n",
      "Epoch 00159: val_loss did not improve from 0.17690\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1211 - accuracy: 0.9571 - val_loss: 0.1788 - val_accuracy: 0.9255\n",
      "Epoch 160/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1189 - accuracy: 0.9600\n",
      "Epoch 00160: val_loss improved from 0.17690 to 0.17659, saving model to ./model\\160-0.1766.hdf5\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.1207 - accuracy: 0.9571 - val_loss: 0.1766 - val_accuracy: 0.9255\n",
      "Epoch 161/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1277 - accuracy: 0.9580\n",
      "Epoch 00161: val_loss improved from 0.17659 to 0.17536, saving model to ./model\\161-0.1754.hdf5\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.1202 - accuracy: 0.9602 - val_loss: 0.1754 - val_accuracy: 0.9317\n",
      "Epoch 162/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1012 - accuracy: 0.9700\n",
      "Epoch 00162: val_loss improved from 0.17536 to 0.17532, saving model to ./model\\162-0.1753.hdf5\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.1208 - accuracy: 0.9617 - val_loss: 0.1753 - val_accuracy: 0.9348\n",
      "Epoch 163/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1187 - accuracy: 0.9640\n",
      "Epoch 00163: val_loss did not improve from 0.17532\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1204 - accuracy: 0.9617 - val_loss: 0.1766 - val_accuracy: 0.9286\n",
      "Epoch 164/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1231 - accuracy: 0.9580\n",
      "Epoch 00164: val_loss did not improve from 0.17532\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1201 - accuracy: 0.9571 - val_loss: 0.1798 - val_accuracy: 0.9224\n",
      "Epoch 165/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1161 - accuracy: 0.9560\n",
      "Epoch 00165: val_loss did not improve from 0.17532\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.1204 - accuracy: 0.9556 - val_loss: 0.1800 - val_accuracy: 0.9224\n",
      "Epoch 166/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1093 - accuracy: 0.9640\n",
      "Epoch 00166: val_loss did not improve from 0.17532\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1205 - accuracy: 0.9571 - val_loss: 0.1768 - val_accuracy: 0.9255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 167/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1213 - accuracy: 0.9540\n",
      "Epoch 00167: val_loss improved from 0.17532 to 0.17350, saving model to ./model\\167-0.1735.hdf5\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.1192 - accuracy: 0.9571 - val_loss: 0.1735 - val_accuracy: 0.9286\n",
      "Epoch 168/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1129 - accuracy: 0.9600\n",
      "Epoch 00168: val_loss improved from 0.17350 to 0.17310, saving model to ./model\\168-0.1731.hdf5\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.1187 - accuracy: 0.9602 - val_loss: 0.1731 - val_accuracy: 0.9286\n",
      "Epoch 169/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1298 - accuracy: 0.9620\n",
      "Epoch 00169: val_loss improved from 0.17310 to 0.17179, saving model to ./model\\169-0.1718.hdf5\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.1191 - accuracy: 0.9617 - val_loss: 0.1718 - val_accuracy: 0.9255\n",
      "Epoch 170/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1292 - accuracy: 0.9600\n",
      "Epoch 00170: val_loss did not improve from 0.17179\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.1180 - accuracy: 0.9617 - val_loss: 0.1735 - val_accuracy: 0.9286\n",
      "Epoch 171/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1291 - accuracy: 0.9520\n",
      "Epoch 00171: val_loss did not improve from 0.17179\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1188 - accuracy: 0.9571 - val_loss: 0.1764 - val_accuracy: 0.9224\n",
      "Epoch 172/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1109 - accuracy: 0.9580\n",
      "Epoch 00172: val_loss did not improve from 0.17179\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.1199 - accuracy: 0.9556 - val_loss: 0.1741 - val_accuracy: 0.9224\n",
      "Epoch 173/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1202 - accuracy: 0.9540\n",
      "Epoch 00173: val_loss improved from 0.17179 to 0.17022, saving model to ./model\\173-0.1702.hdf5\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.1177 - accuracy: 0.9556 - val_loss: 0.1702 - val_accuracy: 0.9348\n",
      "Epoch 174/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1275 - accuracy: 0.9580\n",
      "Epoch 00174: val_loss improved from 0.17022 to 0.16992, saving model to ./model\\174-0.1699.hdf5\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.1182 - accuracy: 0.9617 - val_loss: 0.1699 - val_accuracy: 0.9286\n",
      "Epoch 175/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1116 - accuracy: 0.9620\n",
      "Epoch 00175: val_loss improved from 0.16992 to 0.16905, saving model to ./model\\175-0.1691.hdf5\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.1192 - accuracy: 0.9632 - val_loss: 0.1691 - val_accuracy: 0.9348\n",
      "Epoch 176/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1246 - accuracy: 0.9640\n",
      "Epoch 00176: val_loss did not improve from 0.16905\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.1174 - accuracy: 0.9632 - val_loss: 0.1695 - val_accuracy: 0.9317\n",
      "Epoch 177/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1226 - accuracy: 0.9560\n",
      "Epoch 00177: val_loss did not improve from 0.16905\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.1158 - accuracy: 0.9602 - val_loss: 0.1740 - val_accuracy: 0.9286\n",
      "Epoch 178/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1167 - accuracy: 0.9600\n",
      "Epoch 00178: val_loss did not improve from 0.16905\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1181 - accuracy: 0.9571 - val_loss: 0.1778 - val_accuracy: 0.9224\n",
      "Epoch 179/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1084 - accuracy: 0.9580\n",
      "Epoch 00179: val_loss did not improve from 0.16905\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.1208 - accuracy: 0.9525 - val_loss: 0.1721 - val_accuracy: 0.9286\n",
      "Epoch 180/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1297 - accuracy: 0.9540\n",
      "Epoch 00180: val_loss improved from 0.16905 to 0.16504, saving model to ./model\\180-0.1650.hdf5\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.1188 - accuracy: 0.9571 - val_loss: 0.1650 - val_accuracy: 0.9317\n",
      "Epoch 181/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1148 - accuracy: 0.9600\n",
      "Epoch 00181: val_loss did not improve from 0.16504\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.1152 - accuracy: 0.9602 - val_loss: 0.1652 - val_accuracy: 0.9286\n",
      "Epoch 182/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1089 - accuracy: 0.9720\n",
      "Epoch 00182: val_loss improved from 0.16504 to 0.16461, saving model to ./model\\182-0.1646.hdf5\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.1169 - accuracy: 0.9663 - val_loss: 0.1646 - val_accuracy: 0.9286\n",
      "Epoch 183/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1047 - accuracy: 0.9700\n",
      "Epoch 00183: val_loss improved from 0.16461 to 0.16245, saving model to ./model\\183-0.1624.hdf5\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.1167 - accuracy: 0.9648 - val_loss: 0.1624 - val_accuracy: 0.9317\n",
      "Epoch 184/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1167 - accuracy: 0.9600\n",
      "Epoch 00184: val_loss did not improve from 0.16245\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.1144 - accuracy: 0.9632 - val_loss: 0.1628 - val_accuracy: 0.9317\n",
      "Epoch 185/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1056 - accuracy: 0.9600\n",
      "Epoch 00185: val_loss did not improve from 0.16245\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1147 - accuracy: 0.9602 - val_loss: 0.1651 - val_accuracy: 0.9255\n",
      "Epoch 186/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1225 - accuracy: 0.9560\n",
      "Epoch 00186: val_loss did not improve from 0.16245\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.1161 - accuracy: 0.9556 - val_loss: 0.1634 - val_accuracy: 0.9286\n",
      "Epoch 187/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1114 - accuracy: 0.9560\n",
      "Epoch 00187: val_loss improved from 0.16245 to 0.16020, saving model to ./model\\187-0.1602.hdf5\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.1152 - accuracy: 0.9571 - val_loss: 0.1602 - val_accuracy: 0.9348\n",
      "Epoch 188/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1192 - accuracy: 0.9560\n",
      "Epoch 00188: val_loss improved from 0.16020 to 0.16016, saving model to ./model\\188-0.1602.hdf5\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.1141 - accuracy: 0.9587 - val_loss: 0.1602 - val_accuracy: 0.9348\n",
      "Epoch 189/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1164 - accuracy: 0.9620\n",
      "Epoch 00189: val_loss improved from 0.16016 to 0.16004, saving model to ./model\\189-0.1600.hdf5\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.1137 - accuracy: 0.9632 - val_loss: 0.1600 - val_accuracy: 0.9348\n",
      "Epoch 190/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0966 - accuracy: 0.9740\n",
      "Epoch 00190: val_loss improved from 0.16004 to 0.15985, saving model to ./model\\190-0.1599.hdf5\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.1135 - accuracy: 0.9648 - val_loss: 0.1599 - val_accuracy: 0.9348\n",
      "Epoch 191/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0988 - accuracy: 0.9680\n",
      "Epoch 00191: val_loss improved from 0.15985 to 0.15963, saving model to ./model\\191-0.1596.hdf5\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.1133 - accuracy: 0.9648 - val_loss: 0.1596 - val_accuracy: 0.9348\n",
      "Epoch 192/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1167 - accuracy: 0.9660\n",
      "Epoch 00192: val_loss did not improve from 0.15963\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.1131 - accuracy: 0.9648 - val_loss: 0.1600 - val_accuracy: 0.9348\n",
      "Epoch 193/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1202 - accuracy: 0.9660\n",
      "Epoch 00193: val_loss did not improve from 0.15963\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.1126 - accuracy: 0.9617 - val_loss: 0.1604 - val_accuracy: 0.9348\n",
      "Epoch 194/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1112 - accuracy: 0.9640\n",
      "Epoch 00194: val_loss improved from 0.15963 to 0.15922, saving model to ./model\\194-0.1592.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 56ms/step - loss: 0.1124 - accuracy: 0.9602 - val_loss: 0.1592 - val_accuracy: 0.9348\n",
      "Epoch 195/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1210 - accuracy: 0.9580\n",
      "Epoch 00195: val_loss improved from 0.15922 to 0.15819, saving model to ./model\\195-0.1582.hdf5\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.1119 - accuracy: 0.9617 - val_loss: 0.1582 - val_accuracy: 0.9317\n",
      "Epoch 196/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1069 - accuracy: 0.9640\n",
      "Epoch 00196: val_loss improved from 0.15819 to 0.15806, saving model to ./model\\196-0.1581.hdf5\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.1125 - accuracy: 0.9617 - val_loss: 0.1581 - val_accuracy: 0.9348\n",
      "Epoch 197/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1052 - accuracy: 0.9620\n",
      "Epoch 00197: val_loss did not improve from 0.15806\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.1118 - accuracy: 0.9617 - val_loss: 0.1590 - val_accuracy: 0.9348\n",
      "Epoch 198/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1260 - accuracy: 0.9560\n",
      "Epoch 00198: val_loss did not improve from 0.15806\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1115 - accuracy: 0.9617 - val_loss: 0.1595 - val_accuracy: 0.9286\n",
      "Epoch 199/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1077 - accuracy: 0.9640\n",
      "Epoch 00199: val_loss did not improve from 0.15806\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1118 - accuracy: 0.9602 - val_loss: 0.1586 - val_accuracy: 0.9317\n",
      "Epoch 200/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0947 - accuracy: 0.9640\n",
      "Epoch 00200: val_loss improved from 0.15806 to 0.15664, saving model to ./model\\200-0.1566.hdf5\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.1109 - accuracy: 0.9617 - val_loss: 0.1566 - val_accuracy: 0.9348\n",
      "Epoch 201/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1190 - accuracy: 0.9600\n",
      "Epoch 00201: val_loss did not improve from 0.15664\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1117 - accuracy: 0.9632 - val_loss: 0.1567 - val_accuracy: 0.9317\n",
      "Epoch 202/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1164 - accuracy: 0.9620\n",
      "Epoch 00202: val_loss did not improve from 0.15664\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.1110 - accuracy: 0.9648 - val_loss: 0.1568 - val_accuracy: 0.9348\n",
      "Epoch 203/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1106 - accuracy: 0.9680\n",
      "Epoch 00203: val_loss did not improve from 0.15664\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.1098 - accuracy: 0.9648 - val_loss: 0.1588 - val_accuracy: 0.9286\n",
      "Epoch 204/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1102 - accuracy: 0.9580\n",
      "Epoch 00204: val_loss did not improve from 0.15664\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1108 - accuracy: 0.9587 - val_loss: 0.1601 - val_accuracy: 0.9286\n",
      "Epoch 205/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1173 - accuracy: 0.9600\n",
      "Epoch 00205: val_loss did not improve from 0.15664\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.1111 - accuracy: 0.9602 - val_loss: 0.1571 - val_accuracy: 0.9348\n",
      "Epoch 206/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1187 - accuracy: 0.9580\n",
      "Epoch 00206: val_loss improved from 0.15664 to 0.15460, saving model to ./model\\206-0.1546.hdf5\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.1096 - accuracy: 0.9617 - val_loss: 0.1546 - val_accuracy: 0.9348\n",
      "Epoch 207/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1114 - accuracy: 0.9640\n",
      "Epoch 00207: val_loss improved from 0.15460 to 0.15360, saving model to ./model\\207-0.1536.hdf5\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.1092 - accuracy: 0.9648 - val_loss: 0.1536 - val_accuracy: 0.9286\n",
      "Epoch 208/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1063 - accuracy: 0.9660\n",
      "Epoch 00208: val_loss improved from 0.15360 to 0.15264, saving model to ./model\\208-0.1526.hdf5\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.1099 - accuracy: 0.9648 - val_loss: 0.1526 - val_accuracy: 0.9286\n",
      "Epoch 209/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1151 - accuracy: 0.9600\n",
      "Epoch 00209: val_loss improved from 0.15264 to 0.15258, saving model to ./model\\209-0.1526.hdf5\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.1089 - accuracy: 0.9648 - val_loss: 0.1526 - val_accuracy: 0.9348\n",
      "Epoch 210/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1108 - accuracy: 0.9580\n",
      "Epoch 00210: val_loss did not improve from 0.15258\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1085 - accuracy: 0.9602 - val_loss: 0.1548 - val_accuracy: 0.9317\n",
      "Epoch 211/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0993 - accuracy: 0.9640\n",
      "Epoch 00211: val_loss did not improve from 0.15258\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.1102 - accuracy: 0.9571 - val_loss: 0.1551 - val_accuracy: 0.9317\n",
      "Epoch 212/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1237 - accuracy: 0.9520\n",
      "Epoch 00212: val_loss did not improve from 0.15258\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1100 - accuracy: 0.9571 - val_loss: 0.1526 - val_accuracy: 0.9317\n",
      "Epoch 213/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1113 - accuracy: 0.9580\n",
      "Epoch 00213: val_loss improved from 0.15258 to 0.15150, saving model to ./model\\213-0.1515.hdf5\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.1088 - accuracy: 0.9632 - val_loss: 0.1515 - val_accuracy: 0.9348\n",
      "Epoch 214/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1107 - accuracy: 0.9660\n",
      "Epoch 00214: val_loss improved from 0.15150 to 0.15139, saving model to ./model\\214-0.1514.hdf5\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.1075 - accuracy: 0.9663 - val_loss: 0.1514 - val_accuracy: 0.9348\n",
      "Epoch 215/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1056 - accuracy: 0.9640\n",
      "Epoch 00215: val_loss improved from 0.15139 to 0.15104, saving model to ./model\\215-0.1510.hdf5\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.1073 - accuracy: 0.9632 - val_loss: 0.1510 - val_accuracy: 0.9348\n",
      "Epoch 216/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1108 - accuracy: 0.9640\n",
      "Epoch 00216: val_loss improved from 0.15104 to 0.14990, saving model to ./model\\216-0.1499.hdf5\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.1073 - accuracy: 0.9648 - val_loss: 0.1499 - val_accuracy: 0.9348\n",
      "Epoch 217/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0910 - accuracy: 0.9700\n",
      "Epoch 00217: val_loss improved from 0.14990 to 0.14852, saving model to ./model\\217-0.1485.hdf5\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.1067 - accuracy: 0.9648 - val_loss: 0.1485 - val_accuracy: 0.9348\n",
      "Epoch 218/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1054 - accuracy: 0.9660\n",
      "Epoch 00218: val_loss improved from 0.14852 to 0.14686, saving model to ./model\\218-0.1469.hdf5\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.1063 - accuracy: 0.9648 - val_loss: 0.1469 - val_accuracy: 0.9379\n",
      "Epoch 219/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1080 - accuracy: 0.9720\n",
      "Epoch 00219: val_loss improved from 0.14686 to 0.14590, saving model to ./model\\219-0.1459.hdf5\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.1060 - accuracy: 0.9678 - val_loss: 0.1459 - val_accuracy: 0.9348\n",
      "Epoch 220/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1076 - accuracy: 0.9700\n",
      "Epoch 00220: val_loss did not improve from 0.14590\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.1063 - accuracy: 0.9694 - val_loss: 0.1461 - val_accuracy: 0.9348\n",
      "Epoch 221/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1067 - accuracy: 0.9700\n",
      "Epoch 00221: val_loss improved from 0.14590 to 0.14512, saving model to ./model\\221-0.1451.hdf5\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.1067 - accuracy: 0.9694 - val_loss: 0.1451 - val_accuracy: 0.9348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 222/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1105 - accuracy: 0.9660\n",
      "Epoch 00222: val_loss improved from 0.14512 to 0.14340, saving model to ./model\\222-0.1434.hdf5\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.1055 - accuracy: 0.9678 - val_loss: 0.1434 - val_accuracy: 0.9317\n",
      "Epoch 223/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0965 - accuracy: 0.9660\n",
      "Epoch 00223: val_loss did not improve from 0.14340\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.1049 - accuracy: 0.9602 - val_loss: 0.1456 - val_accuracy: 0.9317\n",
      "Epoch 224/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1103 - accuracy: 0.9600\n",
      "Epoch 00224: val_loss improved from 0.14340 to 0.14172, saving model to ./model\\224-0.1417.hdf5\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.1070 - accuracy: 0.9587 - val_loss: 0.1417 - val_accuracy: 0.9317\n",
      "Epoch 225/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0913 - accuracy: 0.9660\n",
      "Epoch 00225: val_loss did not improve from 0.14172\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.1036 - accuracy: 0.9632 - val_loss: 0.1439 - val_accuracy: 0.9348\n",
      "Epoch 226/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1029 - accuracy: 0.9680\n",
      "Epoch 00226: val_loss did not improve from 0.14172\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.1058 - accuracy: 0.9694 - val_loss: 0.1435 - val_accuracy: 0.9379\n",
      "Epoch 227/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0881 - accuracy: 0.9800\n",
      "Epoch 00227: val_loss improved from 0.14172 to 0.14068, saving model to ./model\\227-0.1407.hdf5\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.1068 - accuracy: 0.9709 - val_loss: 0.1407 - val_accuracy: 0.9348\n",
      "Epoch 228/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1018 - accuracy: 0.9660\n",
      "Epoch 00228: val_loss improved from 0.14068 to 0.14063, saving model to ./model\\228-0.1406.hdf5\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.1029 - accuracy: 0.9663 - val_loss: 0.1406 - val_accuracy: 0.9348\n",
      "Epoch 229/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0984 - accuracy: 0.9700\n",
      "Epoch 00229: val_loss did not improve from 0.14063\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.1023 - accuracy: 0.9678 - val_loss: 0.1409 - val_accuracy: 0.9317\n",
      "Epoch 230/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1180 - accuracy: 0.9620\n",
      "Epoch 00230: val_loss improved from 0.14063 to 0.13936, saving model to ./model\\230-0.1394.hdf5\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.1023 - accuracy: 0.9678 - val_loss: 0.1394 - val_accuracy: 0.9379\n",
      "Epoch 231/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0891 - accuracy: 0.9740\n",
      "Epoch 00231: val_loss improved from 0.13936 to 0.13816, saving model to ./model\\231-0.1382.hdf5\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.1020 - accuracy: 0.9663 - val_loss: 0.1382 - val_accuracy: 0.9379\n",
      "Epoch 232/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0975 - accuracy: 0.9680\n",
      "Epoch 00232: val_loss improved from 0.13816 to 0.13627, saving model to ./model\\232-0.1363.hdf5\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.1012 - accuracy: 0.9648 - val_loss: 0.1363 - val_accuracy: 0.9348\n",
      "Epoch 233/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1022 - accuracy: 0.9680\n",
      "Epoch 00233: val_loss did not improve from 0.13627\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.1006 - accuracy: 0.9663 - val_loss: 0.1365 - val_accuracy: 0.9379\n",
      "Epoch 234/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1038 - accuracy: 0.9660\n",
      "Epoch 00234: val_loss did not improve from 0.13627\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1006 - accuracy: 0.9694 - val_loss: 0.1370 - val_accuracy: 0.9348\n",
      "Epoch 235/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1048 - accuracy: 0.9680\n",
      "Epoch 00235: val_loss improved from 0.13627 to 0.13520, saving model to ./model\\235-0.1352.hdf5\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.1002 - accuracy: 0.9678 - val_loss: 0.1352 - val_accuracy: 0.9379\n",
      "Epoch 236/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1101 - accuracy: 0.9660\n",
      "Epoch 00236: val_loss did not improve from 0.13520\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0993 - accuracy: 0.9678 - val_loss: 0.1362 - val_accuracy: 0.9348\n",
      "Epoch 237/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1016 - accuracy: 0.9580\n",
      "Epoch 00237: val_loss did not improve from 0.13520\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1005 - accuracy: 0.9602 - val_loss: 0.1359 - val_accuracy: 0.9348\n",
      "Epoch 238/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0988 - accuracy: 0.9680\n",
      "Epoch 00238: val_loss did not improve from 0.13520\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0991 - accuracy: 0.9678 - val_loss: 0.1402 - val_accuracy: 0.9286\n",
      "Epoch 239/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1026 - accuracy: 0.9660\n",
      "Epoch 00239: val_loss did not improve from 0.13520\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1000 - accuracy: 0.9678 - val_loss: 0.1414 - val_accuracy: 0.9286\n",
      "Epoch 240/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0972 - accuracy: 0.9720\n",
      "Epoch 00240: val_loss did not improve from 0.13520\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.1006 - accuracy: 0.9694 - val_loss: 0.1382 - val_accuracy: 0.9348\n",
      "Epoch 241/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0891 - accuracy: 0.9700\n",
      "Epoch 00241: val_loss did not improve from 0.13520\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0984 - accuracy: 0.9663 - val_loss: 0.1387 - val_accuracy: 0.9379\n",
      "Epoch 242/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1009 - accuracy: 0.9680\n",
      "Epoch 00242: val_loss did not improve from 0.13520\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0979 - accuracy: 0.9678 - val_loss: 0.1403 - val_accuracy: 0.9379\n",
      "Epoch 243/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0949 - accuracy: 0.9720\n",
      "Epoch 00243: val_loss did not improve from 0.13520\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0978 - accuracy: 0.9678 - val_loss: 0.1410 - val_accuracy: 0.9348\n",
      "Epoch 244/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0988 - accuracy: 0.9680\n",
      "Epoch 00244: val_loss did not improve from 0.13520\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0976 - accuracy: 0.9678 - val_loss: 0.1421 - val_accuracy: 0.9317\n",
      "Epoch 245/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1036 - accuracy: 0.9640\n",
      "Epoch 00245: val_loss did not improve from 0.13520\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0969 - accuracy: 0.9663 - val_loss: 0.1435 - val_accuracy: 0.9348\n",
      "Epoch 246/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1026 - accuracy: 0.9600\n",
      "Epoch 00246: val_loss did not improve from 0.13520\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0987 - accuracy: 0.9617 - val_loss: 0.1386 - val_accuracy: 0.9317\n",
      "Epoch 247/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0989 - accuracy: 0.9660\n",
      "Epoch 00247: val_loss did not improve from 0.13520\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0966 - accuracy: 0.9648 - val_loss: 0.1364 - val_accuracy: 0.9286\n",
      "Epoch 248/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1104 - accuracy: 0.9660\n",
      "Epoch 00248: val_loss did not improve from 0.13520\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0978 - accuracy: 0.9694 - val_loss: 0.1377 - val_accuracy: 0.9348\n",
      "Epoch 249/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1084 - accuracy: 0.9640\n",
      "Epoch 00249: val_loss improved from 0.13520 to 0.13053, saving model to ./model\\249-0.1305.hdf5\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0992 - accuracy: 0.9678 - val_loss: 0.1305 - val_accuracy: 0.9348\n",
      "Epoch 250/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0952 - accuracy: 0.9680\n",
      "Epoch 00250: val_loss did not improve from 0.13053\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0972 - accuracy: 0.9617 - val_loss: 0.1321 - val_accuracy: 0.9379\n",
      "Epoch 251/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1015 - accuracy: 0.9620\n",
      "Epoch 00251: val_loss improved from 0.13053 to 0.12873, saving model to ./model\\251-0.1287.hdf5\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.0975 - accuracy: 0.9663 - val_loss: 0.1287 - val_accuracy: 0.9348\n",
      "Epoch 252/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0972 - accuracy: 0.9660\n",
      "Epoch 00252: val_loss did not improve from 0.12873\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0956 - accuracy: 0.9694 - val_loss: 0.1299 - val_accuracy: 0.9317\n",
      "Epoch 253/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1039 - accuracy: 0.9660\n",
      "Epoch 00253: val_loss improved from 0.12873 to 0.12699, saving model to ./model\\253-0.1270.hdf5\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.0963 - accuracy: 0.9694 - val_loss: 0.1270 - val_accuracy: 0.9379\n",
      "Epoch 254/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0935 - accuracy: 0.9680\n",
      "Epoch 00254: val_loss improved from 0.12699 to 0.12666, saving model to ./model\\254-0.1267.hdf5\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.0949 - accuracy: 0.9678 - val_loss: 0.1267 - val_accuracy: 0.9379\n",
      "Epoch 255/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1002 - accuracy: 0.9660\n",
      "Epoch 00255: val_loss did not improve from 0.12666\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0946 - accuracy: 0.9678 - val_loss: 0.1283 - val_accuracy: 0.9379\n",
      "Epoch 256/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0974 - accuracy: 0.9720\n",
      "Epoch 00256: val_loss did not improve from 0.12666\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0965 - accuracy: 0.9678 - val_loss: 0.1291 - val_accuracy: 0.9317\n",
      "Epoch 257/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0952 - accuracy: 0.9640\n",
      "Epoch 00257: val_loss did not improve from 0.12666\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0948 - accuracy: 0.9678 - val_loss: 0.1278 - val_accuracy: 0.9379\n",
      "Epoch 258/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0918 - accuracy: 0.9640\n",
      "Epoch 00258: val_loss did not improve from 0.12666\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0980 - accuracy: 0.9617 - val_loss: 0.1313 - val_accuracy: 0.9379\n",
      "Epoch 259/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0949 - accuracy: 0.9580\n",
      "Epoch 00259: val_loss improved from 0.12666 to 0.12425, saving model to ./model\\259-0.1243.hdf5\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.0984 - accuracy: 0.9556 - val_loss: 0.1243 - val_accuracy: 0.9379\n",
      "Epoch 260/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0961 - accuracy: 0.9660\n",
      "Epoch 00260: val_loss did not improve from 0.12425\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0966 - accuracy: 0.9694 - val_loss: 0.1299 - val_accuracy: 0.9410\n",
      "Epoch 261/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0949 - accuracy: 0.9700\n",
      "Epoch 00261: val_loss did not improve from 0.12425\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0954 - accuracy: 0.9678 - val_loss: 0.1253 - val_accuracy: 0.9410\n",
      "Epoch 262/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0957 - accuracy: 0.9580\n",
      "Epoch 00262: val_loss did not improve from 0.12425\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0964 - accuracy: 0.9587 - val_loss: 0.1338 - val_accuracy: 0.9410\n",
      "Epoch 263/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1072 - accuracy: 0.9540\n",
      "Epoch 00263: val_loss improved from 0.12425 to 0.12209, saving model to ./model\\263-0.1221.hdf5\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.1007 - accuracy: 0.9571 - val_loss: 0.1221 - val_accuracy: 0.9472\n",
      "Epoch 264/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1010 - accuracy: 0.9620\n",
      "Epoch 00264: val_loss did not improve from 0.12209\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0926 - accuracy: 0.9663 - val_loss: 0.1313 - val_accuracy: 0.9410\n",
      "Epoch 265/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1014 - accuracy: 0.9680\n",
      "Epoch 00265: val_loss did not improve from 0.12209\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0972 - accuracy: 0.9663 - val_loss: 0.1230 - val_accuracy: 0.9379\n",
      "Epoch 266/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0971 - accuracy: 0.9640\n",
      "Epoch 00266: val_loss did not improve from 0.12209\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0929 - accuracy: 0.9663 - val_loss: 0.1227 - val_accuracy: 0.9472\n",
      "Epoch 267/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0886 - accuracy: 0.9640\n",
      "Epoch 00267: val_loss did not improve from 0.12209\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0910 - accuracy: 0.9663 - val_loss: 0.1246 - val_accuracy: 0.9410\n",
      "Epoch 268/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0900 - accuracy: 0.9700\n",
      "Epoch 00268: val_loss did not improve from 0.12209\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0923 - accuracy: 0.9663 - val_loss: 0.1236 - val_accuracy: 0.9410\n",
      "Epoch 269/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0935 - accuracy: 0.9660\n",
      "Epoch 00269: val_loss did not improve from 0.12209\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0913 - accuracy: 0.9648 - val_loss: 0.1240 - val_accuracy: 0.9379\n",
      "Epoch 270/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0995 - accuracy: 0.9640\n",
      "Epoch 00270: val_loss did not improve from 0.12209\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0932 - accuracy: 0.9648 - val_loss: 0.1247 - val_accuracy: 0.9379\n",
      "Epoch 271/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1011 - accuracy: 0.9600\n",
      "Epoch 00271: val_loss did not improve from 0.12209\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0924 - accuracy: 0.9632 - val_loss: 0.1256 - val_accuracy: 0.9379\n",
      "Epoch 272/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0889 - accuracy: 0.9660\n",
      "Epoch 00272: val_loss improved from 0.12209 to 0.12122, saving model to ./model\\272-0.1212.hdf5\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.0928 - accuracy: 0.9632 - val_loss: 0.1212 - val_accuracy: 0.9472\n",
      "Epoch 273/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0895 - accuracy: 0.9720\n",
      "Epoch 00273: val_loss did not improve from 0.12122\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0909 - accuracy: 0.9663 - val_loss: 0.1218 - val_accuracy: 0.9410\n",
      "Epoch 274/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0877 - accuracy: 0.9680\n",
      "Epoch 00274: val_loss improved from 0.12122 to 0.12073, saving model to ./model\\274-0.1207.hdf5\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0895 - accuracy: 0.9648 - val_loss: 0.1207 - val_accuracy: 0.9472\n",
      "Epoch 275/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0963 - accuracy: 0.9640\n",
      "Epoch 00275: val_loss improved from 0.12073 to 0.12005, saving model to ./model\\275-0.1200.hdf5\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0901 - accuracy: 0.9663 - val_loss: 0.1200 - val_accuracy: 0.9472\n",
      "Epoch 276/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1005 - accuracy: 0.9620\n",
      "Epoch 00276: val_loss improved from 0.12005 to 0.11990, saving model to ./model\\276-0.1199.hdf5\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0897 - accuracy: 0.9648 - val_loss: 0.1199 - val_accuracy: 0.9441\n",
      "Epoch 277/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0901 - accuracy: 0.9680\n",
      "Epoch 00277: val_loss did not improve from 0.11990\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0891 - accuracy: 0.9663 - val_loss: 0.1202 - val_accuracy: 0.9410\n",
      "Epoch 278/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0914 - accuracy: 0.9600\n",
      "Epoch 00278: val_loss improved from 0.11990 to 0.11815, saving model to ./model\\278-0.1182.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 38ms/step - loss: 0.0893 - accuracy: 0.9663 - val_loss: 0.1182 - val_accuracy: 0.9503\n",
      "Epoch 279/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1003 - accuracy: 0.9620\n",
      "Epoch 00279: val_loss did not improve from 0.11815\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0881 - accuracy: 0.9678 - val_loss: 0.1190 - val_accuracy: 0.9472\n",
      "Epoch 280/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0930 - accuracy: 0.9680\n",
      "Epoch 00280: val_loss did not improve from 0.11815\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0888 - accuracy: 0.9678 - val_loss: 0.1195 - val_accuracy: 0.9441\n",
      "Epoch 281/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0820 - accuracy: 0.9700\n",
      "Epoch 00281: val_loss did not improve from 0.11815\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0881 - accuracy: 0.9678 - val_loss: 0.1204 - val_accuracy: 0.9472\n",
      "Epoch 282/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0853 - accuracy: 0.9700\n",
      "Epoch 00282: val_loss did not improve from 0.11815\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0876 - accuracy: 0.9663 - val_loss: 0.1221 - val_accuracy: 0.9472\n",
      "Epoch 283/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0784 - accuracy: 0.9680\n",
      "Epoch 00283: val_loss did not improve from 0.11815\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0878 - accuracy: 0.9663 - val_loss: 0.1225 - val_accuracy: 0.9503\n",
      "Epoch 284/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0805 - accuracy: 0.9660\n",
      "Epoch 00284: val_loss did not improve from 0.11815\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0869 - accuracy: 0.9663 - val_loss: 0.1231 - val_accuracy: 0.9472\n",
      "Epoch 285/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0761 - accuracy: 0.9760\n",
      "Epoch 00285: val_loss did not improve from 0.11815\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0876 - accuracy: 0.9663 - val_loss: 0.1235 - val_accuracy: 0.9503\n",
      "Epoch 286/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0824 - accuracy: 0.9680\n",
      "Epoch 00286: val_loss did not improve from 0.11815\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0866 - accuracy: 0.9678 - val_loss: 0.1277 - val_accuracy: 0.9410\n",
      "Epoch 287/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1012 - accuracy: 0.9680\n",
      "Epoch 00287: val_loss improved from 0.11815 to 0.11781, saving model to ./model\\287-0.1178.hdf5\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.0901 - accuracy: 0.9724 - val_loss: 0.1178 - val_accuracy: 0.9503\n",
      "Epoch 288/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0935 - accuracy: 0.9660\n",
      "Epoch 00288: val_loss did not improve from 0.11781\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0858 - accuracy: 0.9678 - val_loss: 0.1240 - val_accuracy: 0.9410\n",
      "Epoch 289/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0847 - accuracy: 0.9620\n",
      "Epoch 00289: val_loss improved from 0.11781 to 0.11662, saving model to ./model\\289-0.1166.hdf5\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.0921 - accuracy: 0.9632 - val_loss: 0.1166 - val_accuracy: 0.9472\n",
      "Epoch 290/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0811 - accuracy: 0.9700\n",
      "Epoch 00290: val_loss did not improve from 0.11662\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0878 - accuracy: 0.9663 - val_loss: 0.1182 - val_accuracy: 0.9503\n",
      "Epoch 291/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0731 - accuracy: 0.9720\n",
      "Epoch 00291: val_loss improved from 0.11662 to 0.11621, saving model to ./model\\291-0.1162.hdf5\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.0864 - accuracy: 0.9678 - val_loss: 0.1162 - val_accuracy: 0.9503\n",
      "Epoch 292/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0814 - accuracy: 0.9740\n",
      "Epoch 00292: val_loss did not improve from 0.11621\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0858 - accuracy: 0.9678 - val_loss: 0.1164 - val_accuracy: 0.9534\n",
      "Epoch 293/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0802 - accuracy: 0.9680\n",
      "Epoch 00293: val_loss improved from 0.11621 to 0.11505, saving model to ./model\\293-0.1151.hdf5\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0865 - accuracy: 0.9663 - val_loss: 0.1151 - val_accuracy: 0.9534\n",
      "Epoch 294/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0805 - accuracy: 0.9700\n",
      "Epoch 00294: val_loss improved from 0.11505 to 0.11443, saving model to ./model\\294-0.1144.hdf5\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0852 - accuracy: 0.9678 - val_loss: 0.1144 - val_accuracy: 0.9534\n",
      "Epoch 295/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0974 - accuracy: 0.9600\n",
      "Epoch 00295: val_loss did not improve from 0.11443\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0848 - accuracy: 0.9663 - val_loss: 0.1156 - val_accuracy: 0.9472\n",
      "Epoch 296/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0777 - accuracy: 0.9700\n",
      "Epoch 00296: val_loss did not improve from 0.11443\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0845 - accuracy: 0.9648 - val_loss: 0.1149 - val_accuracy: 0.9534\n",
      "Epoch 297/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0729 - accuracy: 0.9740\n",
      "Epoch 00297: val_loss improved from 0.11443 to 0.11393, saving model to ./model\\297-0.1139.hdf5\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.0839 - accuracy: 0.9663 - val_loss: 0.1139 - val_accuracy: 0.9565\n",
      "Epoch 298/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0916 - accuracy: 0.9620\n",
      "Epoch 00298: val_loss improved from 0.11393 to 0.11264, saving model to ./model\\298-0.1126.hdf5\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.0840 - accuracy: 0.9678 - val_loss: 0.1126 - val_accuracy: 0.9472\n",
      "Epoch 299/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0877 - accuracy: 0.9680\n",
      "Epoch 00299: val_loss improved from 0.11264 to 0.11185, saving model to ./model\\299-0.1119.hdf5\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.0842 - accuracy: 0.9678 - val_loss: 0.1119 - val_accuracy: 0.9472\n",
      "Epoch 300/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0775 - accuracy: 0.9720\n",
      "Epoch 00300: val_loss improved from 0.11185 to 0.11141, saving model to ./model\\300-0.1114.hdf5\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0835 - accuracy: 0.9678 - val_loss: 0.1114 - val_accuracy: 0.9565\n",
      "Epoch 301/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0860 - accuracy: 0.9700\n",
      "Epoch 00301: val_loss did not improve from 0.11141\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0843 - accuracy: 0.9694 - val_loss: 0.1124 - val_accuracy: 0.9534\n",
      "Epoch 302/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0915 - accuracy: 0.9660\n",
      "Epoch 00302: val_loss did not improve from 0.11141\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0846 - accuracy: 0.9694 - val_loss: 0.1115 - val_accuracy: 0.9534\n",
      "Epoch 303/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0600 - accuracy: 0.9800\n",
      "Epoch 00303: val_loss did not improve from 0.11141\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0832 - accuracy: 0.9678 - val_loss: 0.1230 - val_accuracy: 0.9472\n",
      "Epoch 304/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0887 - accuracy: 0.9720\n",
      "Epoch 00304: val_loss did not improve from 0.11141\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0905 - accuracy: 0.9709 - val_loss: 0.1195 - val_accuracy: 0.9410\n",
      "Epoch 305/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0890 - accuracy: 0.9740\n",
      "Epoch 00305: val_loss did not improve from 0.11141\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0835 - accuracy: 0.9709 - val_loss: 0.1265 - val_accuracy: 0.9441\n",
      "Epoch 306/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0731 - accuracy: 0.9660\n",
      "Epoch 00306: val_loss did not improve from 0.11141\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0919 - accuracy: 0.9617 - val_loss: 0.1229 - val_accuracy: 0.9472\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 307/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0806 - accuracy: 0.9660\n",
      "Epoch 00307: val_loss did not improve from 0.11141\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0866 - accuracy: 0.9663 - val_loss: 0.1266 - val_accuracy: 0.9441\n",
      "Epoch 308/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0883 - accuracy: 0.9760\n",
      "Epoch 00308: val_loss did not improve from 0.11141\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0893 - accuracy: 0.9709 - val_loss: 0.1173 - val_accuracy: 0.9503\n",
      "Epoch 309/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0909 - accuracy: 0.9700\n",
      "Epoch 00309: val_loss did not improve from 0.11141\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0832 - accuracy: 0.9678 - val_loss: 0.1310 - val_accuracy: 0.9472\n",
      "Epoch 310/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0690 - accuracy: 0.9700\n",
      "Epoch 00310: val_loss did not improve from 0.11141\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0916 - accuracy: 0.9632 - val_loss: 0.1130 - val_accuracy: 0.9565\n",
      "Epoch 311/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0848 - accuracy: 0.9700\n",
      "Epoch 00311: val_loss did not improve from 0.11141\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0847 - accuracy: 0.9709 - val_loss: 0.1262 - val_accuracy: 0.9503\n",
      "Epoch 312/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0995 - accuracy: 0.9700\n",
      "Epoch 00312: val_loss did not improve from 0.11141\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0891 - accuracy: 0.9724 - val_loss: 0.1117 - val_accuracy: 0.9503\n",
      "Epoch 313/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0801 - accuracy: 0.9680\n",
      "Epoch 00313: val_loss did not improve from 0.11141\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0834 - accuracy: 0.9709 - val_loss: 0.1310 - val_accuracy: 0.9441\n",
      "Epoch 314/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0995 - accuracy: 0.9580\n",
      "Epoch 00314: val_loss did not improve from 0.11141\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0968 - accuracy: 0.9617 - val_loss: 0.1124 - val_accuracy: 0.9534\n",
      "Epoch 315/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0924 - accuracy: 0.9640\n",
      "Epoch 00315: val_loss did not improve from 0.11141\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0834 - accuracy: 0.9678 - val_loss: 0.1204 - val_accuracy: 0.9472\n",
      "Epoch 316/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0881 - accuracy: 0.9740\n",
      "Epoch 00316: val_loss did not improve from 0.11141\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0889 - accuracy: 0.9740 - val_loss: 0.1183 - val_accuracy: 0.9441\n",
      "Epoch 317/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0950 - accuracy: 0.9700\n",
      "Epoch 00317: val_loss did not improve from 0.11141\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0842 - accuracy: 0.9755 - val_loss: 0.1131 - val_accuracy: 0.9565\n",
      "Epoch 318/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0881 - accuracy: 0.9680\n",
      "Epoch 00318: val_loss did not improve from 0.11141\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0843 - accuracy: 0.9663 - val_loss: 0.1235 - val_accuracy: 0.9472\n",
      "Epoch 319/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0976 - accuracy: 0.9600\n",
      "Epoch 00319: val_loss did not improve from 0.11141\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0893 - accuracy: 0.9648 - val_loss: 0.1131 - val_accuracy: 0.9565\n",
      "Epoch 320/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0907 - accuracy: 0.9680\n",
      "Epoch 00320: val_loss did not improve from 0.11141\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0810 - accuracy: 0.9709 - val_loss: 0.1176 - val_accuracy: 0.9441\n",
      "Epoch 321/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0859 - accuracy: 0.9740\n",
      "Epoch 00321: val_loss did not improve from 0.11141\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0813 - accuracy: 0.9740 - val_loss: 0.1148 - val_accuracy: 0.9534\n",
      "Epoch 322/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0742 - accuracy: 0.9720\n",
      "Epoch 00322: val_loss did not improve from 0.11141\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0801 - accuracy: 0.9694 - val_loss: 0.1179 - val_accuracy: 0.9534\n",
      "Epoch 323/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0767 - accuracy: 0.9700\n",
      "Epoch 00323: val_loss did not improve from 0.11141\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0826 - accuracy: 0.9663 - val_loss: 0.1137 - val_accuracy: 0.9534\n",
      "Epoch 324/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0689 - accuracy: 0.9740\n",
      "Epoch 00324: val_loss did not improve from 0.11141\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0812 - accuracy: 0.9709 - val_loss: 0.1192 - val_accuracy: 0.9379\n",
      "Epoch 325/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0949 - accuracy: 0.9680\n",
      "Epoch 00325: val_loss did not improve from 0.11141\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.0821 - accuracy: 0.9724 - val_loss: 0.1122 - val_accuracy: 0.9565\n",
      "Epoch 326/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0690 - accuracy: 0.9680\n",
      "Epoch 00326: val_loss improved from 0.11141 to 0.11131, saving model to ./model\\326-0.1113.hdf5\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0804 - accuracy: 0.9678 - val_loss: 0.1113 - val_accuracy: 0.9565\n",
      "Epoch 327/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0809 - accuracy: 0.9700\n",
      "Epoch 00327: val_loss improved from 0.11131 to 0.11109, saving model to ./model\\327-0.1111.hdf5\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0796 - accuracy: 0.9709 - val_loss: 0.1111 - val_accuracy: 0.9534\n",
      "Epoch 328/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0809 - accuracy: 0.9700\n",
      "Epoch 00328: val_loss improved from 0.11109 to 0.11038, saving model to ./model\\328-0.1104.hdf5\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0797 - accuracy: 0.9724 - val_loss: 0.1104 - val_accuracy: 0.9534\n",
      "Epoch 329/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0819 - accuracy: 0.9760\n",
      "Epoch 00329: val_loss improved from 0.11038 to 0.10994, saving model to ./model\\329-0.1099.hdf5\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0789 - accuracy: 0.9740 - val_loss: 0.1099 - val_accuracy: 0.9565\n",
      "Epoch 330/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0712 - accuracy: 0.9700\n",
      "Epoch 00330: val_loss did not improve from 0.10994\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0798 - accuracy: 0.9694 - val_loss: 0.1107 - val_accuracy: 0.9565\n",
      "Epoch 331/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0723 - accuracy: 0.9760\n",
      "Epoch 00331: val_loss did not improve from 0.10994\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0787 - accuracy: 0.9694 - val_loss: 0.1144 - val_accuracy: 0.9503\n",
      "Epoch 332/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0788 - accuracy: 0.9700\n",
      "Epoch 00332: val_loss did not improve from 0.10994\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0791 - accuracy: 0.9724 - val_loss: 0.1119 - val_accuracy: 0.9565\n",
      "Epoch 333/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0834 - accuracy: 0.9680\n",
      "Epoch 00333: val_loss did not improve from 0.10994\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0780 - accuracy: 0.9709 - val_loss: 0.1177 - val_accuracy: 0.9565\n",
      "Epoch 334/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0903 - accuracy: 0.9640\n",
      "Epoch 00334: val_loss did not improve from 0.10994\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0820 - accuracy: 0.9678 - val_loss: 0.1124 - val_accuracy: 0.9565\n",
      "Epoch 335/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0876 - accuracy: 0.9660\n",
      "Epoch 00335: val_loss did not improve from 0.10994\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0783 - accuracy: 0.9694 - val_loss: 0.1134 - val_accuracy: 0.9565\n",
      "Epoch 336/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0766 - accuracy: 0.9700\n",
      "Epoch 00336: val_loss did not improve from 0.10994\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0786 - accuracy: 0.9709 - val_loss: 0.1108 - val_accuracy: 0.9534\n",
      "Epoch 337/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0676 - accuracy: 0.9800\n",
      "Epoch 00337: val_loss improved from 0.10994 to 0.10908, saving model to ./model\\337-0.1091.hdf5\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.0784 - accuracy: 0.9724 - val_loss: 0.1091 - val_accuracy: 0.9565\n",
      "Epoch 338/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0736 - accuracy: 0.9740\n",
      "Epoch 00338: val_loss improved from 0.10908 to 0.10839, saving model to ./model\\338-0.1084.hdf5\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.0768 - accuracy: 0.9724 - val_loss: 0.1084 - val_accuracy: 0.9534\n",
      "Epoch 339/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0770 - accuracy: 0.9740\n",
      "Epoch 00339: val_loss improved from 0.10839 to 0.10770, saving model to ./model\\339-0.1077.hdf5\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.0772 - accuracy: 0.9740 - val_loss: 0.1077 - val_accuracy: 0.9534\n",
      "Epoch 340/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0863 - accuracy: 0.9680\n",
      "Epoch 00340: val_loss improved from 0.10770 to 0.10690, saving model to ./model\\340-0.1069.hdf5\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.0768 - accuracy: 0.9740 - val_loss: 0.1069 - val_accuracy: 0.9596\n",
      "Epoch 341/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0792 - accuracy: 0.9680\n",
      "Epoch 00341: val_loss did not improve from 0.10690\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0772 - accuracy: 0.9709 - val_loss: 0.1080 - val_accuracy: 0.9596\n",
      "Epoch 342/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0691 - accuracy: 0.9700\n",
      "Epoch 00342: val_loss did not improve from 0.10690\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0764 - accuracy: 0.9709 - val_loss: 0.1107 - val_accuracy: 0.9503\n",
      "Epoch 343/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0806 - accuracy: 0.9740\n",
      "Epoch 00343: val_loss did not improve from 0.10690\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0776 - accuracy: 0.9740 - val_loss: 0.1132 - val_accuracy: 0.9472\n",
      "Epoch 344/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0902 - accuracy: 0.9660\n",
      "Epoch 00344: val_loss did not improve from 0.10690\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0773 - accuracy: 0.9740 - val_loss: 0.1107 - val_accuracy: 0.9596\n",
      "Epoch 345/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0811 - accuracy: 0.9700\n",
      "Epoch 00345: val_loss did not improve from 0.10690\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0773 - accuracy: 0.9694 - val_loss: 0.1168 - val_accuracy: 0.9565\n",
      "Epoch 346/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0867 - accuracy: 0.9680\n",
      "Epoch 00346: val_loss did not improve from 0.10690\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0804 - accuracy: 0.9694 - val_loss: 0.1099 - val_accuracy: 0.9565\n",
      "Epoch 347/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0791 - accuracy: 0.9740\n",
      "Epoch 00347: val_loss did not improve from 0.10690\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0756 - accuracy: 0.9740 - val_loss: 0.1169 - val_accuracy: 0.9410\n",
      "Epoch 348/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0907 - accuracy: 0.9700\n",
      "Epoch 00348: val_loss improved from 0.10690 to 0.10664, saving model to ./model\\348-0.1066.hdf5\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.0797 - accuracy: 0.9755 - val_loss: 0.1066 - val_accuracy: 0.9565\n",
      "Epoch 349/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0743 - accuracy: 0.9700\n",
      "Epoch 00349: val_loss did not improve from 0.10664\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0757 - accuracy: 0.9724 - val_loss: 0.1129 - val_accuracy: 0.9596\n",
      "Epoch 350/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0890 - accuracy: 0.9680\n",
      "Epoch 00350: val_loss did not improve from 0.10664\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0805 - accuracy: 0.9694 - val_loss: 0.1067 - val_accuracy: 0.9565\n",
      "Epoch 351/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0753 - accuracy: 0.9740\n",
      "Epoch 00351: val_loss did not improve from 0.10664\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0761 - accuracy: 0.9724 - val_loss: 0.1132 - val_accuracy: 0.9441\n",
      "Epoch 352/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0858 - accuracy: 0.9720\n",
      "Epoch 00352: val_loss did not improve from 0.10664\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0778 - accuracy: 0.9740 - val_loss: 0.1086 - val_accuracy: 0.9565\n",
      "Epoch 353/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0778 - accuracy: 0.9720\n",
      "Epoch 00353: val_loss did not improve from 0.10664\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0749 - accuracy: 0.9724 - val_loss: 0.1101 - val_accuracy: 0.9596\n",
      "Epoch 354/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0659 - accuracy: 0.9780\n",
      "Epoch 00354: val_loss did not improve from 0.10664\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0754 - accuracy: 0.9724 - val_loss: 0.1084 - val_accuracy: 0.9534\n",
      "Epoch 355/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0745 - accuracy: 0.9760\n",
      "Epoch 00355: val_loss did not improve from 0.10664\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0743 - accuracy: 0.9740 - val_loss: 0.1092 - val_accuracy: 0.9565\n",
      "Epoch 356/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0768 - accuracy: 0.9740\n",
      "Epoch 00356: val_loss did not improve from 0.10664\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0747 - accuracy: 0.9755 - val_loss: 0.1068 - val_accuracy: 0.9596\n",
      "Epoch 357/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0697 - accuracy: 0.9780\n",
      "Epoch 00357: val_loss did not improve from 0.10664\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0743 - accuracy: 0.9740 - val_loss: 0.1067 - val_accuracy: 0.9596\n",
      "Epoch 358/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0743 - accuracy: 0.9740\n",
      "Epoch 00358: val_loss improved from 0.10664 to 0.10645, saving model to ./model\\358-0.1064.hdf5\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0742 - accuracy: 0.9740 - val_loss: 0.1064 - val_accuracy: 0.9627\n",
      "Epoch 359/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0818 - accuracy: 0.9740\n",
      "Epoch 00359: val_loss improved from 0.10645 to 0.10596, saving model to ./model\\359-0.1060.hdf5\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0736 - accuracy: 0.9740 - val_loss: 0.1060 - val_accuracy: 0.9596\n",
      "Epoch 360/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0829 - accuracy: 0.9700\n",
      "Epoch 00360: val_loss improved from 0.10596 to 0.10508, saving model to ./model\\360-0.1051.hdf5\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.0735 - accuracy: 0.9740 - val_loss: 0.1051 - val_accuracy: 0.9658\n",
      "Epoch 361/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0740 - accuracy: 0.9720\n",
      "Epoch 00361: val_loss improved from 0.10508 to 0.10451, saving model to ./model\\361-0.1045.hdf5\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0750 - accuracy: 0.9724 - val_loss: 0.1045 - val_accuracy: 0.9596\n",
      "Epoch 362/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0768 - accuracy: 0.9780\n",
      "Epoch 00362: val_loss did not improve from 0.10451\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0743 - accuracy: 0.9770 - val_loss: 0.1071 - val_accuracy: 0.9565\n",
      "Epoch 363/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0795 - accuracy: 0.9740\n",
      "Epoch 00363: val_loss did not improve from 0.10451\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0738 - accuracy: 0.9770 - val_loss: 0.1054 - val_accuracy: 0.9658\n",
      "Epoch 364/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0729 - accuracy: 0.9780\n",
      "Epoch 00364: val_loss did not improve from 0.10451\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0747 - accuracy: 0.9770 - val_loss: 0.1056 - val_accuracy: 0.9596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 365/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0707 - accuracy: 0.9740\n",
      "Epoch 00365: val_loss improved from 0.10451 to 0.10418, saving model to ./model\\365-0.1042.hdf5\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.0740 - accuracy: 0.9755 - val_loss: 0.1042 - val_accuracy: 0.9596\n",
      "Epoch 366/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0684 - accuracy: 0.9780\n",
      "Epoch 00366: val_loss improved from 0.10418 to 0.10341, saving model to ./model\\366-0.1034.hdf5\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0743 - accuracy: 0.9770 - val_loss: 0.1034 - val_accuracy: 0.9596\n",
      "Epoch 367/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0655 - accuracy: 0.9800\n",
      "Epoch 00367: val_loss improved from 0.10341 to 0.10000, saving model to ./model\\367-0.1000.hdf5\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0747 - accuracy: 0.9770 - val_loss: 0.1000 - val_accuracy: 0.9658\n",
      "Epoch 368/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0721 - accuracy: 0.9780\n",
      "Epoch 00368: val_loss did not improve from 0.10000\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0726 - accuracy: 0.9770 - val_loss: 0.1003 - val_accuracy: 0.9689\n",
      "Epoch 369/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0777 - accuracy: 0.9720\n",
      "Epoch 00369: val_loss did not improve from 0.10000\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0728 - accuracy: 0.9755 - val_loss: 0.1011 - val_accuracy: 0.9689\n",
      "Epoch 370/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0709 - accuracy: 0.9740\n",
      "Epoch 00370: val_loss did not improve from 0.10000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0722 - accuracy: 0.9770 - val_loss: 0.1022 - val_accuracy: 0.9689\n",
      "Epoch 371/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0742 - accuracy: 0.9760\n",
      "Epoch 00371: val_loss did not improve from 0.10000\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.0720 - accuracy: 0.9755 - val_loss: 0.1042 - val_accuracy: 0.9658\n",
      "Epoch 372/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0755 - accuracy: 0.9760\n",
      "Epoch 00372: val_loss did not improve from 0.10000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0718 - accuracy: 0.9755 - val_loss: 0.1038 - val_accuracy: 0.9689\n",
      "Epoch 373/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0727 - accuracy: 0.9760\n",
      "Epoch 00373: val_loss did not improve from 0.10000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0716 - accuracy: 0.9740 - val_loss: 0.1052 - val_accuracy: 0.9596\n",
      "Epoch 374/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0797 - accuracy: 0.9740\n",
      "Epoch 00374: val_loss did not improve from 0.10000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0735 - accuracy: 0.9755 - val_loss: 0.1064 - val_accuracy: 0.9596\n",
      "Epoch 375/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0834 - accuracy: 0.9740\n",
      "Epoch 00375: val_loss did not improve from 0.10000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0748 - accuracy: 0.9770 - val_loss: 0.1008 - val_accuracy: 0.9689\n",
      "Epoch 376/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0794 - accuracy: 0.9700\n",
      "Epoch 00376: val_loss did not improve from 0.10000\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0731 - accuracy: 0.9740 - val_loss: 0.1016 - val_accuracy: 0.9627\n",
      "Epoch 377/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0835 - accuracy: 0.9720\n",
      "Epoch 00377: val_loss did not improve from 0.10000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0713 - accuracy: 0.9770 - val_loss: 0.1008 - val_accuracy: 0.9689\n",
      "Epoch 378/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0747 - accuracy: 0.9760\n",
      "Epoch 00378: val_loss did not improve from 0.10000\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0722 - accuracy: 0.9740 - val_loss: 0.1016 - val_accuracy: 0.9658\n",
      "Epoch 379/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0648 - accuracy: 0.9760\n",
      "Epoch 00379: val_loss did not improve from 0.10000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0707 - accuracy: 0.9755 - val_loss: 0.1070 - val_accuracy: 0.9534\n",
      "Epoch 380/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0754 - accuracy: 0.9800\n",
      "Epoch 00380: val_loss did not improve from 0.10000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0731 - accuracy: 0.9786 - val_loss: 0.1057 - val_accuracy: 0.9596\n",
      "Epoch 381/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0670 - accuracy: 0.9760\n",
      "Epoch 00381: val_loss did not improve from 0.10000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0701 - accuracy: 0.9770 - val_loss: 0.1101 - val_accuracy: 0.9565\n",
      "Epoch 382/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0849 - accuracy: 0.9740\n",
      "Epoch 00382: val_loss did not improve from 0.10000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0745 - accuracy: 0.9786 - val_loss: 0.1090 - val_accuracy: 0.9596\n",
      "Epoch 383/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0826 - accuracy: 0.9760\n",
      "Epoch 00383: val_loss did not improve from 0.10000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0744 - accuracy: 0.9755 - val_loss: 0.1058 - val_accuracy: 0.9627\n",
      "Epoch 384/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0677 - accuracy: 0.9760\n",
      "Epoch 00384: val_loss did not improve from 0.10000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0704 - accuracy: 0.9755 - val_loss: 0.1049 - val_accuracy: 0.9627\n",
      "Epoch 385/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0710 - accuracy: 0.9760\n",
      "Epoch 00385: val_loss did not improve from 0.10000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0704 - accuracy: 0.9755 - val_loss: 0.1033 - val_accuracy: 0.9720\n",
      "Epoch 386/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0688 - accuracy: 0.9740\n",
      "Epoch 00386: val_loss did not improve from 0.10000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0696 - accuracy: 0.9755 - val_loss: 0.1022 - val_accuracy: 0.9689\n",
      "Epoch 387/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0756 - accuracy: 0.9780\n",
      "Epoch 00387: val_loss did not improve from 0.10000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0701 - accuracy: 0.9786 - val_loss: 0.1031 - val_accuracy: 0.9627\n",
      "Epoch 388/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0699 - accuracy: 0.9760\n",
      "Epoch 00388: val_loss did not improve from 0.10000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0714 - accuracy: 0.9770 - val_loss: 0.1008 - val_accuracy: 0.9658\n",
      "Epoch 389/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0773 - accuracy: 0.9760\n",
      "Epoch 00389: val_loss did not improve from 0.10000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0701 - accuracy: 0.9801 - val_loss: 0.1007 - val_accuracy: 0.9658\n",
      "Epoch 390/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0657 - accuracy: 0.9780\n",
      "Epoch 00390: val_loss did not improve from 0.10000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0696 - accuracy: 0.9770 - val_loss: 0.1013 - val_accuracy: 0.9658\n",
      "Epoch 391/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0708 - accuracy: 0.9760\n",
      "Epoch 00391: val_loss did not improve from 0.10000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0698 - accuracy: 0.9770 - val_loss: 0.1007 - val_accuracy: 0.9689\n",
      "Epoch 392/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0650 - accuracy: 0.9800\n",
      "Epoch 00392: val_loss did not improve from 0.10000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0688 - accuracy: 0.9755 - val_loss: 0.1014 - val_accuracy: 0.9658\n",
      "Epoch 393/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0652 - accuracy: 0.9780\n",
      "Epoch 00393: val_loss did not improve from 0.10000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0687 - accuracy: 0.9755 - val_loss: 0.1011 - val_accuracy: 0.9658\n",
      "Epoch 394/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0705 - accuracy: 0.9780\n",
      "Epoch 00394: val_loss did not improve from 0.10000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0686 - accuracy: 0.9770 - val_loss: 0.1001 - val_accuracy: 0.9658\n",
      "Epoch 395/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0638 - accuracy: 0.9780\n",
      "Epoch 00395: val_loss improved from 0.10000 to 0.09821, saving model to ./model\\395-0.0982.hdf5\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.0686 - accuracy: 0.9786 - val_loss: 0.0982 - val_accuracy: 0.9658\n",
      "Epoch 396/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0670 - accuracy: 0.9780\n",
      "Epoch 00396: val_loss improved from 0.09821 to 0.09751, saving model to ./model\\396-0.0975.hdf5\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.0685 - accuracy: 0.9801 - val_loss: 0.0975 - val_accuracy: 0.9720\n",
      "Epoch 397/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0761 - accuracy: 0.9740\n",
      "Epoch 00397: val_loss improved from 0.09751 to 0.09698, saving model to ./model\\397-0.0970.hdf5\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 0.0689 - accuracy: 0.9770 - val_loss: 0.0970 - val_accuracy: 0.9720\n",
      "Epoch 398/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0688 - accuracy: 0.9780\n",
      "Epoch 00398: val_loss did not improve from 0.09698\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0676 - accuracy: 0.9786 - val_loss: 0.1010 - val_accuracy: 0.9596\n",
      "Epoch 399/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0760 - accuracy: 0.9780\n",
      "Epoch 00399: val_loss did not improve from 0.09698\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0713 - accuracy: 0.9801 - val_loss: 0.1007 - val_accuracy: 0.9627\n",
      "Epoch 400/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0767 - accuracy: 0.9780\n",
      "Epoch 00400: val_loss did not improve from 0.09698\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0710 - accuracy: 0.9786 - val_loss: 0.0991 - val_accuracy: 0.9689\n",
      "Epoch 401/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0645 - accuracy: 0.9800\n",
      "Epoch 00401: val_loss did not improve from 0.09698\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0683 - accuracy: 0.9770 - val_loss: 0.0992 - val_accuracy: 0.9689\n",
      "Epoch 402/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0730 - accuracy: 0.9780\n",
      "Epoch 00402: val_loss did not improve from 0.09698\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0681 - accuracy: 0.9786 - val_loss: 0.1021 - val_accuracy: 0.9627\n",
      "Epoch 403/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0757 - accuracy: 0.9800\n",
      "Epoch 00403: val_loss did not improve from 0.09698\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0691 - accuracy: 0.9801 - val_loss: 0.0999 - val_accuracy: 0.9689\n",
      "Epoch 404/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0647 - accuracy: 0.9760\n",
      "Epoch 00404: val_loss did not improve from 0.09698\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0671 - accuracy: 0.9770 - val_loss: 0.1060 - val_accuracy: 0.9596\n",
      "Epoch 405/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0656 - accuracy: 0.9800\n",
      "Epoch 00405: val_loss did not improve from 0.09698\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0722 - accuracy: 0.9770 - val_loss: 0.0977 - val_accuracy: 0.9720\n",
      "Epoch 406/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0779 - accuracy: 0.9740\n",
      "Epoch 00406: val_loss did not improve from 0.09698\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0669 - accuracy: 0.9786 - val_loss: 0.1044 - val_accuracy: 0.9596\n",
      "Epoch 407/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0805 - accuracy: 0.9780\n",
      "Epoch 00407: val_loss did not improve from 0.09698\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0719 - accuracy: 0.9801 - val_loss: 0.0989 - val_accuracy: 0.9627\n",
      "Epoch 408/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0505 - accuracy: 0.9840\n",
      "Epoch 00408: val_loss did not improve from 0.09698\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0665 - accuracy: 0.9801 - val_loss: 0.1044 - val_accuracy: 0.9596\n",
      "Epoch 409/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0607 - accuracy: 0.9800\n",
      "Epoch 00409: val_loss did not improve from 0.09698\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0717 - accuracy: 0.9755 - val_loss: 0.1004 - val_accuracy: 0.9658\n",
      "Epoch 410/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0768 - accuracy: 0.9740\n",
      "Epoch 00410: val_loss did not improve from 0.09698\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0680 - accuracy: 0.9770 - val_loss: 0.0998 - val_accuracy: 0.9627\n",
      "Epoch 411/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0810 - accuracy: 0.9760\n",
      "Epoch 00411: val_loss did not improve from 0.09698\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0692 - accuracy: 0.9801 - val_loss: 0.1007 - val_accuracy: 0.9627\n",
      "Epoch 412/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0659 - accuracy: 0.9800\n",
      "Epoch 00412: val_loss did not improve from 0.09698\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0680 - accuracy: 0.9786 - val_loss: 0.1014 - val_accuracy: 0.9627\n",
      "Epoch 413/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0702 - accuracy: 0.9780\n",
      "Epoch 00413: val_loss did not improve from 0.09698\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0700 - accuracy: 0.9770 - val_loss: 0.1013 - val_accuracy: 0.9658\n",
      "Epoch 414/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0577 - accuracy: 0.9780\n",
      "Epoch 00414: val_loss did not improve from 0.09698\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0668 - accuracy: 0.9786 - val_loss: 0.1077 - val_accuracy: 0.9565\n",
      "Epoch 415/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0729 - accuracy: 0.9760\n",
      "Epoch 00415: val_loss did not improve from 0.09698\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0707 - accuracy: 0.9801 - val_loss: 0.1044 - val_accuracy: 0.9596\n",
      "Epoch 416/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0699 - accuracy: 0.9800\n",
      "Epoch 00416: val_loss did not improve from 0.09698\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0667 - accuracy: 0.9816 - val_loss: 0.1058 - val_accuracy: 0.9627\n",
      "Epoch 417/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0639 - accuracy: 0.9760\n",
      "Epoch 00417: val_loss did not improve from 0.09698\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0676 - accuracy: 0.9786 - val_loss: 0.1060 - val_accuracy: 0.9658\n",
      "Epoch 418/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0705 - accuracy: 0.9760\n",
      "Epoch 00418: val_loss did not improve from 0.09698\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0661 - accuracy: 0.9755 - val_loss: 0.1081 - val_accuracy: 0.9565\n",
      "Epoch 419/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0663 - accuracy: 0.9840\n",
      "Epoch 00419: val_loss did not improve from 0.09698\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0687 - accuracy: 0.9801 - val_loss: 0.1064 - val_accuracy: 0.9565\n",
      "Epoch 420/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0660 - accuracy: 0.9840\n",
      "Epoch 00420: val_loss did not improve from 0.09698\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0673 - accuracy: 0.9816 - val_loss: 0.1046 - val_accuracy: 0.9627\n",
      "Epoch 421/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0808 - accuracy: 0.9720\n",
      "Epoch 00421: val_loss did not improve from 0.09698\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0678 - accuracy: 0.9786 - val_loss: 0.1020 - val_accuracy: 0.9627\n",
      "Epoch 422/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0559 - accuracy: 0.9760\n",
      "Epoch 00422: val_loss did not improve from 0.09698\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0684 - accuracy: 0.9755 - val_loss: 0.1021 - val_accuracy: 0.9658\n",
      "Epoch 423/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0631 - accuracy: 0.9780\n",
      "Epoch 00423: val_loss did not improve from 0.09698\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0662 - accuracy: 0.9786 - val_loss: 0.1016 - val_accuracy: 0.9627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 424/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0741 - accuracy: 0.9720\n",
      "Epoch 00424: val_loss did not improve from 0.09698\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0658 - accuracy: 0.9770 - val_loss: 0.1008 - val_accuracy: 0.9627\n",
      "Epoch 425/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0716 - accuracy: 0.9740\n",
      "Epoch 00425: val_loss did not improve from 0.09698\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0654 - accuracy: 0.9755 - val_loss: 0.0996 - val_accuracy: 0.9689\n",
      "Epoch 426/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0601 - accuracy: 0.9800\n",
      "Epoch 00426: val_loss did not improve from 0.09698\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0660 - accuracy: 0.9801 - val_loss: 0.0995 - val_accuracy: 0.9689\n",
      "Epoch 427/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0651 - accuracy: 0.9840\n",
      "Epoch 00427: val_loss did not improve from 0.09698\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0652 - accuracy: 0.9801 - val_loss: 0.1012 - val_accuracy: 0.9627\n",
      "Epoch 428/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0630 - accuracy: 0.9820\n",
      "Epoch 00428: val_loss did not improve from 0.09698\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0660 - accuracy: 0.9786 - val_loss: 0.1003 - val_accuracy: 0.9627\n",
      "Epoch 429/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0743 - accuracy: 0.9740\n",
      "Epoch 00429: val_loss did not improve from 0.09698\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0652 - accuracy: 0.9786 - val_loss: 0.0983 - val_accuracy: 0.9658\n",
      "Epoch 430/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0476 - accuracy: 0.9880\n",
      "Epoch 00430: val_loss did not improve from 0.09698\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0670 - accuracy: 0.9816 - val_loss: 0.0974 - val_accuracy: 0.9689\n",
      "Epoch 431/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0485 - accuracy: 0.9840\n",
      "Epoch 00431: val_loss did not improve from 0.09698\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0632 - accuracy: 0.9816 - val_loss: 0.1084 - val_accuracy: 0.9627\n",
      "Epoch 432/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0780 - accuracy: 0.9720\n",
      "Epoch 00432: val_loss did not improve from 0.09698\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0729 - accuracy: 0.9755 - val_loss: 0.0974 - val_accuracy: 0.9658\n",
      "Epoch 433/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0673 - accuracy: 0.9840\n",
      "Epoch 00433: val_loss did not improve from 0.09698\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0654 - accuracy: 0.9816 - val_loss: 0.1085 - val_accuracy: 0.9596\n",
      "Epoch 434/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0712 - accuracy: 0.9820\n",
      "Epoch 00434: val_loss did not improve from 0.09698\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0716 - accuracy: 0.9801 - val_loss: 0.0994 - val_accuracy: 0.9689\n",
      "Epoch 435/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0672 - accuracy: 0.9800\n",
      "Epoch 00435: val_loss did not improve from 0.09698\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0653 - accuracy: 0.9801 - val_loss: 0.1129 - val_accuracy: 0.9596\n",
      "Epoch 436/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0713 - accuracy: 0.9780\n",
      "Epoch 00436: val_loss did not improve from 0.09698\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0702 - accuracy: 0.9786 - val_loss: 0.1096 - val_accuracy: 0.9565\n",
      "Epoch 437/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0691 - accuracy: 0.9780\n",
      "Epoch 00437: val_loss did not improve from 0.09698\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0709 - accuracy: 0.9770 - val_loss: 0.1338 - val_accuracy: 0.9596\n",
      "Epoch 438/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0677 - accuracy: 0.9700\n",
      "Epoch 00438: val_loss did not improve from 0.09698\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0800 - accuracy: 0.9709 - val_loss: 0.1055 - val_accuracy: 0.9689\n",
      "Epoch 439/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0693 - accuracy: 0.9760\n",
      "Epoch 00439: val_loss did not improve from 0.09698\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0669 - accuracy: 0.9770 - val_loss: 0.1302 - val_accuracy: 0.9503\n",
      "Epoch 440/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0874 - accuracy: 0.9680\n",
      "Epoch 00440: val_loss did not improve from 0.09698\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0827 - accuracy: 0.9709 - val_loss: 0.1111 - val_accuracy: 0.9565\n",
      "Epoch 441/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0752 - accuracy: 0.9760\n",
      "Epoch 00441: val_loss did not improve from 0.09698\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0660 - accuracy: 0.9801 - val_loss: 0.1262 - val_accuracy: 0.9596\n",
      "Epoch 442/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0699 - accuracy: 0.9740\n",
      "Epoch 00442: val_loss did not improve from 0.09698\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0756 - accuracy: 0.9740 - val_loss: 0.1088 - val_accuracy: 0.9565\n",
      "Epoch 443/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0746 - accuracy: 0.9800\n",
      "Epoch 00443: val_loss did not improve from 0.09698\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0660 - accuracy: 0.9832 - val_loss: 0.1092 - val_accuracy: 0.9565\n",
      "Epoch 444/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0746 - accuracy: 0.9740\n",
      "Epoch 00444: val_loss did not improve from 0.09698\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0669 - accuracy: 0.9770 - val_loss: 0.1038 - val_accuracy: 0.9627\n",
      "Epoch 445/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0698 - accuracy: 0.9740\n",
      "Epoch 00445: val_loss did not improve from 0.09698\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0633 - accuracy: 0.9770 - val_loss: 0.1076 - val_accuracy: 0.9565\n",
      "Epoch 446/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0511 - accuracy: 0.9840\n",
      "Epoch 00446: val_loss did not improve from 0.09698\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0672 - accuracy: 0.9816 - val_loss: 0.1022 - val_accuracy: 0.9627\n",
      "Epoch 447/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0709 - accuracy: 0.9760\n",
      "Epoch 00447: val_loss did not improve from 0.09698\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0633 - accuracy: 0.9786 - val_loss: 0.1075 - val_accuracy: 0.9596\n",
      "Epoch 448/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0688 - accuracy: 0.9760\n",
      "Epoch 00448: val_loss did not improve from 0.09698\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0676 - accuracy: 0.9755 - val_loss: 0.1008 - val_accuracy: 0.9658\n",
      "Epoch 449/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0679 - accuracy: 0.9800\n",
      "Epoch 00449: val_loss did not improve from 0.09698\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0623 - accuracy: 0.9816 - val_loss: 0.1044 - val_accuracy: 0.9596\n",
      "Epoch 450/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0623 - accuracy: 0.9860\n",
      "Epoch 00450: val_loss did not improve from 0.09698\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0668 - accuracy: 0.9816 - val_loss: 0.0992 - val_accuracy: 0.9658\n",
      "Epoch 451/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0592 - accuracy: 0.9840\n",
      "Epoch 00451: val_loss did not improve from 0.09698\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0621 - accuracy: 0.9832 - val_loss: 0.1057 - val_accuracy: 0.9658\n",
      "Epoch 452/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0717 - accuracy: 0.9760\n",
      "Epoch 00452: val_loss did not improve from 0.09698\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0685 - accuracy: 0.9770 - val_loss: 0.0972 - val_accuracy: 0.9689\n",
      "Epoch 453/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0577 - accuracy: 0.9820\n",
      "Epoch 00453: val_loss did not improve from 0.09698\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0624 - accuracy: 0.9801 - val_loss: 0.1000 - val_accuracy: 0.9689\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 454/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0732 - accuracy: 0.9800\n",
      "Epoch 00454: val_loss improved from 0.09698 to 0.09543, saving model to ./model\\454-0.0954.hdf5\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0654 - accuracy: 0.9816 - val_loss: 0.0954 - val_accuracy: 0.9720\n",
      "Epoch 455/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0731 - accuracy: 0.9760\n",
      "Epoch 00455: val_loss improved from 0.09543 to 0.09468, saving model to ./model\\455-0.0947.hdf5\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.0644 - accuracy: 0.9816 - val_loss: 0.0947 - val_accuracy: 0.9689\n",
      "Epoch 456/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0621 - accuracy: 0.9840\n",
      "Epoch 00456: val_loss did not improve from 0.09468\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0624 - accuracy: 0.9832 - val_loss: 0.0955 - val_accuracy: 0.9720\n",
      "Epoch 457/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0586 - accuracy: 0.9820\n",
      "Epoch 00457: val_loss did not improve from 0.09468\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0637 - accuracy: 0.9816 - val_loss: 0.0994 - val_accuracy: 0.9689\n",
      "Epoch 458/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0629 - accuracy: 0.9820\n",
      "Epoch 00458: val_loss improved from 0.09468 to 0.09462, saving model to ./model\\458-0.0946.hdf5\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0650 - accuracy: 0.9832 - val_loss: 0.0946 - val_accuracy: 0.9689\n",
      "Epoch 459/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0520 - accuracy: 0.9860\n",
      "Epoch 00459: val_loss did not improve from 0.09462\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0633 - accuracy: 0.9786 - val_loss: 0.0978 - val_accuracy: 0.9689\n",
      "Epoch 460/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0722 - accuracy: 0.9720\n",
      "Epoch 00460: val_loss did not improve from 0.09462\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0633 - accuracy: 0.9770 - val_loss: 0.0959 - val_accuracy: 0.9689\n",
      "Epoch 461/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0539 - accuracy: 0.9840\n",
      "Epoch 00461: val_loss did not improve from 0.09462\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0651 - accuracy: 0.9816 - val_loss: 0.0947 - val_accuracy: 0.9689\n",
      "Epoch 462/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0615 - accuracy: 0.9840\n",
      "Epoch 00462: val_loss did not improve from 0.09462\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0620 - accuracy: 0.9801 - val_loss: 0.0991 - val_accuracy: 0.9689\n",
      "Epoch 463/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0700 - accuracy: 0.9780\n",
      "Epoch 00463: val_loss improved from 0.09462 to 0.09298, saving model to ./model\\463-0.0930.hdf5\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0674 - accuracy: 0.9770 - val_loss: 0.0930 - val_accuracy: 0.9689\n",
      "Epoch 464/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0402 - accuracy: 0.9900\n",
      "Epoch 00464: val_loss did not improve from 0.09298\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0668 - accuracy: 0.9832 - val_loss: 0.1006 - val_accuracy: 0.9658\n",
      "Epoch 465/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0774 - accuracy: 0.9780\n",
      "Epoch 00465: val_loss improved from 0.09298 to 0.09262, saving model to ./model\\465-0.0926.hdf5\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0680 - accuracy: 0.9816 - val_loss: 0.0926 - val_accuracy: 0.9689\n",
      "Epoch 466/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0651 - accuracy: 0.9800\n",
      "Epoch 00466: val_loss did not improve from 0.09262\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0620 - accuracy: 0.9801 - val_loss: 0.0929 - val_accuracy: 0.9689\n",
      "Epoch 467/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0495 - accuracy: 0.9900\n",
      "Epoch 00467: val_loss did not improve from 0.09262\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0613 - accuracy: 0.9832 - val_loss: 0.0928 - val_accuracy: 0.9689\n",
      "Epoch 468/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0667 - accuracy: 0.9820\n",
      "Epoch 00468: val_loss did not improve from 0.09262\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0612 - accuracy: 0.9832 - val_loss: 0.0930 - val_accuracy: 0.9689\n",
      "Epoch 469/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0476 - accuracy: 0.9920\n",
      "Epoch 00469: val_loss did not improve from 0.09262\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0604 - accuracy: 0.9847 - val_loss: 0.0950 - val_accuracy: 0.9689\n",
      "Epoch 470/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0639 - accuracy: 0.9820\n",
      "Epoch 00470: val_loss did not improve from 0.09262\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0613 - accuracy: 0.9816 - val_loss: 0.0954 - val_accuracy: 0.9689\n",
      "Epoch 471/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0664 - accuracy: 0.9800\n",
      "Epoch 00471: val_loss did not improve from 0.09262\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0605 - accuracy: 0.9832 - val_loss: 0.0961 - val_accuracy: 0.9720\n",
      "Epoch 472/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0578 - accuracy: 0.9820\n",
      "Epoch 00472: val_loss did not improve from 0.09262\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0621 - accuracy: 0.9816 - val_loss: 0.0966 - val_accuracy: 0.9720\n",
      "Epoch 473/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0683 - accuracy: 0.9800\n",
      "Epoch 00473: val_loss did not improve from 0.09262\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0611 - accuracy: 0.9816 - val_loss: 0.0978 - val_accuracy: 0.9689\n",
      "Epoch 474/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0617 - accuracy: 0.9820\n",
      "Epoch 00474: val_loss did not improve from 0.09262\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0618 - accuracy: 0.9786 - val_loss: 0.0985 - val_accuracy: 0.9689\n",
      "Epoch 475/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0498 - accuracy: 0.9880\n",
      "Epoch 00475: val_loss did not improve from 0.09262\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0617 - accuracy: 0.9816 - val_loss: 0.0988 - val_accuracy: 0.9689\n",
      "Epoch 476/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0667 - accuracy: 0.9800\n",
      "Epoch 00476: val_loss did not improve from 0.09262\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0601 - accuracy: 0.9801 - val_loss: 0.1000 - val_accuracy: 0.9689\n",
      "Epoch 477/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0683 - accuracy: 0.9800\n",
      "Epoch 00477: val_loss did not improve from 0.09262\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0599 - accuracy: 0.9816 - val_loss: 0.1009 - val_accuracy: 0.9689\n",
      "Epoch 478/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0552 - accuracy: 0.9820\n",
      "Epoch 00478: val_loss did not improve from 0.09262\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0599 - accuracy: 0.9816 - val_loss: 0.1023 - val_accuracy: 0.9689\n",
      "Epoch 479/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0570 - accuracy: 0.9820\n",
      "Epoch 00479: val_loss did not improve from 0.09262\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0607 - accuracy: 0.9816 - val_loss: 0.1026 - val_accuracy: 0.9689\n",
      "Epoch 480/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0687 - accuracy: 0.9800\n",
      "Epoch 00480: val_loss did not improve from 0.09262\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0612 - accuracy: 0.9816 - val_loss: 0.0994 - val_accuracy: 0.9689\n",
      "Epoch 481/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0515 - accuracy: 0.9860\n",
      "Epoch 00481: val_loss did not improve from 0.09262\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0598 - accuracy: 0.9816 - val_loss: 0.0983 - val_accuracy: 0.9689\n",
      "Epoch 482/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0565 - accuracy: 0.9800\n",
      "Epoch 00482: val_loss did not improve from 0.09262\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0596 - accuracy: 0.9816 - val_loss: 0.0967 - val_accuracy: 0.9720\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 483/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0555 - accuracy: 0.9820\n",
      "Epoch 00483: val_loss did not improve from 0.09262\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0597 - accuracy: 0.9816 - val_loss: 0.0969 - val_accuracy: 0.9689\n",
      "Epoch 484/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0654 - accuracy: 0.9800\n",
      "Epoch 00484: val_loss did not improve from 0.09262\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0600 - accuracy: 0.9816 - val_loss: 0.0976 - val_accuracy: 0.9689\n",
      "Epoch 485/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0687 - accuracy: 0.9760\n",
      "Epoch 00485: val_loss did not improve from 0.09262\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0591 - accuracy: 0.9816 - val_loss: 0.1010 - val_accuracy: 0.9720\n",
      "Epoch 486/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0671 - accuracy: 0.9800\n",
      "Epoch 00486: val_loss did not improve from 0.09262\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0621 - accuracy: 0.9816 - val_loss: 0.1064 - val_accuracy: 0.9627\n",
      "Epoch 487/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0718 - accuracy: 0.9820\n",
      "Epoch 00487: val_loss did not improve from 0.09262\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0644 - accuracy: 0.9832 - val_loss: 0.0982 - val_accuracy: 0.9720\n",
      "Epoch 488/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0618 - accuracy: 0.9840\n",
      "Epoch 00488: val_loss did not improve from 0.09262\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0604 - accuracy: 0.9816 - val_loss: 0.1026 - val_accuracy: 0.9658\n",
      "Epoch 489/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0493 - accuracy: 0.9860\n",
      "Epoch 00489: val_loss did not improve from 0.09262\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0616 - accuracy: 0.9832 - val_loss: 0.0994 - val_accuracy: 0.9689\n",
      "Epoch 490/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0665 - accuracy: 0.9800\n",
      "Epoch 00490: val_loss did not improve from 0.09262\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0599 - accuracy: 0.9816 - val_loss: 0.1051 - val_accuracy: 0.9596\n",
      "Epoch 491/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0781 - accuracy: 0.9800\n",
      "Epoch 00491: val_loss did not improve from 0.09262\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0656 - accuracy: 0.9832 - val_loss: 0.0948 - val_accuracy: 0.9689\n",
      "Epoch 492/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0483 - accuracy: 0.9880\n",
      "Epoch 00492: val_loss did not improve from 0.09262\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0593 - accuracy: 0.9816 - val_loss: 0.0939 - val_accuracy: 0.9752\n",
      "Epoch 493/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0687 - accuracy: 0.9740\n",
      "Epoch 00493: val_loss improved from 0.09262 to 0.09103, saving model to ./model\\493-0.0910.hdf5\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0608 - accuracy: 0.9786 - val_loss: 0.0910 - val_accuracy: 0.9720\n",
      "Epoch 494/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0657 - accuracy: 0.9800\n",
      "Epoch 00494: val_loss did not improve from 0.09103\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0584 - accuracy: 0.9847 - val_loss: 0.0956 - val_accuracy: 0.9689\n",
      "Epoch 495/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0642 - accuracy: 0.9860\n",
      "Epoch 00495: val_loss did not improve from 0.09103\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0623 - accuracy: 0.9832 - val_loss: 0.0922 - val_accuracy: 0.9689\n",
      "Epoch 496/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0631 - accuracy: 0.9800\n",
      "Epoch 00496: val_loss did not improve from 0.09103\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0590 - accuracy: 0.9832 - val_loss: 0.0981 - val_accuracy: 0.9689\n",
      "Epoch 497/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0538 - accuracy: 0.9800\n",
      "Epoch 00497: val_loss did not improve from 0.09103\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0620 - accuracy: 0.9770 - val_loss: 0.0942 - val_accuracy: 0.9720\n",
      "Epoch 498/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0629 - accuracy: 0.9820\n",
      "Epoch 00498: val_loss did not improve from 0.09103\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0582 - accuracy: 0.9832 - val_loss: 0.0977 - val_accuracy: 0.9658\n",
      "Epoch 499/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0629 - accuracy: 0.9840\n",
      "Epoch 00499: val_loss did not improve from 0.09103\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0603 - accuracy: 0.9816 - val_loss: 0.0960 - val_accuracy: 0.9720\n",
      "Epoch 500/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0650 - accuracy: 0.9820\n",
      "Epoch 00500: val_loss did not improve from 0.09103\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0584 - accuracy: 0.9832 - val_loss: 0.0971 - val_accuracy: 0.9658\n",
      "Epoch 501/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0527 - accuracy: 0.9860\n",
      "Epoch 00501: val_loss did not improve from 0.09103\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0592 - accuracy: 0.9832 - val_loss: 0.0951 - val_accuracy: 0.9689\n",
      "Epoch 502/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0610 - accuracy: 0.9800\n",
      "Epoch 00502: val_loss did not improve from 0.09103\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0579 - accuracy: 0.9832 - val_loss: 0.0954 - val_accuracy: 0.9689\n",
      "Epoch 503/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0509 - accuracy: 0.9820\n",
      "Epoch 00503: val_loss did not improve from 0.09103\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0599 - accuracy: 0.9816 - val_loss: 0.0943 - val_accuracy: 0.9689\n",
      "Epoch 504/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0653 - accuracy: 0.9820\n",
      "Epoch 00504: val_loss did not improve from 0.09103\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0596 - accuracy: 0.9816 - val_loss: 0.0931 - val_accuracy: 0.9689\n",
      "Epoch 505/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0629 - accuracy: 0.9800\n",
      "Epoch 00505: val_loss did not improve from 0.09103\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0590 - accuracy: 0.9816 - val_loss: 0.0923 - val_accuracy: 0.9720\n",
      "Epoch 506/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0658 - accuracy: 0.9820\n",
      "Epoch 00506: val_loss did not improve from 0.09103\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0569 - accuracy: 0.9847 - val_loss: 0.0995 - val_accuracy: 0.9689\n",
      "Epoch 507/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0539 - accuracy: 0.9900\n",
      "Epoch 00507: val_loss did not improve from 0.09103\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0648 - accuracy: 0.9832 - val_loss: 0.0926 - val_accuracy: 0.9689\n",
      "Epoch 508/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0570 - accuracy: 0.9840\n",
      "Epoch 00508: val_loss did not improve from 0.09103\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0576 - accuracy: 0.9847 - val_loss: 0.1066 - val_accuracy: 0.9689\n",
      "Epoch 509/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0699 - accuracy: 0.9800\n",
      "Epoch 00509: val_loss did not improve from 0.09103\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0690 - accuracy: 0.9786 - val_loss: 0.0946 - val_accuracy: 0.9689\n",
      "Epoch 510/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0635 - accuracy: 0.9840\n",
      "Epoch 00510: val_loss did not improve from 0.09103\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0615 - accuracy: 0.9847 - val_loss: 0.1131 - val_accuracy: 0.9596\n",
      "Epoch 511/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0739 - accuracy: 0.9840\n",
      "Epoch 00511: val_loss did not improve from 0.09103\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0684 - accuracy: 0.9816 - val_loss: 0.0968 - val_accuracy: 0.9720\n",
      "Epoch 512/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0565 - accuracy: 0.9840\n",
      "Epoch 00512: val_loss did not improve from 0.09103\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0574 - accuracy: 0.9847 - val_loss: 0.1074 - val_accuracy: 0.9658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 513/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0779 - accuracy: 0.9760\n",
      "Epoch 00513: val_loss did not improve from 0.09103\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0662 - accuracy: 0.9801 - val_loss: 0.0958 - val_accuracy: 0.9689\n",
      "Epoch 514/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0484 - accuracy: 0.9860\n",
      "Epoch 00514: val_loss did not improve from 0.09103\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0585 - accuracy: 0.9832 - val_loss: 0.1087 - val_accuracy: 0.9596\n",
      "Epoch 515/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0643 - accuracy: 0.9820\n",
      "Epoch 00515: val_loss did not improve from 0.09103\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0679 - accuracy: 0.9832 - val_loss: 0.0926 - val_accuracy: 0.9689\n",
      "Epoch 516/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0544 - accuracy: 0.9860\n",
      "Epoch 00516: val_loss did not improve from 0.09103\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0567 - accuracy: 0.9862 - val_loss: 0.0982 - val_accuracy: 0.9689\n",
      "Epoch 517/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0592 - accuracy: 0.9860\n",
      "Epoch 00517: val_loss did not improve from 0.09103\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0643 - accuracy: 0.9816 - val_loss: 0.0912 - val_accuracy: 0.9783\n",
      "Epoch 518/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0618 - accuracy: 0.9840\n",
      "Epoch 00518: val_loss did not improve from 0.09103\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0570 - accuracy: 0.9847 - val_loss: 0.0968 - val_accuracy: 0.9627\n",
      "Epoch 519/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0556 - accuracy: 0.9840\n",
      "Epoch 00519: val_loss did not improve from 0.09103\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0629 - accuracy: 0.9832 - val_loss: 0.0972 - val_accuracy: 0.9658\n",
      "Epoch 520/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0758 - accuracy: 0.9780\n",
      "Epoch 00520: val_loss did not improve from 0.09103\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0629 - accuracy: 0.9832 - val_loss: 0.0925 - val_accuracy: 0.9752\n",
      "Epoch 521/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0488 - accuracy: 0.9880\n",
      "Epoch 00521: val_loss did not improve from 0.09103\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0573 - accuracy: 0.9847 - val_loss: 0.0942 - val_accuracy: 0.9720\n",
      "Epoch 522/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0622 - accuracy: 0.9840\n",
      "Epoch 00522: val_loss did not improve from 0.09103\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0570 - accuracy: 0.9862 - val_loss: 0.0957 - val_accuracy: 0.9689\n",
      "Epoch 523/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0598 - accuracy: 0.9840\n",
      "Epoch 00523: val_loss did not improve from 0.09103\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0567 - accuracy: 0.9832 - val_loss: 0.0977 - val_accuracy: 0.9689\n",
      "Epoch 524/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0612 - accuracy: 0.9840\n",
      "Epoch 00524: val_loss did not improve from 0.09103\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0574 - accuracy: 0.9832 - val_loss: 0.0949 - val_accuracy: 0.9720\n",
      "Epoch 525/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0514 - accuracy: 0.9880\n",
      "Epoch 00525: val_loss did not improve from 0.09103\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0560 - accuracy: 0.9862 - val_loss: 0.0949 - val_accuracy: 0.9720\n",
      "Epoch 526/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0552 - accuracy: 0.9840\n",
      "Epoch 00526: val_loss did not improve from 0.09103\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0572 - accuracy: 0.9847 - val_loss: 0.0936 - val_accuracy: 0.9720\n",
      "Epoch 527/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0609 - accuracy: 0.9860\n",
      "Epoch 00527: val_loss did not improve from 0.09103\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0565 - accuracy: 0.9877 - val_loss: 0.0934 - val_accuracy: 0.9689\n",
      "Epoch 528/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0515 - accuracy: 0.9860\n",
      "Epoch 00528: val_loss did not improve from 0.09103\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0577 - accuracy: 0.9847 - val_loss: 0.0935 - val_accuracy: 0.9689\n",
      "Epoch 529/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0480 - accuracy: 0.9880\n",
      "Epoch 00529: val_loss did not improve from 0.09103\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0564 - accuracy: 0.9847 - val_loss: 0.0930 - val_accuracy: 0.9720\n",
      "Epoch 530/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0639 - accuracy: 0.9840\n",
      "Epoch 00530: val_loss improved from 0.09103 to 0.09102, saving model to ./model\\530-0.0910.hdf5\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0586 - accuracy: 0.9847 - val_loss: 0.0910 - val_accuracy: 0.9752\n",
      "Epoch 531/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0629 - accuracy: 0.9860\n",
      "Epoch 00531: val_loss did not improve from 0.09102\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0552 - accuracy: 0.9893 - val_loss: 0.0965 - val_accuracy: 0.9689\n",
      "Epoch 532/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0588 - accuracy: 0.9820\n",
      "Epoch 00532: val_loss did not improve from 0.09102\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0614 - accuracy: 0.9847 - val_loss: 0.0978 - val_accuracy: 0.9720\n",
      "Epoch 533/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0654 - accuracy: 0.9840\n",
      "Epoch 00533: val_loss did not improve from 0.09102\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0615 - accuracy: 0.9847 - val_loss: 0.0912 - val_accuracy: 0.9720\n",
      "Epoch 534/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0549 - accuracy: 0.9900\n",
      "Epoch 00534: val_loss did not improve from 0.09102\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0570 - accuracy: 0.9893 - val_loss: 0.0916 - val_accuracy: 0.9720\n",
      "Epoch 535/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0612 - accuracy: 0.9840\n",
      "Epoch 00535: val_loss did not improve from 0.09102\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0556 - accuracy: 0.9862 - val_loss: 0.0918 - val_accuracy: 0.9689\n",
      "Epoch 536/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0661 - accuracy: 0.9820\n",
      "Epoch 00536: val_loss did not improve from 0.09102\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0575 - accuracy: 0.9832 - val_loss: 0.0936 - val_accuracy: 0.9689\n",
      "Epoch 537/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0570 - accuracy: 0.9900\n",
      "Epoch 00537: val_loss improved from 0.09102 to 0.08830, saving model to ./model\\537-0.0883.hdf5\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0590 - accuracy: 0.9847 - val_loss: 0.0883 - val_accuracy: 0.9752\n",
      "Epoch 538/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0600 - accuracy: 0.9840\n",
      "Epoch 00538: val_loss did not improve from 0.08830\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0565 - accuracy: 0.9847 - val_loss: 0.0927 - val_accuracy: 0.9752\n",
      "Epoch 539/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0636 - accuracy: 0.9840\n",
      "Epoch 00539: val_loss did not improve from 0.08830\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0579 - accuracy: 0.9847 - val_loss: 0.0917 - val_accuracy: 0.9689\n",
      "Epoch 540/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0566 - accuracy: 0.9860\n",
      "Epoch 00540: val_loss did not improve from 0.08830\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0558 - accuracy: 0.9847 - val_loss: 0.0972 - val_accuracy: 0.9689\n",
      "Epoch 541/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0649 - accuracy: 0.9840\n",
      "Epoch 00541: val_loss did not improve from 0.08830\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0583 - accuracy: 0.9847 - val_loss: 0.0932 - val_accuracy: 0.9720\n",
      "Epoch 542/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0577 - accuracy: 0.9860\n",
      "Epoch 00542: val_loss did not improve from 0.08830\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0558 - accuracy: 0.9862 - val_loss: 0.0959 - val_accuracy: 0.9720\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 543/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0382 - accuracy: 0.9860\n",
      "Epoch 00543: val_loss did not improve from 0.08830\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0580 - accuracy: 0.9847 - val_loss: 0.0922 - val_accuracy: 0.9720\n",
      "Epoch 544/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0478 - accuracy: 0.9840\n",
      "Epoch 00544: val_loss did not improve from 0.08830\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0547 - accuracy: 0.9862 - val_loss: 0.0917 - val_accuracy: 0.9689\n",
      "Epoch 545/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0518 - accuracy: 0.9840\n",
      "Epoch 00545: val_loss did not improve from 0.08830\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0554 - accuracy: 0.9847 - val_loss: 0.0904 - val_accuracy: 0.9689\n",
      "Epoch 546/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0470 - accuracy: 0.9880\n",
      "Epoch 00546: val_loss did not improve from 0.08830\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0548 - accuracy: 0.9847 - val_loss: 0.0907 - val_accuracy: 0.9752\n",
      "Epoch 547/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0595 - accuracy: 0.9880\n",
      "Epoch 00547: val_loss did not improve from 0.08830\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0568 - accuracy: 0.9877 - val_loss: 0.0909 - val_accuracy: 0.9720\n",
      "Epoch 548/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0583 - accuracy: 0.9880\n",
      "Epoch 00548: val_loss did not improve from 0.08830\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0555 - accuracy: 0.9862 - val_loss: 0.0932 - val_accuracy: 0.9689\n",
      "Epoch 549/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0495 - accuracy: 0.9840\n",
      "Epoch 00549: val_loss did not improve from 0.08830\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0561 - accuracy: 0.9832 - val_loss: 0.0920 - val_accuracy: 0.9720\n",
      "Epoch 550/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0612 - accuracy: 0.9840\n",
      "Epoch 00550: val_loss did not improve from 0.08830\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0544 - accuracy: 0.9877 - val_loss: 0.0936 - val_accuracy: 0.9689\n",
      "Epoch 551/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0464 - accuracy: 0.9920\n",
      "Epoch 00551: val_loss did not improve from 0.08830\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0561 - accuracy: 0.9862 - val_loss: 0.0930 - val_accuracy: 0.9689\n",
      "Epoch 552/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0592 - accuracy: 0.9880\n",
      "Epoch 00552: val_loss did not improve from 0.08830\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0556 - accuracy: 0.9877 - val_loss: 0.0918 - val_accuracy: 0.9720\n",
      "Epoch 553/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0559 - accuracy: 0.9880\n",
      "Epoch 00553: val_loss did not improve from 0.08830\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0542 - accuracy: 0.9862 - val_loss: 0.0922 - val_accuracy: 0.9689\n",
      "Epoch 554/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0640 - accuracy: 0.9800\n",
      "Epoch 00554: val_loss did not improve from 0.08830\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0547 - accuracy: 0.9847 - val_loss: 0.0910 - val_accuracy: 0.9689\n",
      "Epoch 555/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0588 - accuracy: 0.9840\n",
      "Epoch 00555: val_loss did not improve from 0.08830\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0546 - accuracy: 0.9847 - val_loss: 0.0894 - val_accuracy: 0.9720\n",
      "Epoch 556/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0506 - accuracy: 0.9860\n",
      "Epoch 00556: val_loss did not improve from 0.08830\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0540 - accuracy: 0.9862 - val_loss: 0.0911 - val_accuracy: 0.9720\n",
      "Epoch 557/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0638 - accuracy: 0.9840\n",
      "Epoch 00557: val_loss did not improve from 0.08830\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0555 - accuracy: 0.9862 - val_loss: 0.0911 - val_accuracy: 0.9720\n",
      "Epoch 558/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0495 - accuracy: 0.9880\n",
      "Epoch 00558: val_loss did not improve from 0.08830\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0551 - accuracy: 0.9862 - val_loss: 0.0929 - val_accuracy: 0.9720\n",
      "Epoch 559/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0633 - accuracy: 0.9840\n",
      "Epoch 00559: val_loss did not improve from 0.08830\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0540 - accuracy: 0.9862 - val_loss: 0.0943 - val_accuracy: 0.9720\n",
      "Epoch 560/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0488 - accuracy: 0.9920\n",
      "Epoch 00560: val_loss did not improve from 0.08830\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0542 - accuracy: 0.9893 - val_loss: 0.0955 - val_accuracy: 0.9720\n",
      "Epoch 561/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0601 - accuracy: 0.9860\n",
      "Epoch 00561: val_loss did not improve from 0.08830\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0545 - accuracy: 0.9877 - val_loss: 0.0965 - val_accuracy: 0.9720\n",
      "Epoch 562/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0560 - accuracy: 0.9880\n",
      "Epoch 00562: val_loss did not improve from 0.08830\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0546 - accuracy: 0.9862 - val_loss: 0.0994 - val_accuracy: 0.9689\n",
      "Epoch 563/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0597 - accuracy: 0.9820\n",
      "Epoch 00563: val_loss did not improve from 0.08830\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0555 - accuracy: 0.9832 - val_loss: 0.0972 - val_accuracy: 0.9720\n",
      "Epoch 564/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0544 - accuracy: 0.9900\n",
      "Epoch 00564: val_loss did not improve from 0.08830\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0535 - accuracy: 0.9877 - val_loss: 0.0973 - val_accuracy: 0.9720\n",
      "Epoch 565/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0600 - accuracy: 0.9840\n",
      "Epoch 00565: val_loss did not improve from 0.08830\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0544 - accuracy: 0.9862 - val_loss: 0.0949 - val_accuracy: 0.9720\n",
      "Epoch 566/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0500 - accuracy: 0.9860\n",
      "Epoch 00566: val_loss did not improve from 0.08830\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0542 - accuracy: 0.9862 - val_loss: 0.0933 - val_accuracy: 0.9689\n",
      "Epoch 567/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0602 - accuracy: 0.9860\n",
      "Epoch 00567: val_loss did not improve from 0.08830\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0533 - accuracy: 0.9877 - val_loss: 0.0926 - val_accuracy: 0.9720\n",
      "Epoch 568/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0569 - accuracy: 0.9880\n",
      "Epoch 00568: val_loss did not improve from 0.08830\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0535 - accuracy: 0.9877 - val_loss: 0.0919 - val_accuracy: 0.9720\n",
      "Epoch 569/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0619 - accuracy: 0.9880\n",
      "Epoch 00569: val_loss did not improve from 0.08830\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0535 - accuracy: 0.9877 - val_loss: 0.0911 - val_accuracy: 0.9720\n",
      "Epoch 570/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0632 - accuracy: 0.9840\n",
      "Epoch 00570: val_loss did not improve from 0.08830\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0533 - accuracy: 0.9877 - val_loss: 0.0913 - val_accuracy: 0.9720\n",
      "Epoch 571/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0458 - accuracy: 0.9880\n",
      "Epoch 00571: val_loss did not improve from 0.08830\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0535 - accuracy: 0.9862 - val_loss: 0.0916 - val_accuracy: 0.9720\n",
      "Epoch 572/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0550 - accuracy: 0.9900\n",
      "Epoch 00572: val_loss did not improve from 0.08830\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0528 - accuracy: 0.9877 - val_loss: 0.0931 - val_accuracy: 0.9752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 573/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0444 - accuracy: 0.9880\n",
      "Epoch 00573: val_loss did not improve from 0.08830\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0548 - accuracy: 0.9847 - val_loss: 0.0916 - val_accuracy: 0.9720\n",
      "Epoch 574/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0395 - accuracy: 0.9860\n",
      "Epoch 00574: val_loss did not improve from 0.08830\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0537 - accuracy: 0.9862 - val_loss: 0.0972 - val_accuracy: 0.9689\n",
      "Epoch 575/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0554 - accuracy: 0.9820\n",
      "Epoch 00575: val_loss did not improve from 0.08830\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0567 - accuracy: 0.9832 - val_loss: 0.0917 - val_accuracy: 0.9720\n",
      "Epoch 576/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0500 - accuracy: 0.9880\n",
      "Epoch 00576: val_loss did not improve from 0.08830\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0521 - accuracy: 0.9877 - val_loss: 0.0987 - val_accuracy: 0.9783\n",
      "Epoch 577/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0652 - accuracy: 0.9860\n",
      "Epoch 00577: val_loss did not improve from 0.08830\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0584 - accuracy: 0.9862 - val_loss: 0.0940 - val_accuracy: 0.9720\n",
      "Epoch 578/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0621 - accuracy: 0.9820\n",
      "Epoch 00578: val_loss did not improve from 0.08830\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0524 - accuracy: 0.9862 - val_loss: 0.1072 - val_accuracy: 0.9627\n",
      "Epoch 579/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0520 - accuracy: 0.9900\n",
      "Epoch 00579: val_loss did not improve from 0.08830\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0612 - accuracy: 0.9832 - val_loss: 0.0978 - val_accuracy: 0.9689\n",
      "Epoch 580/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0619 - accuracy: 0.9800\n",
      "Epoch 00580: val_loss did not improve from 0.08830\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0544 - accuracy: 0.9832 - val_loss: 0.1061 - val_accuracy: 0.9752\n",
      "Epoch 581/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0648 - accuracy: 0.9880\n",
      "Epoch 00581: val_loss did not improve from 0.08830\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0612 - accuracy: 0.9847 - val_loss: 0.0994 - val_accuracy: 0.9720\n",
      "Epoch 582/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0618 - accuracy: 0.9800\n",
      "Epoch 00582: val_loss did not improve from 0.08830\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0520 - accuracy: 0.9847 - val_loss: 0.1150 - val_accuracy: 0.9565\n",
      "Epoch 583/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0738 - accuracy: 0.9800\n",
      "Epoch 00583: val_loss did not improve from 0.08830\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0629 - accuracy: 0.9832 - val_loss: 0.1066 - val_accuracy: 0.9658\n",
      "Epoch 584/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0615 - accuracy: 0.9820\n",
      "Epoch 00584: val_loss did not improve from 0.08830\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0557 - accuracy: 0.9832 - val_loss: 0.1018 - val_accuracy: 0.9689\n",
      "Epoch 585/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0619 - accuracy: 0.9800\n",
      "Epoch 00585: val_loss did not improve from 0.08830\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0571 - accuracy: 0.9847 - val_loss: 0.0987 - val_accuracy: 0.9689\n",
      "Epoch 586/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0606 - accuracy: 0.9820\n",
      "Epoch 00586: val_loss did not improve from 0.08830\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0547 - accuracy: 0.9832 - val_loss: 0.0986 - val_accuracy: 0.9658\n",
      "Epoch 587/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0504 - accuracy: 0.9840\n",
      "Epoch 00587: val_loss did not improve from 0.08830\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0548 - accuracy: 0.9847 - val_loss: 0.0955 - val_accuracy: 0.9689\n",
      "Epoch 588/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0554 - accuracy: 0.9860\n",
      "Epoch 00588: val_loss did not improve from 0.08830\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0538 - accuracy: 0.9847 - val_loss: 0.0927 - val_accuracy: 0.9752\n",
      "Epoch 589/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0497 - accuracy: 0.9860\n",
      "Epoch 00589: val_loss did not improve from 0.08830\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0527 - accuracy: 0.9862 - val_loss: 0.0942 - val_accuracy: 0.9783\n",
      "Epoch 590/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0487 - accuracy: 0.9860\n",
      "Epoch 00590: val_loss did not improve from 0.08830\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0551 - accuracy: 0.9832 - val_loss: 0.0947 - val_accuracy: 0.9752\n",
      "Epoch 591/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0508 - accuracy: 0.9840\n",
      "Epoch 00591: val_loss did not improve from 0.08830\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0533 - accuracy: 0.9847 - val_loss: 0.0955 - val_accuracy: 0.9720\n",
      "Epoch 592/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0621 - accuracy: 0.9840\n",
      "Epoch 00592: val_loss did not improve from 0.08830\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0519 - accuracy: 0.9862 - val_loss: 0.0994 - val_accuracy: 0.9689\n",
      "Epoch 593/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0624 - accuracy: 0.9800\n",
      "Epoch 00593: val_loss did not improve from 0.08830\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0533 - accuracy: 0.9847 - val_loss: 0.1003 - val_accuracy: 0.9689\n",
      "Epoch 594/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0367 - accuracy: 0.9880\n",
      "Epoch 00594: val_loss did not improve from 0.08830\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0525 - accuracy: 0.9847 - val_loss: 0.0998 - val_accuracy: 0.9720\n",
      "Epoch 595/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0509 - accuracy: 0.9860\n",
      "Epoch 00595: val_loss did not improve from 0.08830\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0535 - accuracy: 0.9847 - val_loss: 0.1003 - val_accuracy: 0.9720\n",
      "Epoch 596/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0527 - accuracy: 0.9840\n",
      "Epoch 00596: val_loss did not improve from 0.08830\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0551 - accuracy: 0.9862 - val_loss: 0.0971 - val_accuracy: 0.9689\n",
      "Epoch 597/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0589 - accuracy: 0.9840\n",
      "Epoch 00597: val_loss did not improve from 0.08830\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0523 - accuracy: 0.9847 - val_loss: 0.0979 - val_accuracy: 0.9689\n",
      "Epoch 598/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0354 - accuracy: 0.9860\n",
      "Epoch 00598: val_loss did not improve from 0.08830\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0546 - accuracy: 0.9832 - val_loss: 0.0906 - val_accuracy: 0.9720\n",
      "Epoch 599/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0487 - accuracy: 0.9880\n",
      "Epoch 00599: val_loss did not improve from 0.08830\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0524 - accuracy: 0.9877 - val_loss: 0.0967 - val_accuracy: 0.9814\n",
      "Epoch 600/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0665 - accuracy: 0.9840\n",
      "Epoch 00600: val_loss improved from 0.08830 to 0.08745, saving model to ./model\\600-0.0874.hdf5\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.0590 - accuracy: 0.9877 - val_loss: 0.0874 - val_accuracy: 0.9689\n",
      "Epoch 601/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0475 - accuracy: 0.9880\n",
      "Epoch 00601: val_loss did not improve from 0.08745\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0560 - accuracy: 0.9862 - val_loss: 0.0940 - val_accuracy: 0.9689\n",
      "Epoch 602/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0709 - accuracy: 0.9780\n",
      "Epoch 00602: val_loss did not improve from 0.08745\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0591 - accuracy: 0.9832 - val_loss: 0.0875 - val_accuracy: 0.9752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 603/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0553 - accuracy: 0.9860\n",
      "Epoch 00603: val_loss did not improve from 0.08745\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0520 - accuracy: 0.9862 - val_loss: 0.0899 - val_accuracy: 0.9752\n",
      "Epoch 604/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0482 - accuracy: 0.9880\n",
      "Epoch 00604: val_loss did not improve from 0.08745\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0527 - accuracy: 0.9877 - val_loss: 0.0909 - val_accuracy: 0.9689\n",
      "Epoch 605/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0530 - accuracy: 0.9880\n",
      "Epoch 00605: val_loss did not improve from 0.08745\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0515 - accuracy: 0.9862 - val_loss: 0.0967 - val_accuracy: 0.9689\n",
      "Epoch 606/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0504 - accuracy: 0.9820\n",
      "Epoch 00606: val_loss did not improve from 0.08745\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0542 - accuracy: 0.9832 - val_loss: 0.0952 - val_accuracy: 0.9689\n",
      "Epoch 607/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0390 - accuracy: 0.9860\n",
      "Epoch 00607: val_loss did not improve from 0.08745\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0513 - accuracy: 0.9862 - val_loss: 0.0970 - val_accuracy: 0.9720\n",
      "Epoch 608/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0472 - accuracy: 0.9860\n",
      "Epoch 00608: val_loss did not improve from 0.08745\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0545 - accuracy: 0.9847 - val_loss: 0.0977 - val_accuracy: 0.9720\n",
      "Epoch 609/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0612 - accuracy: 0.9820\n",
      "Epoch 00609: val_loss did not improve from 0.08745\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0531 - accuracy: 0.9847 - val_loss: 0.0979 - val_accuracy: 0.9689\n",
      "Epoch 610/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0551 - accuracy: 0.9860\n",
      "Epoch 00610: val_loss did not improve from 0.08745\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0534 - accuracy: 0.9847 - val_loss: 0.0972 - val_accuracy: 0.9689\n",
      "Epoch 611/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0601 - accuracy: 0.9820\n",
      "Epoch 00611: val_loss did not improve from 0.08745\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0548 - accuracy: 0.9847 - val_loss: 0.0905 - val_accuracy: 0.9689\n",
      "Epoch 612/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0600 - accuracy: 0.9840\n",
      "Epoch 00612: val_loss did not improve from 0.08745\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0514 - accuracy: 0.9862 - val_loss: 0.0890 - val_accuracy: 0.9689\n",
      "Epoch 613/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0493 - accuracy: 0.9860\n",
      "Epoch 00613: val_loss did not improve from 0.08745\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0511 - accuracy: 0.9877 - val_loss: 0.0886 - val_accuracy: 0.9689\n",
      "Epoch 614/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0464 - accuracy: 0.9880\n",
      "Epoch 00614: val_loss did not improve from 0.08745\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0511 - accuracy: 0.9877 - val_loss: 0.0886 - val_accuracy: 0.9689\n",
      "Epoch 615/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0547 - accuracy: 0.9860\n",
      "Epoch 00615: val_loss did not improve from 0.08745\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0514 - accuracy: 0.9893 - val_loss: 0.0901 - val_accuracy: 0.9689\n",
      "Epoch 616/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0564 - accuracy: 0.9860\n",
      "Epoch 00616: val_loss did not improve from 0.08745\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0510 - accuracy: 0.9877 - val_loss: 0.0922 - val_accuracy: 0.9689\n",
      "Epoch 617/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0571 - accuracy: 0.9860\n",
      "Epoch 00617: val_loss did not improve from 0.08745\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0520 - accuracy: 0.9877 - val_loss: 0.0899 - val_accuracy: 0.9689\n",
      "Epoch 618/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0399 - accuracy: 0.9880\n",
      "Epoch 00618: val_loss did not improve from 0.08745\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0508 - accuracy: 0.9862 - val_loss: 0.0892 - val_accuracy: 0.9752\n",
      "Epoch 619/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0595 - accuracy: 0.9860\n",
      "Epoch 00619: val_loss did not improve from 0.08745\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0513 - accuracy: 0.9877 - val_loss: 0.0887 - val_accuracy: 0.9752\n",
      "Epoch 620/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0379 - accuracy: 0.9900\n",
      "Epoch 00620: val_loss did not improve from 0.08745\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0526 - accuracy: 0.9893 - val_loss: 0.0894 - val_accuracy: 0.9720\n",
      "Epoch 621/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0356 - accuracy: 0.9900\n",
      "Epoch 00621: val_loss did not improve from 0.08745\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0502 - accuracy: 0.9893 - val_loss: 0.0959 - val_accuracy: 0.9783\n",
      "Epoch 622/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0603 - accuracy: 0.9860\n",
      "Epoch 00622: val_loss did not improve from 0.08745\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0548 - accuracy: 0.9862 - val_loss: 0.0942 - val_accuracy: 0.9720\n",
      "Epoch 623/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0537 - accuracy: 0.9860\n",
      "Epoch 00623: val_loss did not improve from 0.08745\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0515 - accuracy: 0.9847 - val_loss: 0.0970 - val_accuracy: 0.9689\n",
      "Epoch 624/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0438 - accuracy: 0.9860\n",
      "Epoch 00624: val_loss did not improve from 0.08745\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0517 - accuracy: 0.9832 - val_loss: 0.0943 - val_accuracy: 0.9720\n",
      "Epoch 625/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0471 - accuracy: 0.9900\n",
      "Epoch 00625: val_loss did not improve from 0.08745\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0503 - accuracy: 0.9893 - val_loss: 0.0934 - val_accuracy: 0.9752\n",
      "Epoch 626/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0560 - accuracy: 0.9840\n",
      "Epoch 00626: val_loss did not improve from 0.08745\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0519 - accuracy: 0.9862 - val_loss: 0.0887 - val_accuracy: 0.9720\n",
      "Epoch 627/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0567 - accuracy: 0.9860\n",
      "Epoch 00627: val_loss did not improve from 0.08745\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0511 - accuracy: 0.9862 - val_loss: 0.0893 - val_accuracy: 0.9689\n",
      "Epoch 628/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0613 - accuracy: 0.9820\n",
      "Epoch 00628: val_loss improved from 0.08745 to 0.08655, saving model to ./model\\628-0.0865.hdf5\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0524 - accuracy: 0.9847 - val_loss: 0.0865 - val_accuracy: 0.9783\n",
      "Epoch 629/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0515 - accuracy: 0.9920\n",
      "Epoch 00629: val_loss did not improve from 0.08655\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0502 - accuracy: 0.9893 - val_loss: 0.0888 - val_accuracy: 0.9783\n",
      "Epoch 630/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0556 - accuracy: 0.9840\n",
      "Epoch 00630: val_loss did not improve from 0.08655\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0517 - accuracy: 0.9862 - val_loss: 0.0892 - val_accuracy: 0.9689\n",
      "Epoch 631/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0376 - accuracy: 0.9940\n",
      "Epoch 00631: val_loss did not improve from 0.08655\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0528 - accuracy: 0.9877 - val_loss: 0.0928 - val_accuracy: 0.9689\n",
      "Epoch 632/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0602 - accuracy: 0.9860\n",
      "Epoch 00632: val_loss did not improve from 0.08655\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0520 - accuracy: 0.9877 - val_loss: 0.0947 - val_accuracy: 0.9720\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 633/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0478 - accuracy: 0.9900\n",
      "Epoch 00633: val_loss did not improve from 0.08655\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0520 - accuracy: 0.9893 - val_loss: 0.0946 - val_accuracy: 0.9720\n",
      "Epoch 634/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0455 - accuracy: 0.9920\n",
      "Epoch 00634: val_loss did not improve from 0.08655\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0509 - accuracy: 0.9877 - val_loss: 0.0974 - val_accuracy: 0.9689\n",
      "Epoch 635/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0435 - accuracy: 0.9840\n",
      "Epoch 00635: val_loss did not improve from 0.08655\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0503 - accuracy: 0.9847 - val_loss: 0.0984 - val_accuracy: 0.9720\n",
      "Epoch 636/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0577 - accuracy: 0.9860\n",
      "Epoch 00636: val_loss did not improve from 0.08655\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0505 - accuracy: 0.9877 - val_loss: 0.1001 - val_accuracy: 0.9752\n",
      "Epoch 637/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0484 - accuracy: 0.9860\n",
      "Epoch 00637: val_loss did not improve from 0.08655\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0512 - accuracy: 0.9862 - val_loss: 0.0994 - val_accuracy: 0.9720\n",
      "Epoch 638/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0549 - accuracy: 0.9880\n",
      "Epoch 00638: val_loss did not improve from 0.08655\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0502 - accuracy: 0.9862 - val_loss: 0.0974 - val_accuracy: 0.9720\n",
      "Epoch 639/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0489 - accuracy: 0.9860\n",
      "Epoch 00639: val_loss did not improve from 0.08655\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0502 - accuracy: 0.9877 - val_loss: 0.0936 - val_accuracy: 0.9720\n",
      "Epoch 640/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0434 - accuracy: 0.9880\n",
      "Epoch 00640: val_loss did not improve from 0.08655\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0499 - accuracy: 0.9877 - val_loss: 0.0913 - val_accuracy: 0.9752\n",
      "Epoch 641/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0523 - accuracy: 0.9880\n",
      "Epoch 00641: val_loss did not improve from 0.08655\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0499 - accuracy: 0.9877 - val_loss: 0.0893 - val_accuracy: 0.9689\n",
      "Epoch 642/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0507 - accuracy: 0.9880\n",
      "Epoch 00642: val_loss did not improve from 0.08655\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0512 - accuracy: 0.9862 - val_loss: 0.0899 - val_accuracy: 0.9689\n",
      "Epoch 643/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0542 - accuracy: 0.9860\n",
      "Epoch 00643: val_loss did not improve from 0.08655\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0517 - accuracy: 0.9862 - val_loss: 0.0875 - val_accuracy: 0.9783\n",
      "Epoch 644/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0511 - accuracy: 0.9900\n",
      "Epoch 00644: val_loss did not improve from 0.08655\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0522 - accuracy: 0.9862 - val_loss: 0.0882 - val_accuracy: 0.9783\n",
      "Epoch 645/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0491 - accuracy: 0.9900\n",
      "Epoch 00645: val_loss did not improve from 0.08655\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0510 - accuracy: 0.9862 - val_loss: 0.0905 - val_accuracy: 0.9689\n",
      "Epoch 646/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0441 - accuracy: 0.9880\n",
      "Epoch 00646: val_loss did not improve from 0.08655\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0510 - accuracy: 0.9893 - val_loss: 0.0909 - val_accuracy: 0.9689\n",
      "Epoch 647/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0589 - accuracy: 0.9860\n",
      "Epoch 00647: val_loss did not improve from 0.08655\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0501 - accuracy: 0.9893 - val_loss: 0.0926 - val_accuracy: 0.9689\n",
      "Epoch 648/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0587 - accuracy: 0.9860\n",
      "Epoch 00648: val_loss did not improve from 0.08655\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0496 - accuracy: 0.9893 - val_loss: 0.0960 - val_accuracy: 0.9689\n",
      "Epoch 649/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0446 - accuracy: 0.9860\n",
      "Epoch 00649: val_loss did not improve from 0.08655\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0507 - accuracy: 0.9862 - val_loss: 0.0972 - val_accuracy: 0.9689\n",
      "Epoch 650/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0510 - accuracy: 0.9900\n",
      "Epoch 00650: val_loss did not improve from 0.08655\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0504 - accuracy: 0.9877 - val_loss: 0.0988 - val_accuracy: 0.9752\n",
      "Epoch 651/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0573 - accuracy: 0.9860\n",
      "Epoch 00651: val_loss did not improve from 0.08655\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0504 - accuracy: 0.9877 - val_loss: 0.0992 - val_accuracy: 0.9720\n",
      "Epoch 652/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0510 - accuracy: 0.9920\n",
      "Epoch 00652: val_loss did not improve from 0.08655\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0495 - accuracy: 0.9908 - val_loss: 0.0979 - val_accuracy: 0.9720\n",
      "Epoch 653/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0576 - accuracy: 0.9880\n",
      "Epoch 00653: val_loss did not improve from 0.08655\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0495 - accuracy: 0.9908 - val_loss: 0.0953 - val_accuracy: 0.9720\n",
      "Epoch 654/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0565 - accuracy: 0.9880\n",
      "Epoch 00654: val_loss did not improve from 0.08655\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0494 - accuracy: 0.9908 - val_loss: 0.0920 - val_accuracy: 0.9689\n",
      "Epoch 655/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0550 - accuracy: 0.9880\n",
      "Epoch 00655: val_loss did not improve from 0.08655\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0495 - accuracy: 0.9908 - val_loss: 0.0884 - val_accuracy: 0.9752\n",
      "Epoch 656/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0432 - accuracy: 0.9920\n",
      "Epoch 00656: val_loss did not improve from 0.08655\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0493 - accuracy: 0.9908 - val_loss: 0.0871 - val_accuracy: 0.9783\n",
      "Epoch 657/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0394 - accuracy: 0.9940\n",
      "Epoch 00657: val_loss improved from 0.08655 to 0.08634, saving model to ./model\\657-0.0863.hdf5\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.0497 - accuracy: 0.9908 - val_loss: 0.0863 - val_accuracy: 0.9783\n",
      "Epoch 658/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0496 - accuracy: 0.9940\n",
      "Epoch 00658: val_loss did not improve from 0.08634\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0493 - accuracy: 0.9908 - val_loss: 0.0867 - val_accuracy: 0.9752\n",
      "Epoch 659/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0382 - accuracy: 0.9940\n",
      "Epoch 00659: val_loss did not improve from 0.08634\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0504 - accuracy: 0.9877 - val_loss: 0.0884 - val_accuracy: 0.9783\n",
      "Epoch 660/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0454 - accuracy: 0.9880\n",
      "Epoch 00660: val_loss did not improve from 0.08634\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0494 - accuracy: 0.9893 - val_loss: 0.0937 - val_accuracy: 0.9814\n",
      "Epoch 661/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0626 - accuracy: 0.9840\n",
      "Epoch 00661: val_loss did not improve from 0.08634\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0518 - accuracy: 0.9877 - val_loss: 0.0930 - val_accuracy: 0.9689\n",
      "Epoch 662/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0560 - accuracy: 0.9880\n",
      "Epoch 00662: val_loss did not improve from 0.08634\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0493 - accuracy: 0.9893 - val_loss: 0.0982 - val_accuracy: 0.9689\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 663/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0634 - accuracy: 0.9820\n",
      "Epoch 00663: val_loss did not improve from 0.08634\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0516 - accuracy: 0.9862 - val_loss: 0.0974 - val_accuracy: 0.9720\n",
      "Epoch 664/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0553 - accuracy: 0.9900\n",
      "Epoch 00664: val_loss did not improve from 0.08634\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0493 - accuracy: 0.9908 - val_loss: 0.0985 - val_accuracy: 0.9720\n",
      "Epoch 665/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0527 - accuracy: 0.9900\n",
      "Epoch 00665: val_loss did not improve from 0.08634\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0488 - accuracy: 0.9908 - val_loss: 0.1002 - val_accuracy: 0.9689\n",
      "Epoch 666/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0437 - accuracy: 0.9900\n",
      "Epoch 00666: val_loss did not improve from 0.08634\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0504 - accuracy: 0.9877 - val_loss: 0.0982 - val_accuracy: 0.9720\n",
      "Epoch 667/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0412 - accuracy: 0.9940\n",
      "Epoch 00667: val_loss did not improve from 0.08634\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0483 - accuracy: 0.9893 - val_loss: 0.1013 - val_accuracy: 0.9752\n",
      "Epoch 668/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0606 - accuracy: 0.9860\n",
      "Epoch 00668: val_loss did not improve from 0.08634\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0527 - accuracy: 0.9893 - val_loss: 0.0966 - val_accuracy: 0.9720\n",
      "Epoch 669/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0572 - accuracy: 0.9880\n",
      "Epoch 00669: val_loss did not improve from 0.08634\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0478 - accuracy: 0.9908 - val_loss: 0.1067 - val_accuracy: 0.9627\n",
      "Epoch 670/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0688 - accuracy: 0.9780\n",
      "Epoch 00670: val_loss did not improve from 0.08634\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0569 - accuracy: 0.9832 - val_loss: 0.1003 - val_accuracy: 0.9689\n",
      "Epoch 671/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0593 - accuracy: 0.9860\n",
      "Epoch 00671: val_loss did not improve from 0.08634\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0522 - accuracy: 0.9877 - val_loss: 0.0930 - val_accuracy: 0.9752\n",
      "Epoch 672/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0571 - accuracy: 0.9860\n",
      "Epoch 00672: val_loss did not improve from 0.08634\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0506 - accuracy: 0.9877 - val_loss: 0.0940 - val_accuracy: 0.9783\n",
      "Epoch 673/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0521 - accuracy: 0.9900\n",
      "Epoch 00673: val_loss did not improve from 0.08634\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0500 - accuracy: 0.9893 - val_loss: 0.0930 - val_accuracy: 0.9689\n",
      "Epoch 674/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0619 - accuracy: 0.9840\n",
      "Epoch 00674: val_loss did not improve from 0.08634\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0500 - accuracy: 0.9877 - val_loss: 0.0948 - val_accuracy: 0.9689\n",
      "Epoch 675/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0577 - accuracy: 0.9860\n",
      "Epoch 00675: val_loss did not improve from 0.08634\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0516 - accuracy: 0.9862 - val_loss: 0.0896 - val_accuracy: 0.9720\n",
      "Epoch 676/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0551 - accuracy: 0.9880\n",
      "Epoch 00676: val_loss did not improve from 0.08634\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0492 - accuracy: 0.9893 - val_loss: 0.0921 - val_accuracy: 0.9783\n",
      "Epoch 677/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0573 - accuracy: 0.9880\n",
      "Epoch 00677: val_loss did not improve from 0.08634\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0495 - accuracy: 0.9877 - val_loss: 0.0915 - val_accuracy: 0.9689\n",
      "Epoch 678/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0551 - accuracy: 0.9900\n",
      "Epoch 00678: val_loss did not improve from 0.08634\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0498 - accuracy: 0.9893 - val_loss: 0.0929 - val_accuracy: 0.9689\n",
      "Epoch 679/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0422 - accuracy: 0.9900\n",
      "Epoch 00679: val_loss did not improve from 0.08634\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0488 - accuracy: 0.9877 - val_loss: 0.0937 - val_accuracy: 0.9783\n",
      "Epoch 680/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0440 - accuracy: 0.9920\n",
      "Epoch 00680: val_loss did not improve from 0.08634\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0503 - accuracy: 0.9893 - val_loss: 0.0939 - val_accuracy: 0.9720\n",
      "Epoch 681/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0509 - accuracy: 0.9880\n",
      "Epoch 00681: val_loss did not improve from 0.08634\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0484 - accuracy: 0.9862 - val_loss: 0.0966 - val_accuracy: 0.9689\n",
      "Epoch 682/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0566 - accuracy: 0.9860\n",
      "Epoch 00682: val_loss did not improve from 0.08634\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0502 - accuracy: 0.9877 - val_loss: 0.0981 - val_accuracy: 0.9689\n",
      "Epoch 683/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0420 - accuracy: 0.9860\n",
      "Epoch 00683: val_loss did not improve from 0.08634\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0503 - accuracy: 0.9847 - val_loss: 0.0949 - val_accuracy: 0.9720\n",
      "Epoch 684/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0443 - accuracy: 0.9900\n",
      "Epoch 00684: val_loss did not improve from 0.08634\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0500 - accuracy: 0.9893 - val_loss: 0.1035 - val_accuracy: 0.9752\n",
      "Epoch 685/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0542 - accuracy: 0.9840\n",
      "Epoch 00685: val_loss did not improve from 0.08634\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0572 - accuracy: 0.9847 - val_loss: 0.0979 - val_accuracy: 0.9689\n",
      "Epoch 686/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0573 - accuracy: 0.9900\n",
      "Epoch 00686: val_loss did not improve from 0.08634\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0483 - accuracy: 0.9908 - val_loss: 0.1042 - val_accuracy: 0.9689\n",
      "Epoch 687/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0431 - accuracy: 0.9860\n",
      "Epoch 00687: val_loss did not improve from 0.08634\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0511 - accuracy: 0.9847 - val_loss: 0.0993 - val_accuracy: 0.9720\n",
      "Epoch 688/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0534 - accuracy: 0.9880\n",
      "Epoch 00688: val_loss did not improve from 0.08634\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0488 - accuracy: 0.9877 - val_loss: 0.1035 - val_accuracy: 0.9720\n",
      "Epoch 689/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0558 - accuracy: 0.9880\n",
      "Epoch 00689: val_loss did not improve from 0.08634\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0527 - accuracy: 0.9877 - val_loss: 0.1044 - val_accuracy: 0.9689\n",
      "Epoch 690/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0596 - accuracy: 0.9860\n",
      "Epoch 00690: val_loss did not improve from 0.08634\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0498 - accuracy: 0.9877 - val_loss: 0.1163 - val_accuracy: 0.9565\n",
      "Epoch 691/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0521 - accuracy: 0.9840\n",
      "Epoch 00691: val_loss did not improve from 0.08634\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0576 - accuracy: 0.9832 - val_loss: 0.0977 - val_accuracy: 0.9720\n",
      "Epoch 692/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0512 - accuracy: 0.9900\n",
      "Epoch 00692: val_loss did not improve from 0.08634\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0514 - accuracy: 0.9893 - val_loss: 0.0953 - val_accuracy: 0.9752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 693/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0493 - accuracy: 0.9880\n",
      "Epoch 00693: val_loss did not improve from 0.08634\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0514 - accuracy: 0.9893 - val_loss: 0.0905 - val_accuracy: 0.9689\n",
      "Epoch 694/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0490 - accuracy: 0.9920\n",
      "Epoch 00694: val_loss did not improve from 0.08634\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0501 - accuracy: 0.9877 - val_loss: 0.0871 - val_accuracy: 0.9720\n",
      "Epoch 695/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0586 - accuracy: 0.9860\n",
      "Epoch 00695: val_loss did not improve from 0.08634\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0495 - accuracy: 0.9893 - val_loss: 0.0883 - val_accuracy: 0.9814\n",
      "Epoch 696/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0402 - accuracy: 0.9900\n",
      "Epoch 00696: val_loss improved from 0.08634 to 0.08571, saving model to ./model\\696-0.0857.hdf5\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0493 - accuracy: 0.9893 - val_loss: 0.0857 - val_accuracy: 0.9752\n",
      "Epoch 697/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0553 - accuracy: 0.9880\n",
      "Epoch 00697: val_loss did not improve from 0.08571\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0486 - accuracy: 0.9893 - val_loss: 0.0879 - val_accuracy: 0.9689\n",
      "Epoch 698/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0417 - accuracy: 0.9880\n",
      "Epoch 00698: val_loss did not improve from 0.08571\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0496 - accuracy: 0.9877 - val_loss: 0.0871 - val_accuracy: 0.9752\n",
      "Epoch 699/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0341 - accuracy: 0.9880\n",
      "Epoch 00699: val_loss did not improve from 0.08571\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0473 - accuracy: 0.9877 - val_loss: 0.0954 - val_accuracy: 0.9814\n",
      "Epoch 700/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0580 - accuracy: 0.9860\n",
      "Epoch 00700: val_loss did not improve from 0.08571\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0532 - accuracy: 0.9877 - val_loss: 0.0907 - val_accuracy: 0.9720\n",
      "Epoch 701/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0562 - accuracy: 0.9860\n",
      "Epoch 00701: val_loss did not improve from 0.08571\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0483 - accuracy: 0.9877 - val_loss: 0.1014 - val_accuracy: 0.9658\n",
      "Epoch 702/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0663 - accuracy: 0.9820\n",
      "Epoch 00702: val_loss did not improve from 0.08571\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0552 - accuracy: 0.9847 - val_loss: 0.0904 - val_accuracy: 0.9689\n",
      "Epoch 703/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0475 - accuracy: 0.9920\n",
      "Epoch 00703: val_loss did not improve from 0.08571\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0476 - accuracy: 0.9877 - val_loss: 0.0926 - val_accuracy: 0.9814\n",
      "Epoch 704/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0451 - accuracy: 0.9900\n",
      "Epoch 00704: val_loss did not improve from 0.08571\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0511 - accuracy: 0.9893 - val_loss: 0.0896 - val_accuracy: 0.9783\n",
      "Epoch 705/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0398 - accuracy: 0.9920\n",
      "Epoch 00705: val_loss did not improve from 0.08571\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0489 - accuracy: 0.9877 - val_loss: 0.0887 - val_accuracy: 0.9752\n",
      "Epoch 706/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0535 - accuracy: 0.9880\n",
      "Epoch 00706: val_loss did not improve from 0.08571\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0471 - accuracy: 0.9893 - val_loss: 0.0901 - val_accuracy: 0.9752\n",
      "Epoch 707/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0404 - accuracy: 0.9900\n",
      "Epoch 00707: val_loss did not improve from 0.08571\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0471 - accuracy: 0.9893 - val_loss: 0.0921 - val_accuracy: 0.9752\n",
      "Epoch 708/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0522 - accuracy: 0.9900\n",
      "Epoch 00708: val_loss did not improve from 0.08571\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0476 - accuracy: 0.9908 - val_loss: 0.0932 - val_accuracy: 0.9752\n",
      "Epoch 709/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0291 - accuracy: 0.9940\n",
      "Epoch 00709: val_loss did not improve from 0.08571\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0484 - accuracy: 0.9923 - val_loss: 0.0939 - val_accuracy: 0.9720\n",
      "Epoch 710/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0404 - accuracy: 0.9900\n",
      "Epoch 00710: val_loss did not improve from 0.08571\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0473 - accuracy: 0.9908 - val_loss: 0.0951 - val_accuracy: 0.9752\n",
      "Epoch 711/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0454 - accuracy: 0.9920\n",
      "Epoch 00711: val_loss did not improve from 0.08571\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0480 - accuracy: 0.9893 - val_loss: 0.0950 - val_accuracy: 0.9689\n",
      "Epoch 712/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0396 - accuracy: 0.9940\n",
      "Epoch 00712: val_loss did not improve from 0.08571\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0484 - accuracy: 0.9908 - val_loss: 0.0956 - val_accuracy: 0.9689\n",
      "Epoch 713/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0509 - accuracy: 0.9920\n",
      "Epoch 00713: val_loss did not improve from 0.08571\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0469 - accuracy: 0.9908 - val_loss: 0.0964 - val_accuracy: 0.9752\n",
      "Epoch 714/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0445 - accuracy: 0.9900\n",
      "Epoch 00714: val_loss did not improve from 0.08571\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0489 - accuracy: 0.9893 - val_loss: 0.0953 - val_accuracy: 0.9720\n",
      "Epoch 715/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0535 - accuracy: 0.9880\n",
      "Epoch 00715: val_loss did not improve from 0.08571\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0470 - accuracy: 0.9893 - val_loss: 0.0965 - val_accuracy: 0.9689\n",
      "Epoch 716/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0574 - accuracy: 0.9860\n",
      "Epoch 00716: val_loss did not improve from 0.08571\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0480 - accuracy: 0.9893 - val_loss: 0.0929 - val_accuracy: 0.9689\n",
      "Epoch 717/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0293 - accuracy: 0.9940\n",
      "Epoch 00717: val_loss did not improve from 0.08571\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0475 - accuracy: 0.9908 - val_loss: 0.0910 - val_accuracy: 0.9845\n",
      "Epoch 718/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0524 - accuracy: 0.9880\n",
      "Epoch 00718: val_loss did not improve from 0.08571\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0512 - accuracy: 0.9893 - val_loss: 0.0891 - val_accuracy: 0.9845\n",
      "Epoch 719/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0483 - accuracy: 0.9900\n",
      "Epoch 00719: val_loss did not improve from 0.08571\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0480 - accuracy: 0.9877 - val_loss: 0.0915 - val_accuracy: 0.9689\n",
      "Epoch 720/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0552 - accuracy: 0.9860\n",
      "Epoch 00720: val_loss did not improve from 0.08571\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0508 - accuracy: 0.9862 - val_loss: 0.0904 - val_accuracy: 0.9689\n",
      "Epoch 721/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0587 - accuracy: 0.9840\n",
      "Epoch 00721: val_loss did not improve from 0.08571\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0496 - accuracy: 0.9877 - val_loss: 0.0883 - val_accuracy: 0.9752\n",
      "Epoch 722/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0495 - accuracy: 0.9860\n",
      "Epoch 00722: val_loss did not improve from 0.08571\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0482 - accuracy: 0.9877 - val_loss: 0.0887 - val_accuracy: 0.9720\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 723/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0392 - accuracy: 0.9900\n",
      "Epoch 00723: val_loss did not improve from 0.08571\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0469 - accuracy: 0.9877 - val_loss: 0.0934 - val_accuracy: 0.9689\n",
      "Epoch 724/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0419 - accuracy: 0.9880\n",
      "Epoch 00724: val_loss did not improve from 0.08571\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0493 - accuracy: 0.9862 - val_loss: 0.0905 - val_accuracy: 0.9720\n",
      "Epoch 725/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0525 - accuracy: 0.9880\n",
      "Epoch 00725: val_loss did not improve from 0.08571\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0471 - accuracy: 0.9908 - val_loss: 0.0941 - val_accuracy: 0.9814\n",
      "Epoch 726/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0455 - accuracy: 0.9920\n",
      "Epoch 00726: val_loss did not improve from 0.08571\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0490 - accuracy: 0.9877 - val_loss: 0.0926 - val_accuracy: 0.9720\n",
      "Epoch 727/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0528 - accuracy: 0.9880\n",
      "Epoch 00727: val_loss did not improve from 0.08571\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0462 - accuracy: 0.9908 - val_loss: 0.0961 - val_accuracy: 0.9689\n",
      "Epoch 728/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0396 - accuracy: 0.9920\n",
      "Epoch 00728: val_loss did not improve from 0.08571\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0489 - accuracy: 0.9893 - val_loss: 0.0941 - val_accuracy: 0.9720\n",
      "Epoch 729/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0492 - accuracy: 0.9920\n",
      "Epoch 00729: val_loss did not improve from 0.08571\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0483 - accuracy: 0.9908 - val_loss: 0.0967 - val_accuracy: 0.9752\n",
      "Epoch 730/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0544 - accuracy: 0.9900\n",
      "Epoch 00730: val_loss did not improve from 0.08571\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0476 - accuracy: 0.9893 - val_loss: 0.0991 - val_accuracy: 0.9689\n",
      "Epoch 731/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0516 - accuracy: 0.9880\n",
      "Epoch 00731: val_loss did not improve from 0.08571\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0495 - accuracy: 0.9877 - val_loss: 0.1016 - val_accuracy: 0.9689\n",
      "Epoch 732/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0565 - accuracy: 0.9820\n",
      "Epoch 00732: val_loss did not improve from 0.08571\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0501 - accuracy: 0.9847 - val_loss: 0.0956 - val_accuracy: 0.9720\n",
      "Epoch 733/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0410 - accuracy: 0.9900\n",
      "Epoch 00733: val_loss did not improve from 0.08571\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0479 - accuracy: 0.9877 - val_loss: 0.0962 - val_accuracy: 0.9720\n",
      "Epoch 734/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0429 - accuracy: 0.9900\n",
      "Epoch 00734: val_loss did not improve from 0.08571\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0482 - accuracy: 0.9893 - val_loss: 0.0962 - val_accuracy: 0.9720\n",
      "Epoch 735/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0255 - accuracy: 0.9940\n",
      "Epoch 00735: val_loss did not improve from 0.08571\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0479 - accuracy: 0.9893 - val_loss: 0.0958 - val_accuracy: 0.9720\n",
      "Epoch 736/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0441 - accuracy: 0.9880\n",
      "Epoch 00736: val_loss did not improve from 0.08571\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0477 - accuracy: 0.9893 - val_loss: 0.1005 - val_accuracy: 0.9783\n",
      "Epoch 737/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0587 - accuracy: 0.9860\n",
      "Epoch 00737: val_loss did not improve from 0.08571\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0507 - accuracy: 0.9862 - val_loss: 0.0969 - val_accuracy: 0.9689\n",
      "Epoch 738/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0522 - accuracy: 0.9900\n",
      "Epoch 00738: val_loss did not improve from 0.08571\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0475 - accuracy: 0.9893 - val_loss: 0.1039 - val_accuracy: 0.9658\n",
      "Epoch 739/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0585 - accuracy: 0.9840\n",
      "Epoch 00739: val_loss did not improve from 0.08571\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0529 - accuracy: 0.9847 - val_loss: 0.0930 - val_accuracy: 0.9720\n",
      "Epoch 740/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0347 - accuracy: 0.9920\n",
      "Epoch 00740: val_loss did not improve from 0.08571\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0469 - accuracy: 0.9862 - val_loss: 0.0947 - val_accuracy: 0.9845\n",
      "Epoch 741/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0547 - accuracy: 0.9860\n",
      "Epoch 00741: val_loss did not improve from 0.08571\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0489 - accuracy: 0.9877 - val_loss: 0.0901 - val_accuracy: 0.9689\n",
      "Epoch 742/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0530 - accuracy: 0.9900\n",
      "Epoch 00742: val_loss did not improve from 0.08571\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0466 - accuracy: 0.9908 - val_loss: 0.0966 - val_accuracy: 0.9689\n",
      "Epoch 743/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0660 - accuracy: 0.9820\n",
      "Epoch 00743: val_loss did not improve from 0.08571\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0528 - accuracy: 0.9862 - val_loss: 0.0909 - val_accuracy: 0.9689\n",
      "Epoch 744/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0301 - accuracy: 0.9920\n",
      "Epoch 00744: val_loss did not improve from 0.08571\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0461 - accuracy: 0.9908 - val_loss: 0.0942 - val_accuracy: 0.9814\n",
      "Epoch 745/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0544 - accuracy: 0.9860\n",
      "Epoch 00745: val_loss did not improve from 0.08571\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0531 - accuracy: 0.9877 - val_loss: 0.0910 - val_accuracy: 0.9752\n",
      "Epoch 746/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0308 - accuracy: 0.9900\n",
      "Epoch 00746: val_loss did not improve from 0.08571\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0509 - accuracy: 0.9877 - val_loss: 0.1039 - val_accuracy: 0.9658\n",
      "Epoch 747/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0642 - accuracy: 0.9820\n",
      "Epoch 00747: val_loss did not improve from 0.08571\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0538 - accuracy: 0.9847 - val_loss: 0.0950 - val_accuracy: 0.9752\n",
      "Epoch 748/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0392 - accuracy: 0.9900\n",
      "Epoch 00748: val_loss did not improve from 0.08571\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0468 - accuracy: 0.9877 - val_loss: 0.1009 - val_accuracy: 0.9752\n",
      "Epoch 749/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0523 - accuracy: 0.9900\n",
      "Epoch 00749: val_loss did not improve from 0.08571\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0496 - accuracy: 0.9877 - val_loss: 0.0994 - val_accuracy: 0.9720\n",
      "Epoch 750/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0436 - accuracy: 0.9880\n",
      "Epoch 00750: val_loss did not improve from 0.08571\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0479 - accuracy: 0.9877 - val_loss: 0.1066 - val_accuracy: 0.9658\n",
      "Epoch 751/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0368 - accuracy: 0.9920\n",
      "Epoch 00751: val_loss did not improve from 0.08571\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0500 - accuracy: 0.9847 - val_loss: 0.0984 - val_accuracy: 0.9752\n",
      "Epoch 752/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0534 - accuracy: 0.9860\n",
      "Epoch 00752: val_loss did not improve from 0.08571\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0495 - accuracy: 0.9893 - val_loss: 0.0985 - val_accuracy: 0.9814\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 753/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0558 - accuracy: 0.9880\n",
      "Epoch 00753: val_loss did not improve from 0.08571\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0480 - accuracy: 0.9893 - val_loss: 0.1009 - val_accuracy: 0.9658\n",
      "Epoch 754/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0611 - accuracy: 0.9840\n",
      "Epoch 00754: val_loss did not improve from 0.08571\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0535 - accuracy: 0.9832 - val_loss: 0.1017 - val_accuracy: 0.9658\n",
      "Epoch 755/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0634 - accuracy: 0.9820\n",
      "Epoch 00755: val_loss did not improve from 0.08571\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0547 - accuracy: 0.9862 - val_loss: 0.0868 - val_accuracy: 0.9814\n",
      "Epoch 756/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0508 - accuracy: 0.9880\n",
      "Epoch 00756: val_loss did not improve from 0.08571\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0478 - accuracy: 0.9893 - val_loss: 0.0873 - val_accuracy: 0.9814\n",
      "Epoch 757/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0522 - accuracy: 0.9880\n",
      "Epoch 00757: val_loss did not improve from 0.08571\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0470 - accuracy: 0.9877 - val_loss: 0.0869 - val_accuracy: 0.9720\n",
      "Epoch 758/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0570 - accuracy: 0.9860\n",
      "Epoch 00758: val_loss did not improve from 0.08571\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0486 - accuracy: 0.9893 - val_loss: 0.0896 - val_accuracy: 0.9689\n",
      "Epoch 759/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0463 - accuracy: 0.9860\n",
      "Epoch 00759: val_loss improved from 0.08571 to 0.08505, saving model to ./model\\759-0.0851.hdf5\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.0504 - accuracy: 0.9877 - val_loss: 0.0851 - val_accuracy: 0.9814\n",
      "Epoch 760/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0537 - accuracy: 0.9880\n",
      "Epoch 00760: val_loss did not improve from 0.08505\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0476 - accuracy: 0.9908 - val_loss: 0.0920 - val_accuracy: 0.9845\n",
      "Epoch 761/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0533 - accuracy: 0.9900\n",
      "Epoch 00761: val_loss did not improve from 0.08505\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0484 - accuracy: 0.9893 - val_loss: 0.0921 - val_accuracy: 0.9689\n",
      "Epoch 762/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0311 - accuracy: 0.9900\n",
      "Epoch 00762: val_loss did not improve from 0.08505\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0491 - accuracy: 0.9893 - val_loss: 0.1007 - val_accuracy: 0.9658\n",
      "Epoch 763/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0449 - accuracy: 0.9860\n",
      "Epoch 00763: val_loss did not improve from 0.08505\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0508 - accuracy: 0.9847 - val_loss: 0.0945 - val_accuracy: 0.9752\n",
      "Epoch 764/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0516 - accuracy: 0.9880\n",
      "Epoch 00764: val_loss did not improve from 0.08505\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0519 - accuracy: 0.9862 - val_loss: 0.0988 - val_accuracy: 0.9783\n",
      "Epoch 765/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0509 - accuracy: 0.9880\n",
      "Epoch 00765: val_loss did not improve from 0.08505\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0482 - accuracy: 0.9877 - val_loss: 0.1028 - val_accuracy: 0.9689\n",
      "Epoch 766/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0590 - accuracy: 0.9840\n",
      "Epoch 00766: val_loss did not improve from 0.08505\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0502 - accuracy: 0.9847 - val_loss: 0.1008 - val_accuracy: 0.9689\n",
      "Epoch 767/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0508 - accuracy: 0.9880\n",
      "Epoch 00767: val_loss did not improve from 0.08505\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0483 - accuracy: 0.9877 - val_loss: 0.0938 - val_accuracy: 0.9752\n",
      "Epoch 768/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0381 - accuracy: 0.9940\n",
      "Epoch 00768: val_loss did not improve from 0.08505\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0481 - accuracy: 0.9908 - val_loss: 0.0972 - val_accuracy: 0.9845\n",
      "Epoch 769/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0547 - accuracy: 0.9880\n",
      "Epoch 00769: val_loss did not improve from 0.08505\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0491 - accuracy: 0.9893 - val_loss: 0.0966 - val_accuracy: 0.9689\n",
      "Epoch 770/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0549 - accuracy: 0.9860\n",
      "Epoch 00770: val_loss did not improve from 0.08505\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0501 - accuracy: 0.9847 - val_loss: 0.1055 - val_accuracy: 0.9627\n",
      "Epoch 771/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0488 - accuracy: 0.9820\n",
      "Epoch 00771: val_loss did not improve from 0.08505\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0563 - accuracy: 0.9816 - val_loss: 0.0863 - val_accuracy: 0.9752\n",
      "Epoch 772/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0481 - accuracy: 0.9920\n",
      "Epoch 00772: val_loss did not improve from 0.08505\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.0466 - accuracy: 0.9893 - val_loss: 0.0965 - val_accuracy: 0.9783\n",
      "Epoch 773/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0627 - accuracy: 0.9880\n",
      "Epoch 00773: val_loss did not improve from 0.08505\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0546 - accuracy: 0.9893 - val_loss: 0.0860 - val_accuracy: 0.9720\n",
      "Epoch 774/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0306 - accuracy: 0.9920\n",
      "Epoch 00774: val_loss did not improve from 0.08505\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0500 - accuracy: 0.9908 - val_loss: 0.0978 - val_accuracy: 0.9658\n",
      "Epoch 775/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0421 - accuracy: 0.9860\n",
      "Epoch 00775: val_loss did not improve from 0.08505\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0543 - accuracy: 0.9847 - val_loss: 0.0866 - val_accuracy: 0.9783\n",
      "Epoch 776/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0442 - accuracy: 0.9900\n",
      "Epoch 00776: val_loss did not improve from 0.08505\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0483 - accuracy: 0.9893 - val_loss: 0.1022 - val_accuracy: 0.9752\n",
      "Epoch 777/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0663 - accuracy: 0.9800\n",
      "Epoch 00777: val_loss did not improve from 0.08505\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0566 - accuracy: 0.9832 - val_loss: 0.0941 - val_accuracy: 0.9689\n",
      "Epoch 778/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0424 - accuracy: 0.9860\n",
      "Epoch 00778: val_loss did not improve from 0.08505\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0500 - accuracy: 0.9862 - val_loss: 0.1091 - val_accuracy: 0.9658\n",
      "Epoch 779/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0657 - accuracy: 0.9840\n",
      "Epoch 00779: val_loss did not improve from 0.08505\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0562 - accuracy: 0.9862 - val_loss: 0.0955 - val_accuracy: 0.9720\n",
      "Epoch 780/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0434 - accuracy: 0.9880\n",
      "Epoch 00780: val_loss did not improve from 0.08505\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0463 - accuracy: 0.9893 - val_loss: 0.1021 - val_accuracy: 0.9720\n",
      "Epoch 781/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0469 - accuracy: 0.9880\n",
      "Epoch 00781: val_loss did not improve from 0.08505\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0517 - accuracy: 0.9877 - val_loss: 0.0967 - val_accuracy: 0.9752\n",
      "Epoch 782/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0523 - accuracy: 0.9880\n",
      "Epoch 00782: val_loss did not improve from 0.08505\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0454 - accuracy: 0.9893 - val_loss: 0.1025 - val_accuracy: 0.9689\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 783/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0576 - accuracy: 0.9840\n",
      "Epoch 00783: val_loss did not improve from 0.08505\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0504 - accuracy: 0.9847 - val_loss: 0.0940 - val_accuracy: 0.9689\n",
      "Epoch 784/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0543 - accuracy: 0.9880\n",
      "Epoch 00784: val_loss did not improve from 0.08505\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0481 - accuracy: 0.9877 - val_loss: 0.0876 - val_accuracy: 0.9814\n",
      "Epoch 785/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0405 - accuracy: 0.9920\n",
      "Epoch 00785: val_loss did not improve from 0.08505\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0457 - accuracy: 0.9923 - val_loss: 0.0851 - val_accuracy: 0.9814\n",
      "Epoch 786/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0338 - accuracy: 0.9940\n",
      "Epoch 00786: val_loss improved from 0.08505 to 0.08245, saving model to ./model\\786-0.0824.hdf5\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.0453 - accuracy: 0.9923 - val_loss: 0.0824 - val_accuracy: 0.9720\n",
      "Epoch 787/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0478 - accuracy: 0.9920\n",
      "Epoch 00787: val_loss did not improve from 0.08245\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0457 - accuracy: 0.9908 - val_loss: 0.0835 - val_accuracy: 0.9720\n",
      "Epoch 788/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0420 - accuracy: 0.9880\n",
      "Epoch 00788: val_loss improved from 0.08245 to 0.08104, saving model to ./model\\788-0.0810.hdf5\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.0476 - accuracy: 0.9893 - val_loss: 0.0810 - val_accuracy: 0.9814\n",
      "Epoch 789/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0470 - accuracy: 0.9920\n",
      "Epoch 00789: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0458 - accuracy: 0.9923 - val_loss: 0.0816 - val_accuracy: 0.9814\n",
      "Epoch 790/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0385 - accuracy: 0.9920\n",
      "Epoch 00790: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0460 - accuracy: 0.9908 - val_loss: 0.0820 - val_accuracy: 0.9814\n",
      "Epoch 791/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0392 - accuracy: 0.9900\n",
      "Epoch 00791: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0454 - accuracy: 0.9893 - val_loss: 0.0839 - val_accuracy: 0.9814\n",
      "Epoch 792/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0537 - accuracy: 0.9860\n",
      "Epoch 00792: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0453 - accuracy: 0.9893 - val_loss: 0.0867 - val_accuracy: 0.9720\n",
      "Epoch 793/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0533 - accuracy: 0.9880\n",
      "Epoch 00793: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0456 - accuracy: 0.9893 - val_loss: 0.0903 - val_accuracy: 0.9720\n",
      "Epoch 794/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0329 - accuracy: 0.9940\n",
      "Epoch 00794: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0451 - accuracy: 0.9893 - val_loss: 0.0942 - val_accuracy: 0.9783\n",
      "Epoch 795/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0554 - accuracy: 0.9880\n",
      "Epoch 00795: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0481 - accuracy: 0.9908 - val_loss: 0.0957 - val_accuracy: 0.9783\n",
      "Epoch 796/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0519 - accuracy: 0.9920\n",
      "Epoch 00796: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0447 - accuracy: 0.9923 - val_loss: 0.1017 - val_accuracy: 0.9689\n",
      "Epoch 797/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0538 - accuracy: 0.9820\n",
      "Epoch 00797: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0495 - accuracy: 0.9832 - val_loss: 0.1104 - val_accuracy: 0.9658\n",
      "Epoch 798/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0662 - accuracy: 0.9800\n",
      "Epoch 00798: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0556 - accuracy: 0.9832 - val_loss: 0.0961 - val_accuracy: 0.9689\n",
      "Epoch 799/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0434 - accuracy: 0.9900\n",
      "Epoch 00799: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0467 - accuracy: 0.9908 - val_loss: 0.0973 - val_accuracy: 0.9845\n",
      "Epoch 800/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0404 - accuracy: 0.9920\n",
      "Epoch 00800: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0487 - accuracy: 0.9893 - val_loss: 0.0932 - val_accuracy: 0.9720\n",
      "Epoch 801/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0353 - accuracy: 0.9940\n",
      "Epoch 00801: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0463 - accuracy: 0.9908 - val_loss: 0.0936 - val_accuracy: 0.9720\n",
      "Epoch 802/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0524 - accuracy: 0.9880\n",
      "Epoch 00802: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0453 - accuracy: 0.9893 - val_loss: 0.0933 - val_accuracy: 0.9720\n",
      "Epoch 803/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0485 - accuracy: 0.9900\n",
      "Epoch 00803: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0451 - accuracy: 0.9893 - val_loss: 0.0935 - val_accuracy: 0.9720\n",
      "Epoch 804/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0492 - accuracy: 0.9900\n",
      "Epoch 00804: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0450 - accuracy: 0.9893 - val_loss: 0.0944 - val_accuracy: 0.9720\n",
      "Epoch 805/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0541 - accuracy: 0.9860\n",
      "Epoch 00805: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0461 - accuracy: 0.9877 - val_loss: 0.0921 - val_accuracy: 0.9752\n",
      "Epoch 806/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0489 - accuracy: 0.9900\n",
      "Epoch 00806: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0449 - accuracy: 0.9893 - val_loss: 0.0902 - val_accuracy: 0.9752\n",
      "Epoch 807/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0535 - accuracy: 0.9860\n",
      "Epoch 00807: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0445 - accuracy: 0.9893 - val_loss: 0.0889 - val_accuracy: 0.9752\n",
      "Epoch 808/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0564 - accuracy: 0.9880\n",
      "Epoch 00808: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0448 - accuracy: 0.9908 - val_loss: 0.0898 - val_accuracy: 0.9689\n",
      "Epoch 809/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0207 - accuracy: 0.9940\n",
      "Epoch 00809: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0470 - accuracy: 0.9908 - val_loss: 0.0891 - val_accuracy: 0.9783\n",
      "Epoch 810/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0499 - accuracy: 0.9920\n",
      "Epoch 00810: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0453 - accuracy: 0.9923 - val_loss: 0.1002 - val_accuracy: 0.9783\n",
      "Epoch 811/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0561 - accuracy: 0.9860\n",
      "Epoch 00811: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0506 - accuracy: 0.9877 - val_loss: 0.0938 - val_accuracy: 0.9720\n",
      "Epoch 812/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0530 - accuracy: 0.9900\n",
      "Epoch 00812: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0486 - accuracy: 0.9877 - val_loss: 0.1135 - val_accuracy: 0.9627\n",
      "Epoch 813/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0486 - accuracy: 0.9820\n",
      "Epoch 00813: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0618 - accuracy: 0.9801 - val_loss: 0.0867 - val_accuracy: 0.9752\n",
      "Epoch 814/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0492 - accuracy: 0.9880\n",
      "Epoch 00814: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0523 - accuracy: 0.9847 - val_loss: 0.0975 - val_accuracy: 0.9752\n",
      "Epoch 815/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0579 - accuracy: 0.9880\n",
      "Epoch 00815: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0535 - accuracy: 0.9877 - val_loss: 0.0875 - val_accuracy: 0.9689\n",
      "Epoch 816/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0487 - accuracy: 0.9920\n",
      "Epoch 00816: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0493 - accuracy: 0.9877 - val_loss: 0.0877 - val_accuracy: 0.9689\n",
      "Epoch 817/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0400 - accuracy: 0.9920\n",
      "Epoch 00817: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0458 - accuracy: 0.9908 - val_loss: 0.0911 - val_accuracy: 0.9845\n",
      "Epoch 818/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0543 - accuracy: 0.9880\n",
      "Epoch 00818: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0499 - accuracy: 0.9908 - val_loss: 0.0895 - val_accuracy: 0.9814\n",
      "Epoch 819/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0407 - accuracy: 0.9920\n",
      "Epoch 00819: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0456 - accuracy: 0.9923 - val_loss: 0.0944 - val_accuracy: 0.9689\n",
      "Epoch 820/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0537 - accuracy: 0.9860\n",
      "Epoch 00820: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0493 - accuracy: 0.9862 - val_loss: 0.0921 - val_accuracy: 0.9689\n",
      "Epoch 821/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0459 - accuracy: 0.9920\n",
      "Epoch 00821: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0453 - accuracy: 0.9908 - val_loss: 0.0945 - val_accuracy: 0.9814\n",
      "Epoch 822/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0535 - accuracy: 0.9880\n",
      "Epoch 00822: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0475 - accuracy: 0.9908 - val_loss: 0.0911 - val_accuracy: 0.9752\n",
      "Epoch 823/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0530 - accuracy: 0.9900\n",
      "Epoch 00823: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0433 - accuracy: 0.9923 - val_loss: 0.0998 - val_accuracy: 0.9658\n",
      "Epoch 824/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0422 - accuracy: 0.9860\n",
      "Epoch 00824: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0516 - accuracy: 0.9847 - val_loss: 0.0938 - val_accuracy: 0.9689\n",
      "Epoch 825/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0539 - accuracy: 0.9860\n",
      "Epoch 00825: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0455 - accuracy: 0.9893 - val_loss: 0.0980 - val_accuracy: 0.9845\n",
      "Epoch 826/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0521 - accuracy: 0.9900\n",
      "Epoch 00826: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0500 - accuracy: 0.9908 - val_loss: 0.0929 - val_accuracy: 0.9752\n",
      "Epoch 827/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0488 - accuracy: 0.9920\n",
      "Epoch 00827: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0429 - accuracy: 0.9923 - val_loss: 0.1001 - val_accuracy: 0.9689\n",
      "Epoch 828/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0451 - accuracy: 0.9860\n",
      "Epoch 00828: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0517 - accuracy: 0.9847 - val_loss: 0.0981 - val_accuracy: 0.9689\n",
      "Epoch 829/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0571 - accuracy: 0.9840\n",
      "Epoch 00829: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0485 - accuracy: 0.9877 - val_loss: 0.0946 - val_accuracy: 0.9814\n",
      "Epoch 830/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0526 - accuracy: 0.9880\n",
      "Epoch 00830: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0478 - accuracy: 0.9893 - val_loss: 0.0930 - val_accuracy: 0.9752\n",
      "Epoch 831/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0474 - accuracy: 0.9940\n",
      "Epoch 00831: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0432 - accuracy: 0.9939 - val_loss: 0.0989 - val_accuracy: 0.9689\n",
      "Epoch 832/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0591 - accuracy: 0.9840\n",
      "Epoch 00832: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0498 - accuracy: 0.9862 - val_loss: 0.0973 - val_accuracy: 0.9689\n",
      "Epoch 833/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0450 - accuracy: 0.9860\n",
      "Epoch 00833: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0489 - accuracy: 0.9862 - val_loss: 0.0917 - val_accuracy: 0.9752\n",
      "Epoch 834/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0507 - accuracy: 0.9900\n",
      "Epoch 00834: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0454 - accuracy: 0.9923 - val_loss: 0.0936 - val_accuracy: 0.9845\n",
      "Epoch 835/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0398 - accuracy: 0.9920\n",
      "Epoch 00835: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0454 - accuracy: 0.9923 - val_loss: 0.0923 - val_accuracy: 0.9720\n",
      "Epoch 836/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0497 - accuracy: 0.9900\n",
      "Epoch 00836: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0443 - accuracy: 0.9908 - val_loss: 0.0933 - val_accuracy: 0.9720\n",
      "Epoch 837/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0474 - accuracy: 0.9920\n",
      "Epoch 00837: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0449 - accuracy: 0.9908 - val_loss: 0.0920 - val_accuracy: 0.9752\n",
      "Epoch 838/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0466 - accuracy: 0.9920\n",
      "Epoch 00838: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0441 - accuracy: 0.9923 - val_loss: 0.0917 - val_accuracy: 0.9752\n",
      "Epoch 839/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0496 - accuracy: 0.9900\n",
      "Epoch 00839: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0433 - accuracy: 0.9923 - val_loss: 0.0921 - val_accuracy: 0.9689\n",
      "Epoch 840/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0514 - accuracy: 0.9880\n",
      "Epoch 00840: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0443 - accuracy: 0.9908 - val_loss: 0.0921 - val_accuracy: 0.9689\n",
      "Epoch 841/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0494 - accuracy: 0.9920\n",
      "Epoch 00841: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0451 - accuracy: 0.9908 - val_loss: 0.0886 - val_accuracy: 0.9752\n",
      "Epoch 842/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0203 - accuracy: 0.9980\n",
      "Epoch 00842: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0430 - accuracy: 0.9923 - val_loss: 0.0919 - val_accuracy: 0.9845\n",
      "Epoch 843/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0501 - accuracy: 0.9900\n",
      "Epoch 00843: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0468 - accuracy: 0.9908 - val_loss: 0.0896 - val_accuracy: 0.9783\n",
      "Epoch 844/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0503 - accuracy: 0.9920\n",
      "Epoch 00844: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0432 - accuracy: 0.9939 - val_loss: 0.0956 - val_accuracy: 0.9689\n",
      "Epoch 845/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0424 - accuracy: 0.9880\n",
      "Epoch 00845: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0480 - accuracy: 0.9862 - val_loss: 0.0929 - val_accuracy: 0.9689\n",
      "Epoch 846/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0318 - accuracy: 0.9940\n",
      "Epoch 00846: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0428 - accuracy: 0.9923 - val_loss: 0.0986 - val_accuracy: 0.9845\n",
      "Epoch 847/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0562 - accuracy: 0.9880\n",
      "Epoch 00847: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0497 - accuracy: 0.9893 - val_loss: 0.0951 - val_accuracy: 0.9783\n",
      "Epoch 848/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0532 - accuracy: 0.9880\n",
      "Epoch 00848: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0438 - accuracy: 0.9908 - val_loss: 0.0997 - val_accuracy: 0.9689\n",
      "Epoch 849/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0529 - accuracy: 0.9860\n",
      "Epoch 00849: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0484 - accuracy: 0.9847 - val_loss: 0.0985 - val_accuracy: 0.9689\n",
      "Epoch 850/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0370 - accuracy: 0.9900\n",
      "Epoch 00850: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0463 - accuracy: 0.9877 - val_loss: 0.0932 - val_accuracy: 0.9845\n",
      "Epoch 851/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0519 - accuracy: 0.9900\n",
      "Epoch 00851: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0484 - accuracy: 0.9908 - val_loss: 0.0948 - val_accuracy: 0.9845\n",
      "Epoch 852/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0454 - accuracy: 0.9940\n",
      "Epoch 00852: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0441 - accuracy: 0.9923 - val_loss: 0.0981 - val_accuracy: 0.9689\n",
      "Epoch 853/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0578 - accuracy: 0.9840\n",
      "Epoch 00853: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0484 - accuracy: 0.9862 - val_loss: 0.1035 - val_accuracy: 0.9658\n",
      "Epoch 854/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0416 - accuracy: 0.9880\n",
      "Epoch 00854: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0511 - accuracy: 0.9862 - val_loss: 0.0920 - val_accuracy: 0.9752\n",
      "Epoch 855/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0470 - accuracy: 0.9920\n",
      "Epoch 00855: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0444 - accuracy: 0.9908 - val_loss: 0.0959 - val_accuracy: 0.9783\n",
      "Epoch 856/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0501 - accuracy: 0.9900\n",
      "Epoch 00856: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0473 - accuracy: 0.9893 - val_loss: 0.0898 - val_accuracy: 0.9720\n",
      "Epoch 857/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0369 - accuracy: 0.9900\n",
      "Epoch 00857: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0449 - accuracy: 0.9893 - val_loss: 0.0918 - val_accuracy: 0.9689\n",
      "Epoch 858/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0369 - accuracy: 0.9900\n",
      "Epoch 00858: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0452 - accuracy: 0.9908 - val_loss: 0.0913 - val_accuracy: 0.9845\n",
      "Epoch 859/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0364 - accuracy: 0.9940\n",
      "Epoch 00859: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0454 - accuracy: 0.9893 - val_loss: 0.0994 - val_accuracy: 0.9783\n",
      "Epoch 860/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0463 - accuracy: 0.9900\n",
      "Epoch 00860: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0500 - accuracy: 0.9893 - val_loss: 0.0932 - val_accuracy: 0.9752\n",
      "Epoch 861/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0348 - accuracy: 0.9920\n",
      "Epoch 00861: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0438 - accuracy: 0.9908 - val_loss: 0.0972 - val_accuracy: 0.9720\n",
      "Epoch 862/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0384 - accuracy: 0.9940\n",
      "Epoch 00862: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0453 - accuracy: 0.9908 - val_loss: 0.0948 - val_accuracy: 0.9783\n",
      "Epoch 863/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0386 - accuracy: 0.9920\n",
      "Epoch 00863: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.0439 - accuracy: 0.9908 - val_loss: 0.0994 - val_accuracy: 0.9814\n",
      "Epoch 864/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0389 - accuracy: 0.9900\n",
      "Epoch 00864: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0471 - accuracy: 0.9893 - val_loss: 0.0937 - val_accuracy: 0.9720\n",
      "Epoch 865/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0372 - accuracy: 0.9940\n",
      "Epoch 00865: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0462 - accuracy: 0.9908 - val_loss: 0.0976 - val_accuracy: 0.9658\n",
      "Epoch 866/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0399 - accuracy: 0.9880\n",
      "Epoch 00866: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0458 - accuracy: 0.9893 - val_loss: 0.0926 - val_accuracy: 0.9783\n",
      "Epoch 867/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0409 - accuracy: 0.9920\n",
      "Epoch 00867: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0443 - accuracy: 0.9923 - val_loss: 0.0955 - val_accuracy: 0.9845\n",
      "Epoch 868/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0446 - accuracy: 0.9900\n",
      "Epoch 00868: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0481 - accuracy: 0.9893 - val_loss: 0.0916 - val_accuracy: 0.9752\n",
      "Epoch 869/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0385 - accuracy: 0.9920\n",
      "Epoch 00869: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0430 - accuracy: 0.9923 - val_loss: 0.0931 - val_accuracy: 0.9720\n",
      "Epoch 870/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0201 - accuracy: 0.9960\n",
      "Epoch 00870: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0433 - accuracy: 0.9908 - val_loss: 0.0961 - val_accuracy: 0.9783\n",
      "Epoch 871/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0517 - accuracy: 0.9880\n",
      "Epoch 00871: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0469 - accuracy: 0.9908 - val_loss: 0.0973 - val_accuracy: 0.9783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 872/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0509 - accuracy: 0.9900\n",
      "Epoch 00872: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0454 - accuracy: 0.9893 - val_loss: 0.1020 - val_accuracy: 0.9689\n",
      "Epoch 873/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0365 - accuracy: 0.9920\n",
      "Epoch 00873: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0478 - accuracy: 0.9893 - val_loss: 0.0993 - val_accuracy: 0.9720\n",
      "Epoch 874/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0342 - accuracy: 0.9940\n",
      "Epoch 00874: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0415 - accuracy: 0.9939 - val_loss: 0.1085 - val_accuracy: 0.9720\n",
      "Epoch 875/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0594 - accuracy: 0.9840\n",
      "Epoch 00875: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0545 - accuracy: 0.9877 - val_loss: 0.0992 - val_accuracy: 0.9752\n",
      "Epoch 876/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0385 - accuracy: 0.9920\n",
      "Epoch 00876: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0434 - accuracy: 0.9923 - val_loss: 0.1154 - val_accuracy: 0.9658\n",
      "Epoch 877/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0653 - accuracy: 0.9800\n",
      "Epoch 00877: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0558 - accuracy: 0.9816 - val_loss: 0.1020 - val_accuracy: 0.9689\n",
      "Epoch 878/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0534 - accuracy: 0.9840\n",
      "Epoch 00878: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0462 - accuracy: 0.9862 - val_loss: 0.0961 - val_accuracy: 0.9783\n",
      "Epoch 879/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0491 - accuracy: 0.9900\n",
      "Epoch 00879: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0494 - accuracy: 0.9893 - val_loss: 0.0910 - val_accuracy: 0.9845\n",
      "Epoch 880/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0556 - accuracy: 0.9860\n",
      "Epoch 00880: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0461 - accuracy: 0.9893 - val_loss: 0.0911 - val_accuracy: 0.9689\n",
      "Epoch 881/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0421 - accuracy: 0.9880\n",
      "Epoch 00881: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0485 - accuracy: 0.9862 - val_loss: 0.0876 - val_accuracy: 0.9720\n",
      "Epoch 882/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0554 - accuracy: 0.9880\n",
      "Epoch 00882: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0460 - accuracy: 0.9908 - val_loss: 0.0893 - val_accuracy: 0.9845\n",
      "Epoch 883/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0294 - accuracy: 0.9920\n",
      "Epoch 00883: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0469 - accuracy: 0.9877 - val_loss: 0.0884 - val_accuracy: 0.9845\n",
      "Epoch 884/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0322 - accuracy: 0.9920\n",
      "Epoch 00884: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0462 - accuracy: 0.9908 - val_loss: 0.0863 - val_accuracy: 0.9814\n",
      "Epoch 885/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0349 - accuracy: 0.9940\n",
      "Epoch 00885: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0434 - accuracy: 0.9908 - val_loss: 0.0863 - val_accuracy: 0.9845\n",
      "Epoch 886/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0251 - accuracy: 0.9940\n",
      "Epoch 00886: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0430 - accuracy: 0.9923 - val_loss: 0.0884 - val_accuracy: 0.9845\n",
      "Epoch 887/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0492 - accuracy: 0.9900\n",
      "Epoch 00887: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0438 - accuracy: 0.9923 - val_loss: 0.0897 - val_accuracy: 0.9845\n",
      "Epoch 888/2000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0517 - accuracy: 0.9900\n",
      "Epoch 00888: val_loss did not improve from 0.08104\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0429 - accuracy: 0.9923 - val_loss: 0.0913 - val_accuracy: 0.9720\n"
     ]
    }
   ],
   "source": [
    "# 모델 실행 및 저장\n",
    "history = model.fit(X, Y, validation_split=0.33, epochs=2000, batch_size=500, callbacks=[early_stopping_callback, checkpointer])# 0.33만큼 빼고(테스트), 3500번 돌릴 것\n",
    "\n",
    "# 이전에는 과적합을 피하기 위해서 학습 : 테스트 = 7 :3 과 같이  X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3) 구문을 사용했으나,\n",
    "# 이제는 model.fit에 validation_split=0.33 구문을 넣어 동일한 효과를 보았다.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2851e2a2488>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.989876925945282,\n",
       "  0.4381166994571686,\n",
       "  0.4676666557788849,\n",
       "  0.5112243890762329,\n",
       "  0.5183298587799072,\n",
       "  0.49564996361732483,\n",
       "  0.45226073265075684,\n",
       "  0.40209653973579407,\n",
       "  0.35193589329719543,\n",
       "  0.32810625433921814,\n",
       "  0.334952712059021,\n",
       "  0.34164246916770935,\n",
       "  0.3145049810409546,\n",
       "  0.2849816381931305,\n",
       "  0.2726966142654419,\n",
       "  0.26868483424186707,\n",
       "  0.2651596963405609,\n",
       "  0.2576981782913208,\n",
       "  0.24760125577449799,\n",
       "  0.23925119638442993,\n",
       "  0.2330113798379898,\n",
       "  0.22998346388339996,\n",
       "  0.22651658952236176,\n",
       "  0.2215442955493927,\n",
       "  0.2165757715702057,\n",
       "  0.21313884854316711,\n",
       "  0.20985674858093262,\n",
       "  0.2060505747795105,\n",
       "  0.2021523118019104,\n",
       "  0.19832095503807068,\n",
       "  0.19512848556041718,\n",
       "  0.19224786758422852,\n",
       "  0.18943502008914948,\n",
       "  0.18693731725215912,\n",
       "  0.1840212494134903,\n",
       "  0.18145263195037842,\n",
       "  0.1790817230939865,\n",
       "  0.1765209436416626,\n",
       "  0.17415574193000793,\n",
       "  0.1718628704547882,\n",
       "  0.1700594425201416,\n",
       "  0.16818055510520935,\n",
       "  0.16682042181491852,\n",
       "  0.16551880538463593,\n",
       "  0.16435743868350983,\n",
       "  0.1636853963136673,\n",
       "  0.16324439644813538,\n",
       "  0.16232937574386597,\n",
       "  0.16103044152259827,\n",
       "  0.1600550413131714,\n",
       "  0.15842370688915253,\n",
       "  0.1606048047542572,\n",
       "  0.15812981128692627,\n",
       "  0.15549278259277344,\n",
       "  0.15389882028102875,\n",
       "  0.15405189990997314,\n",
       "  0.15462014079093933,\n",
       "  0.1537000983953476,\n",
       "  0.1521608531475067,\n",
       "  0.15178662538528442,\n",
       "  0.1517084836959839,\n",
       "  0.15098515152931213,\n",
       "  0.15024104714393616,\n",
       "  0.14964497089385986,\n",
       "  0.1492566168308258,\n",
       "  0.14908602833747864,\n",
       "  0.14882522821426392,\n",
       "  0.14819803833961487,\n",
       "  0.14783936738967896,\n",
       "  0.14754489064216614,\n",
       "  0.14723467826843262,\n",
       "  0.1467812955379486,\n",
       "  0.14600728452205658,\n",
       "  0.14659272134304047,\n",
       "  0.14666473865509033,\n",
       "  0.14553795754909515,\n",
       "  0.14485138654708862,\n",
       "  0.14438726007938385,\n",
       "  0.1443118155002594,\n",
       "  0.14437717199325562,\n",
       "  0.14415797591209412,\n",
       "  0.14355595409870148,\n",
       "  0.14296449720859528,\n",
       "  0.142957404255867,\n",
       "  0.14290155470371246,\n",
       "  0.14221404492855072,\n",
       "  0.14132975041866302,\n",
       "  0.14158634841442108,\n",
       "  0.14131687581539154,\n",
       "  0.140455424785614,\n",
       "  0.1401709020137787,\n",
       "  0.14093181490898132,\n",
       "  0.14065870642662048,\n",
       "  0.13952600955963135,\n",
       "  0.1390887200832367,\n",
       "  0.13886059820652008,\n",
       "  0.138673797249794,\n",
       "  0.1382794976234436,\n",
       "  0.13799521327018738,\n",
       "  0.1376921385526657,\n",
       "  0.13746877014636993,\n",
       "  0.13738097250461578,\n",
       "  0.13709776103496552,\n",
       "  0.13652251660823822,\n",
       "  0.13670781254768372,\n",
       "  0.1367064118385315,\n",
       "  0.13597674667835236,\n",
       "  0.13518114387989044,\n",
       "  0.13497814536094666,\n",
       "  0.13525041937828064,\n",
       "  0.1347026526927948,\n",
       "  0.1338391751050949,\n",
       "  0.13473355770111084,\n",
       "  0.13463255763053894,\n",
       "  0.1336527317762375,\n",
       "  0.13285599648952484,\n",
       "  0.13242654502391815,\n",
       "  0.13226236402988434,\n",
       "  0.1320832520723343,\n",
       "  0.13213001191616058,\n",
       "  0.1318877786397934,\n",
       "  0.13108587265014648,\n",
       "  0.13049262762069702,\n",
       "  0.1314673125743866,\n",
       "  0.13197363913059235,\n",
       "  0.13109946250915527,\n",
       "  0.12985849380493164,\n",
       "  0.1309911012649536,\n",
       "  0.1299470067024231,\n",
       "  0.12848412990570068,\n",
       "  0.1286860555410385,\n",
       "  0.12919414043426514,\n",
       "  0.12873610854148865,\n",
       "  0.12784065306186676,\n",
       "  0.12708474695682526,\n",
       "  0.12712807953357697,\n",
       "  0.12708790600299835,\n",
       "  0.12650944292545319,\n",
       "  0.1266689896583557,\n",
       "  0.12600311636924744,\n",
       "  0.12560725212097168,\n",
       "  0.12775827944278717,\n",
       "  0.12662869691848755,\n",
       "  0.12476110458374023,\n",
       "  0.12511153519153595,\n",
       "  0.12505260109901428,\n",
       "  0.1243293359875679,\n",
       "  0.12405166774988174,\n",
       "  0.1241224929690361,\n",
       "  0.12358081340789795,\n",
       "  0.12384908646345139,\n",
       "  0.12338779121637344,\n",
       "  0.12281423062086105,\n",
       "  0.12230869382619858,\n",
       "  0.12211945652961731,\n",
       "  0.12190892547369003,\n",
       "  0.12106093019247055,\n",
       "  0.12073823064565659,\n",
       "  0.12110325694084167,\n",
       "  0.12072698771953583,\n",
       "  0.12023362517356873,\n",
       "  0.1208048164844513,\n",
       "  0.12041349709033966,\n",
       "  0.12010307610034943,\n",
       "  0.12035472691059113,\n",
       "  0.12046075612306595,\n",
       "  0.1191665530204773,\n",
       "  0.11870293319225311,\n",
       "  0.1190517246723175,\n",
       "  0.11802136152982712,\n",
       "  0.11878124624490738,\n",
       "  0.11985970288515091,\n",
       "  0.11771489679813385,\n",
       "  0.11817479133605957,\n",
       "  0.11915487796068192,\n",
       "  0.11738504469394684,\n",
       "  0.11581682413816452,\n",
       "  0.11809860169887543,\n",
       "  0.12082875519990921,\n",
       "  0.11875631660223007,\n",
       "  0.11521237343549728,\n",
       "  0.1168864518404007,\n",
       "  0.11673590540885925,\n",
       "  0.11438954621553421,\n",
       "  0.1147177517414093,\n",
       "  0.11605649441480637,\n",
       "  0.11519516259431839,\n",
       "  0.11409720778465271,\n",
       "  0.1136607825756073,\n",
       "  0.11348970979452133,\n",
       "  0.11328551173210144,\n",
       "  0.11313319206237793,\n",
       "  0.11262035369873047,\n",
       "  0.11241640895605087,\n",
       "  0.111923448741436,\n",
       "  0.11252040416002274,\n",
       "  0.11183006316423416,\n",
       "  0.11154137551784515,\n",
       "  0.11177559196949005,\n",
       "  0.11089875549077988,\n",
       "  0.11173807829618454,\n",
       "  0.11096663773059845,\n",
       "  0.10978161543607712,\n",
       "  0.11082661896944046,\n",
       "  0.11113283038139343,\n",
       "  0.10958375036716461,\n",
       "  0.1092117577791214,\n",
       "  0.10986889153718948,\n",
       "  0.10885307192802429,\n",
       "  0.10852312296628952,\n",
       "  0.11024465411901474,\n",
       "  0.10995615273714066,\n",
       "  0.10878647118806839,\n",
       "  0.10747241973876953,\n",
       "  0.10730548202991486,\n",
       "  0.10728252679109573,\n",
       "  0.10668182373046875,\n",
       "  0.10626629739999771,\n",
       "  0.10600640624761581,\n",
       "  0.1062738448381424,\n",
       "  0.10670417547225952,\n",
       "  0.10552305728197098,\n",
       "  0.10485367476940155,\n",
       "  0.10701733827590942,\n",
       "  0.10357386618852615,\n",
       "  0.10580310225486755,\n",
       "  0.1068277433514595,\n",
       "  0.10289765894412994,\n",
       "  0.1022527888417244,\n",
       "  0.10230600833892822,\n",
       "  0.10196202248334885,\n",
       "  0.10121265798807144,\n",
       "  0.10062234103679657,\n",
       "  0.10063570737838745,\n",
       "  0.10023151338100433,\n",
       "  0.09931894391775131,\n",
       "  0.10048119723796844,\n",
       "  0.09909456968307495,\n",
       "  0.10002443194389343,\n",
       "  0.10060299187898636,\n",
       "  0.09840843826532364,\n",
       "  0.09793554991483688,\n",
       "  0.09775007516145706,\n",
       "  0.0976288765668869,\n",
       "  0.09686914831399918,\n",
       "  0.09873371571302414,\n",
       "  0.09659259766340256,\n",
       "  0.09781364351511002,\n",
       "  0.09916191548109055,\n",
       "  0.09716594964265823,\n",
       "  0.09748507291078568,\n",
       "  0.09564650803804398,\n",
       "  0.09634971618652344,\n",
       "  0.09487106651067734,\n",
       "  0.09455414116382599,\n",
       "  0.09645241498947144,\n",
       "  0.09481458365917206,\n",
       "  0.09803511202335358,\n",
       "  0.09838112443685532,\n",
       "  0.0966135561466217,\n",
       "  0.09537003189325333,\n",
       "  0.09635785222053528,\n",
       "  0.10071767866611481,\n",
       "  0.09261518716812134,\n",
       "  0.09715158492326736,\n",
       "  0.09285227209329605,\n",
       "  0.09095766395330429,\n",
       "  0.09231747686862946,\n",
       "  0.09128893166780472,\n",
       "  0.09316487610340118,\n",
       "  0.09239297360181808,\n",
       "  0.09279566258192062,\n",
       "  0.09094558656215668,\n",
       "  0.08946456760168076,\n",
       "  0.09005977213382721,\n",
       "  0.08965833485126495,\n",
       "  0.08912167698144913,\n",
       "  0.08933380991220474,\n",
       "  0.0880785658955574,\n",
       "  0.08876233547925949,\n",
       "  0.0881086140871048,\n",
       "  0.08758513629436493,\n",
       "  0.0877683013677597,\n",
       "  0.08688802272081375,\n",
       "  0.08763136714696884,\n",
       "  0.08660167455673218,\n",
       "  0.09006236493587494,\n",
       "  0.08577907085418701,\n",
       "  0.09213604778051376,\n",
       "  0.08783639222383499,\n",
       "  0.08642098307609558,\n",
       "  0.08575371652841568,\n",
       "  0.08646971732378006,\n",
       "  0.08521097153425217,\n",
       "  0.08479331433773041,\n",
       "  0.08446387201547623,\n",
       "  0.08393774926662445,\n",
       "  0.08398309350013733,\n",
       "  0.08424315601587296,\n",
       "  0.08353257924318314,\n",
       "  0.08426882326602936,\n",
       "  0.08455061167478561,\n",
       "  0.08323119580745697,\n",
       "  0.09048580378293991,\n",
       "  0.08352478593587875,\n",
       "  0.09188801050186157,\n",
       "  0.08655478060245514,\n",
       "  0.08929301053285599,\n",
       "  0.08324577659368515,\n",
       "  0.09163334220647812,\n",
       "  0.08466892689466476,\n",
       "  0.08912714570760727,\n",
       "  0.08342286944389343,\n",
       "  0.09678126126527786,\n",
       "  0.08336486667394638,\n",
       "  0.08887118846178055,\n",
       "  0.08420612663030624,\n",
       "  0.08427770435810089,\n",
       "  0.08932364732027054,\n",
       "  0.08104705065488815,\n",
       "  0.08125177770853043,\n",
       "  0.08010230213403702,\n",
       "  0.08260376751422882,\n",
       "  0.08117354661226273,\n",
       "  0.08209436386823654,\n",
       "  0.080421581864357,\n",
       "  0.07956576347351074,\n",
       "  0.07971446216106415,\n",
       "  0.07890702039003372,\n",
       "  0.07978922128677368,\n",
       "  0.07869428396224976,\n",
       "  0.07914190739393234,\n",
       "  0.07804188132286072,\n",
       "  0.08201181888580322,\n",
       "  0.07825655490159988,\n",
       "  0.07861088216304779,\n",
       "  0.07843606173992157,\n",
       "  0.07682233303785324,\n",
       "  0.07720562070608139,\n",
       "  0.07677076011896133,\n",
       "  0.0771724060177803,\n",
       "  0.07638140022754669,\n",
       "  0.07764874398708344,\n",
       "  0.07733658701181412,\n",
       "  0.07731452584266663,\n",
       "  0.08035528659820557,\n",
       "  0.07561227679252625,\n",
       "  0.07972945272922516,\n",
       "  0.07568810880184174,\n",
       "  0.08048583567142487,\n",
       "  0.07608018070459366,\n",
       "  0.0777682289481163,\n",
       "  0.07488125562667847,\n",
       "  0.07542160898447037,\n",
       "  0.07426763325929642,\n",
       "  0.07471926510334015,\n",
       "  0.07429971545934677,\n",
       "  0.07421328127384186,\n",
       "  0.0735766813158989,\n",
       "  0.07354409992694855,\n",
       "  0.07502061128616333,\n",
       "  0.07431180775165558,\n",
       "  0.07375781983137131,\n",
       "  0.07472332566976547,\n",
       "  0.07399243861436844,\n",
       "  0.07433377951383591,\n",
       "  0.07468994706869125,\n",
       "  0.07263156771659851,\n",
       "  0.07276995480060577,\n",
       "  0.07218676060438156,\n",
       "  0.07198234647512436,\n",
       "  0.07184933871030807,\n",
       "  0.07164600491523743,\n",
       "  0.07346951216459274,\n",
       "  0.07478011399507523,\n",
       "  0.07305538654327393,\n",
       "  0.07125785201787949,\n",
       "  0.07222127169370651,\n",
       "  0.07074940949678421,\n",
       "  0.07311026751995087,\n",
       "  0.07010407745838165,\n",
       "  0.07449928671121597,\n",
       "  0.0743517354130745,\n",
       "  0.07035709172487259,\n",
       "  0.07036527991294861,\n",
       "  0.06959101557731628,\n",
       "  0.07009650766849518,\n",
       "  0.07144460082054138,\n",
       "  0.07010163366794586,\n",
       "  0.06958025693893433,\n",
       "  0.06976066529750824,\n",
       "  0.06883799284696579,\n",
       "  0.06874708831310272,\n",
       "  0.06862512230873108,\n",
       "  0.06862454861402512,\n",
       "  0.06845489144325256,\n",
       "  0.06891472637653351,\n",
       "  0.06760743260383606,\n",
       "  0.07134732604026794,\n",
       "  0.071044921875,\n",
       "  0.06831487268209457,\n",
       "  0.06813137233257294,\n",
       "  0.06906381994485855,\n",
       "  0.06707288324832916,\n",
       "  0.07219900190830231,\n",
       "  0.0668911412358284,\n",
       "  0.07190798968076706,\n",
       "  0.06653093546628952,\n",
       "  0.07168648391962051,\n",
       "  0.06804299354553223,\n",
       "  0.06923825293779373,\n",
       "  0.06804075837135315,\n",
       "  0.0699799507856369,\n",
       "  0.0668470710515976,\n",
       "  0.07066372036933899,\n",
       "  0.06672797352075577,\n",
       "  0.06764892488718033,\n",
       "  0.06605252623558044,\n",
       "  0.06865519285202026,\n",
       "  0.06733589619398117,\n",
       "  0.06776531785726547,\n",
       "  0.06835315376520157,\n",
       "  0.0662485659122467,\n",
       "  0.06579697132110596,\n",
       "  0.0653739720582962,\n",
       "  0.06604966521263123,\n",
       "  0.06522846966981888,\n",
       "  0.06602257490158081,\n",
       "  0.06515631824731827,\n",
       "  0.06695632636547089,\n",
       "  0.0632382407784462,\n",
       "  0.0728595107793808,\n",
       "  0.06544969230890274,\n",
       "  0.07159500569105148,\n",
       "  0.06533930450677872,\n",
       "  0.07016221433877945,\n",
       "  0.0708843544125557,\n",
       "  0.079971544444561,\n",
       "  0.06688462197780609,\n",
       "  0.08269868791103363,\n",
       "  0.06600193679332733,\n",
       "  0.07558965682983398,\n",
       "  0.06595468521118164,\n",
       "  0.06688590347766876,\n",
       "  0.06332926452159882,\n",
       "  0.06716016680002213,\n",
       "  0.06332249939441681,\n",
       "  0.0676243007183075,\n",
       "  0.06230039894580841,\n",
       "  0.06682419031858444,\n",
       "  0.06210402399301529,\n",
       "  0.06846024096012115,\n",
       "  0.062439579516649246,\n",
       "  0.06541574746370316,\n",
       "  0.06435056030750275,\n",
       "  0.06238916888833046,\n",
       "  0.06367839127779007,\n",
       "  0.06495422124862671,\n",
       "  0.06328700482845306,\n",
       "  0.06327734142541885,\n",
       "  0.06505952030420303,\n",
       "  0.06195155531167984,\n",
       "  0.06741481274366379,\n",
       "  0.06678594648838043,\n",
       "  0.06798405945301056,\n",
       "  0.062046341598033905,\n",
       "  0.06133410334587097,\n",
       "  0.06119404733181,\n",
       "  0.060376640409231186,\n",
       "  0.061347540467977524,\n",
       "  0.060469262301921844,\n",
       "  0.06211806461215019,\n",
       "  0.06112130358815193,\n",
       "  0.06175585463643074,\n",
       "  0.061652954667806625,\n",
       "  0.060053031891584396,\n",
       "  0.0599246472120285,\n",
       "  0.05993988737463951,\n",
       "  0.06065819784998894,\n",
       "  0.06116688996553421,\n",
       "  0.05980255827307701,\n",
       "  0.05963240563869476,\n",
       "  0.05966956913471222,\n",
       "  0.060030195862054825,\n",
       "  0.059092674404382706,\n",
       "  0.06208804249763489,\n",
       "  0.06444886326789856,\n",
       "  0.06036781147122383,\n",
       "  0.06156168878078461,\n",
       "  0.05991202965378761,\n",
       "  0.06556288152933121,\n",
       "  0.05925723910331726,\n",
       "  0.06081783398985863,\n",
       "  0.05843609198927879,\n",
       "  0.06225855275988579,\n",
       "  0.05901085585355759,\n",
       "  0.06197797507047653,\n",
       "  0.05815820395946503,\n",
       "  0.060307785868644714,\n",
       "  0.058446455746889114,\n",
       "  0.05917692556977272,\n",
       "  0.05786948278546333,\n",
       "  0.05992588400840759,\n",
       "  0.05956380441784859,\n",
       "  0.05897282436490059,\n",
       "  0.05692341551184654,\n",
       "  0.0648343414068222,\n",
       "  0.05755188688635826,\n",
       "  0.06902232021093369,\n",
       "  0.061463821679353714,\n",
       "  0.06841135770082474,\n",
       "  0.057400818914175034,\n",
       "  0.06619705259799957,\n",
       "  0.05845043435692787,\n",
       "  0.06789291650056839,\n",
       "  0.05670379847288132,\n",
       "  0.06427329033613205,\n",
       "  0.057026393711566925,\n",
       "  0.06292426586151123,\n",
       "  0.06287160515785217,\n",
       "  0.05728163570165634,\n",
       "  0.057033415883779526,\n",
       "  0.056717969477176666,\n",
       "  0.057352084666490555,\n",
       "  0.05600056052207947,\n",
       "  0.057225488126277924,\n",
       "  0.05651108920574188,\n",
       "  0.057701870799064636,\n",
       "  0.05636199563741684,\n",
       "  0.05855584889650345,\n",
       "  0.05515872314572334,\n",
       "  0.061404917389154434,\n",
       "  0.061526235193014145,\n",
       "  0.056975312530994415,\n",
       "  0.05559837073087692,\n",
       "  0.05749427154660225,\n",
       "  0.05895068496465683,\n",
       "  0.05652933195233345,\n",
       "  0.057898253202438354,\n",
       "  0.05578460171818733,\n",
       "  0.058331459760665894,\n",
       "  0.05584900081157684,\n",
       "  0.058030400425195694,\n",
       "  0.054746609181165695,\n",
       "  0.055445507168769836,\n",
       "  0.05483762174844742,\n",
       "  0.05676804482936859,\n",
       "  0.055508214980363846,\n",
       "  0.05605815723538399,\n",
       "  0.05444099381566048,\n",
       "  0.05614187568426132,\n",
       "  0.05555722862482071,\n",
       "  0.05419505760073662,\n",
       "  0.05471557751297951,\n",
       "  0.05464256554841995,\n",
       "  0.05401241034269333,\n",
       "  0.05547141656279564,\n",
       "  0.05508628487586975,\n",
       "  0.0539984256029129,\n",
       "  0.05422641709446907,\n",
       "  0.05448071286082268,\n",
       "  0.054580237716436386,\n",
       "  0.05545727536082268,\n",
       "  0.0534772053360939,\n",
       "  0.05437639355659485,\n",
       "  0.054162416607141495,\n",
       "  0.053269725292921066,\n",
       "  0.05352429673075676,\n",
       "  0.05353299528360367,\n",
       "  0.053306691348552704,\n",
       "  0.05347653105854988,\n",
       "  0.052818089723587036,\n",
       "  0.0548057034611702,\n",
       "  0.05371807515621185,\n",
       "  0.056697770953178406,\n",
       "  0.05211300030350685,\n",
       "  0.05839775875210762,\n",
       "  0.052375320345163345,\n",
       "  0.06123210862278938,\n",
       "  0.05440712720155716,\n",
       "  0.061175066977739334,\n",
       "  0.05202237144112587,\n",
       "  0.06289023160934448,\n",
       "  0.055689018219709396,\n",
       "  0.05711611732840538,\n",
       "  0.05469098314642906,\n",
       "  0.05480540543794632,\n",
       "  0.05377283692359924,\n",
       "  0.05273040384054184,\n",
       "  0.05510622262954712,\n",
       "  0.05334053933620453,\n",
       "  0.051897503435611725,\n",
       "  0.05330056697130203,\n",
       "  0.05245738476514816,\n",
       "  0.05351506173610687,\n",
       "  0.0550997368991375,\n",
       "  0.052300065755844116,\n",
       "  0.05459509417414665,\n",
       "  0.05237304046750069,\n",
       "  0.059037551283836365,\n",
       "  0.0559832938015461,\n",
       "  0.05911286920309067,\n",
       "  0.051975004374980927,\n",
       "  0.05267694965004921,\n",
       "  0.05153200402855873,\n",
       "  0.054195936769247055,\n",
       "  0.05130036547780037,\n",
       "  0.05447962507605553,\n",
       "  0.053070519119501114,\n",
       "  0.05339972674846649,\n",
       "  0.054824430495500565,\n",
       "  0.051392946392297745,\n",
       "  0.05110916867852211,\n",
       "  0.05111413821578026,\n",
       "  0.051394399255514145,\n",
       "  0.05101587250828743,\n",
       "  0.05197497084736824,\n",
       "  0.050773411989212036,\n",
       "  0.05129538103938103,\n",
       "  0.05261136591434479,\n",
       "  0.050200339406728745,\n",
       "  0.05481044575572014,\n",
       "  0.05149896815419197,\n",
       "  0.05167239531874657,\n",
       "  0.050314344465732574,\n",
       "  0.05194516479969025,\n",
       "  0.05106857791543007,\n",
       "  0.05235395208001137,\n",
       "  0.05015704035758972,\n",
       "  0.05166757479310036,\n",
       "  0.05277835950255394,\n",
       "  0.05199035257101059,\n",
       "  0.05196148157119751,\n",
       "  0.05085514858365059,\n",
       "  0.050291404128074646,\n",
       "  0.0504642091691494,\n",
       "  0.05116016045212746,\n",
       "  0.0502031110227108,\n",
       "  0.05015327408909798,\n",
       "  0.04991263151168823,\n",
       "  0.04990177974104881,\n",
       "  0.05117509141564369,\n",
       "  0.05167317017912865,\n",
       "  0.05217108502984047,\n",
       "  0.05099007114768028,\n",
       "  0.05102454870939255,\n",
       "  0.05010652914643288,\n",
       "  0.04962019622325897,\n",
       "  0.050723008811473846,\n",
       "  0.05038239061832428,\n",
       "  0.05041886121034622,\n",
       "  0.04946722462773323,\n",
       "  0.049507953226566315,\n",
       "  0.04940398782491684,\n",
       "  0.04946928471326828,\n",
       "  0.04930947348475456,\n",
       "  0.04974640905857086,\n",
       "  0.049332454800605774,\n",
       "  0.05039621517062187,\n",
       "  0.049408648163080215,\n",
       "  0.0518343523144722,\n",
       "  0.0493498332798481,\n",
       "  0.05155884101986885,\n",
       "  0.04931894317269325,\n",
       "  0.048803526908159256,\n",
       "  0.05043667554855347,\n",
       "  0.048332683742046356,\n",
       "  0.05274399742484093,\n",
       "  0.04781840369105339,\n",
       "  0.05694004148244858,\n",
       "  0.05224213749170303,\n",
       "  0.05059375613927841,\n",
       "  0.05003487318754196,\n",
       "  0.050010159611701965,\n",
       "  0.05160156264901161,\n",
       "  0.049229737371206284,\n",
       "  0.0495096780359745,\n",
       "  0.04976857826113701,\n",
       "  0.048830796033144,\n",
       "  0.050256628543138504,\n",
       "  0.048414986580610275,\n",
       "  0.05023989453911781,\n",
       "  0.05034080892801285,\n",
       "  0.049984924495220184,\n",
       "  0.057156793773174286,\n",
       "  0.048277102410793304,\n",
       "  0.05105625465512276,\n",
       "  0.048816364258527756,\n",
       "  0.05266746133565903,\n",
       "  0.04979829117655754,\n",
       "  0.057576656341552734,\n",
       "  0.051398251205682755,\n",
       "  0.051375728100538254,\n",
       "  0.050094123929739,\n",
       "  0.049455225467681885,\n",
       "  0.04934310168027878,\n",
       "  0.04856240749359131,\n",
       "  0.049612607806921005,\n",
       "  0.04732375591993332,\n",
       "  0.0531645305454731,\n",
       "  0.0483127161860466,\n",
       "  0.055156439542770386,\n",
       "  0.04757606238126755,\n",
       "  0.05111995339393616,\n",
       "  0.048882123082876205,\n",
       "  0.04711572825908661,\n",
       "  0.04708400368690491,\n",
       "  0.04764211177825928,\n",
       "  0.04836249351501465,\n",
       "  0.04732721298933029,\n",
       "  0.04800340160727501,\n",
       "  0.048437733203172684,\n",
       "  0.04691198840737343,\n",
       "  0.04890315979719162,\n",
       "  0.04703887552022934,\n",
       "  0.04799893870949745,\n",
       "  0.0474652498960495,\n",
       "  0.05117940530180931,\n",
       "  0.04802226647734642,\n",
       "  0.05076722428202629,\n",
       "  0.049600258469581604,\n",
       "  0.04815886914730072,\n",
       "  0.04688318446278572,\n",
       "  0.04927162453532219,\n",
       "  0.04705852270126343,\n",
       "  0.04899616912007332,\n",
       "  0.04616985470056534,\n",
       "  0.04890938848257065,\n",
       "  0.048328980803489685,\n",
       "  0.047575030475854874,\n",
       "  0.049472708255052567,\n",
       "  0.05009510740637779,\n",
       "  0.04789048433303833,\n",
       "  0.04824768379330635,\n",
       "  0.04790520668029785,\n",
       "  0.04774637520313263,\n",
       "  0.05073811486363411,\n",
       "  0.047542985528707504,\n",
       "  0.05289607122540474,\n",
       "  0.0469386987388134,\n",
       "  0.04888960346579552,\n",
       "  0.04662194848060608,\n",
       "  0.0528189018368721,\n",
       "  0.046126361936330795,\n",
       "  0.053088851273059845,\n",
       "  0.050878964364528656,\n",
       "  0.053793083876371384,\n",
       "  0.04684918001294136,\n",
       "  0.049570754170417786,\n",
       "  0.04786672815680504,\n",
       "  0.05003999546170235,\n",
       "  0.04952993616461754,\n",
       "  0.04797552153468132,\n",
       "  0.05353135988116264,\n",
       "  0.05466531962156296,\n",
       "  0.0478496216237545,\n",
       "  0.04697427898645401,\n",
       "  0.04856521263718605,\n",
       "  0.050381630659103394,\n",
       "  0.04761555418372154,\n",
       "  0.04839203506708145,\n",
       "  0.04912758618593216,\n",
       "  0.050821300595998764,\n",
       "  0.051879022270441055,\n",
       "  0.048161499202251434,\n",
       "  0.05023149028420448,\n",
       "  0.04829074442386627,\n",
       "  0.04808977246284485,\n",
       "  0.04906734079122543,\n",
       "  0.050088487565517426,\n",
       "  0.05634867027401924,\n",
       "  0.04658648371696472,\n",
       "  0.05464412644505501,\n",
       "  0.049958325922489166,\n",
       "  0.054303038865327835,\n",
       "  0.04832543060183525,\n",
       "  0.05658502131700516,\n",
       "  0.050022680312395096,\n",
       "  0.056219764053821564,\n",
       "  0.04631972312927246,\n",
       "  0.051732148975133896,\n",
       "  0.045388028025627136,\n",
       "  0.050441596657037735,\n",
       "  0.04812588915228844,\n",
       "  0.04573541507124901,\n",
       "  0.04534747451543808,\n",
       "  0.045651085674762726,\n",
       "  0.04756039381027222,\n",
       "  0.04584069550037384,\n",
       "  0.04595799371600151,\n",
       "  0.04537280276417732,\n",
       "  0.045257288962602615,\n",
       "  0.045563265681266785,\n",
       "  0.04511285945773125,\n",
       "  0.048120032995939255,\n",
       "  0.04469745606184006,\n",
       "  0.04950512573122978,\n",
       "  0.05560243874788284,\n",
       "  0.04669234901666641,\n",
       "  0.04870852455496788,\n",
       "  0.046266548335552216,\n",
       "  0.04528212547302246,\n",
       "  0.04514767974615097,\n",
       "  0.04501631483435631,\n",
       "  0.04614422097802162,\n",
       "  0.044910870492458344,\n",
       "  0.044519051909446716,\n",
       "  0.04475894197821617,\n",
       "  0.04701400175690651,\n",
       "  0.0453178808093071,\n",
       "  0.050603367388248444,\n",
       "  0.04857710748910904,\n",
       "  0.0617777481675148,\n",
       "  0.05231321603059769,\n",
       "  0.05351916328072548,\n",
       "  0.04929419606924057,\n",
       "  0.045830875635147095,\n",
       "  0.04990026354789734,\n",
       "  0.045575205236673355,\n",
       "  0.04931214824318886,\n",
       "  0.04533536359667778,\n",
       "  0.04752282425761223,\n",
       "  0.04331306740641594,\n",
       "  0.051577042788267136,\n",
       "  0.04548162221908569,\n",
       "  0.04995272308588028,\n",
       "  0.04293108731508255,\n",
       "  0.05166371166706085,\n",
       "  0.04845235124230385,\n",
       "  0.04775526374578476,\n",
       "  0.043159790337085724,\n",
       "  0.04979240894317627,\n",
       "  0.048856109380722046,\n",
       "  0.04535011947154999,\n",
       "  0.04541424661874771,\n",
       "  0.0443245954811573,\n",
       "  0.044879961758852005,\n",
       "  0.044060397893190384,\n",
       "  0.043285198509693146,\n",
       "  0.044310037046670914,\n",
       "  0.04509343206882477,\n",
       "  0.04301867634057999,\n",
       "  0.04684578254818916,\n",
       "  0.04316854104399681,\n",
       "  0.047994039952754974,\n",
       "  0.04275009408593178,\n",
       "  0.04969875141978264,\n",
       "  0.04381547123193741,\n",
       "  0.048444464802742004,\n",
       "  0.04631982743740082,\n",
       "  0.048437826335430145,\n",
       "  0.04406062141060829,\n",
       "  0.04844633489847183,\n",
       "  0.05108633637428284,\n",
       "  0.044350843876600266,\n",
       "  0.047261860221624374,\n",
       "  0.044866256415843964,\n",
       "  0.04521685838699341,\n",
       "  0.04540127515792847,\n",
       "  0.04997008666396141,\n",
       "  0.04375073313713074,\n",
       "  0.045266225934028625,\n",
       "  0.043873582035303116,\n",
       "  0.04712037742137909,\n",
       "  0.046152692288160324,\n",
       "  0.04583119973540306,\n",
       "  0.04426317662000656,\n",
       "  0.04813077673316002,\n",
       "  0.04301520809531212,\n",
       "  0.04330809786915779,\n",
       "  0.04692070186138153,\n",
       "  0.045427002012729645,\n",
       "  0.04783981665968895,\n",
       "  0.04148108884692192,\n",
       "  0.05452755093574524,\n",
       "  0.04336551949381828,\n",
       "  0.05577724054455757,\n",
       "  0.04624271020293236,\n",
       "  0.04942983761429787,\n",
       "  0.04614768177270889,\n",
       "  0.04846141114830971,\n",
       "  0.04602723941206932,\n",
       "  0.046898238360881805,\n",
       "  0.04615236446261406,\n",
       "  0.04341039061546326,\n",
       "  0.04300691559910774,\n",
       "  0.04379412904381752,\n",
       "  0.04293736442923546],\n",
       " 'accuracy': [0.43338438868522644,\n",
       "  0.7611026167869568,\n",
       "  0.7656967639923096,\n",
       "  0.7656967639923096,\n",
       "  0.7656967639923096,\n",
       "  0.7656967639923096,\n",
       "  0.7656967639923096,\n",
       "  0.7656967639923096,\n",
       "  0.7656967639923096,\n",
       "  0.7779479622840881,\n",
       "  0.8284839391708374,\n",
       "  0.8698315620422363,\n",
       "  0.895865261554718,\n",
       "  0.8851454854011536,\n",
       "  0.8774884939193726,\n",
       "  0.8698315620422363,\n",
       "  0.8744257092475891,\n",
       "  0.8866768479347229,\n",
       "  0.895865261554718,\n",
       "  0.9142419695854187,\n",
       "  0.9280245304107666,\n",
       "  0.9341500997543335,\n",
       "  0.9372128844261169,\n",
       "  0.9387442469596863,\n",
       "  0.9356814622879028,\n",
       "  0.9341500997543335,\n",
       "  0.9326186776161194,\n",
       "  0.9356814622879028,\n",
       "  0.9356814622879028,\n",
       "  0.9402756690979004,\n",
       "  0.9402756690979004,\n",
       "  0.9402756690979004,\n",
       "  0.9402756690979004,\n",
       "  0.9402756690979004,\n",
       "  0.9402756690979004,\n",
       "  0.9418070316314697,\n",
       "  0.9418070316314697,\n",
       "  0.9418070316314697,\n",
       "  0.9402756690979004,\n",
       "  0.9433384537696838,\n",
       "  0.9418070316314697,\n",
       "  0.9402756690979004,\n",
       "  0.9402756690979004,\n",
       "  0.9402756690979004,\n",
       "  0.9402756690979004,\n",
       "  0.9418070316314697,\n",
       "  0.9448698163032532,\n",
       "  0.9464012384414673,\n",
       "  0.9464012384414673,\n",
       "  0.9433384537696838,\n",
       "  0.9433384537696838,\n",
       "  0.9433384537696838,\n",
       "  0.9464012384414673,\n",
       "  0.9479326009750366,\n",
       "  0.9464012384414673,\n",
       "  0.9448698163032532,\n",
       "  0.9464012384414673,\n",
       "  0.9464012384414673,\n",
       "  0.9448698163032532,\n",
       "  0.9464012384414673,\n",
       "  0.9464012384414673,\n",
       "  0.9464012384414673,\n",
       "  0.9464012384414673,\n",
       "  0.9479326009750366,\n",
       "  0.9448698163032532,\n",
       "  0.9448698163032532,\n",
       "  0.9448698163032532,\n",
       "  0.9448698163032532,\n",
       "  0.9448698163032532,\n",
       "  0.9464012384414673,\n",
       "  0.9464012384414673,\n",
       "  0.9479326009750366,\n",
       "  0.9448698163032532,\n",
       "  0.9402756690979004,\n",
       "  0.9418070316314697,\n",
       "  0.9448698163032532,\n",
       "  0.9464012384414673,\n",
       "  0.9509953856468201,\n",
       "  0.9525268077850342,\n",
       "  0.9509953856468201,\n",
       "  0.9525268077850342,\n",
       "  0.9525268077850342,\n",
       "  0.9494640231132507,\n",
       "  0.9479326009750366,\n",
       "  0.9448698163032532,\n",
       "  0.9448698163032532,\n",
       "  0.9494640231132507,\n",
       "  0.9509953856468201,\n",
       "  0.9525268077850342,\n",
       "  0.9494640231132507,\n",
       "  0.9464012384414673,\n",
       "  0.9464012384414673,\n",
       "  0.9464012384414673,\n",
       "  0.9464012384414673,\n",
       "  0.9479326009750366,\n",
       "  0.9509953856468201,\n",
       "  0.9494640231132507,\n",
       "  0.9479326009750366,\n",
       "  0.9479326009750366,\n",
       "  0.9509953856468201,\n",
       "  0.9509953856468201,\n",
       "  0.9525268077850342,\n",
       "  0.9509953856468201,\n",
       "  0.9509953856468201,\n",
       "  0.9479326009750366,\n",
       "  0.9448698163032532,\n",
       "  0.9448698163032532,\n",
       "  0.9479326009750366,\n",
       "  0.9494640231132507,\n",
       "  0.9509953856468201,\n",
       "  0.9494640231132507,\n",
       "  0.9494640231132507,\n",
       "  0.9525268077850342,\n",
       "  0.9525268077850342,\n",
       "  0.9525268077850342,\n",
       "  0.9494640231132507,\n",
       "  0.9509953856468201,\n",
       "  0.9509953856468201,\n",
       "  0.9525268077850342,\n",
       "  0.9525268077850342,\n",
       "  0.9525268077850342,\n",
       "  0.9540581703186035,\n",
       "  0.9555895924568176,\n",
       "  0.9555895924568176,\n",
       "  0.9540581703186035,\n",
       "  0.957120954990387,\n",
       "  0.9555895924568176,\n",
       "  0.9540581703186035,\n",
       "  0.9555895924568176,\n",
       "  0.9586523771286011,\n",
       "  0.9555895924568176,\n",
       "  0.9586523771286011,\n",
       "  0.9586523771286011,\n",
       "  0.957120954990387,\n",
       "  0.957120954990387,\n",
       "  0.9555895924568176,\n",
       "  0.9555895924568176,\n",
       "  0.9555895924568176,\n",
       "  0.9555895924568176,\n",
       "  0.9555895924568176,\n",
       "  0.957120954990387,\n",
       "  0.957120954990387,\n",
       "  0.9586523771286011,\n",
       "  0.9586523771286011,\n",
       "  0.9555895924568176,\n",
       "  0.9586523771286011,\n",
       "  0.957120954990387,\n",
       "  0.957120954990387,\n",
       "  0.9586523771286011,\n",
       "  0.957120954990387,\n",
       "  0.9540581703186035,\n",
       "  0.9555895924568176,\n",
       "  0.957120954990387,\n",
       "  0.9586523771286011,\n",
       "  0.9586523771286011,\n",
       "  0.9601837396621704,\n",
       "  0.9601837396621704,\n",
       "  0.9586523771286011,\n",
       "  0.957120954990387,\n",
       "  0.957120954990387,\n",
       "  0.9601837396621704,\n",
       "  0.9617151618003845,\n",
       "  0.9617151618003845,\n",
       "  0.957120954990387,\n",
       "  0.9555895924568176,\n",
       "  0.957120954990387,\n",
       "  0.957120954990387,\n",
       "  0.9601837396621704,\n",
       "  0.9617151618003845,\n",
       "  0.9617151618003845,\n",
       "  0.957120954990387,\n",
       "  0.9555895924568176,\n",
       "  0.9555895924568176,\n",
       "  0.9617151618003845,\n",
       "  0.9632465839385986,\n",
       "  0.9632465839385986,\n",
       "  0.9601837396621704,\n",
       "  0.957120954990387,\n",
       "  0.9525268077850342,\n",
       "  0.957120954990387,\n",
       "  0.9601837396621704,\n",
       "  0.9663093686103821,\n",
       "  0.964777946472168,\n",
       "  0.9632465839385986,\n",
       "  0.9601837396621704,\n",
       "  0.9555895924568176,\n",
       "  0.957120954990387,\n",
       "  0.9586523771286011,\n",
       "  0.9632465839385986,\n",
       "  0.964777946472168,\n",
       "  0.964777946472168,\n",
       "  0.964777946472168,\n",
       "  0.9617151618003845,\n",
       "  0.9601837396621704,\n",
       "  0.9617151618003845,\n",
       "  0.9617151618003845,\n",
       "  0.9617151618003845,\n",
       "  0.9617151618003845,\n",
       "  0.9601837396621704,\n",
       "  0.9617151618003845,\n",
       "  0.9632465839385986,\n",
       "  0.964777946472168,\n",
       "  0.964777946472168,\n",
       "  0.9586523771286011,\n",
       "  0.9601837396621704,\n",
       "  0.9617151618003845,\n",
       "  0.964777946472168,\n",
       "  0.964777946472168,\n",
       "  0.964777946472168,\n",
       "  0.9601837396621704,\n",
       "  0.957120954990387,\n",
       "  0.957120954990387,\n",
       "  0.9632465839385986,\n",
       "  0.9663093686103821,\n",
       "  0.9632465839385986,\n",
       "  0.964777946472168,\n",
       "  0.964777946472168,\n",
       "  0.964777946472168,\n",
       "  0.9678407311439514,\n",
       "  0.9693721532821655,\n",
       "  0.9693721532821655,\n",
       "  0.9678407311439514,\n",
       "  0.9601837396621704,\n",
       "  0.9586523771286011,\n",
       "  0.9632465839385986,\n",
       "  0.9693721532821655,\n",
       "  0.9709035158157349,\n",
       "  0.9663093686103821,\n",
       "  0.9678407311439514,\n",
       "  0.9678407311439514,\n",
       "  0.9663093686103821,\n",
       "  0.964777946472168,\n",
       "  0.9663093686103821,\n",
       "  0.9693721532821655,\n",
       "  0.9678407311439514,\n",
       "  0.9678407311439514,\n",
       "  0.9601837396621704,\n",
       "  0.9678407311439514,\n",
       "  0.9678407311439514,\n",
       "  0.9693721532821655,\n",
       "  0.9663093686103821,\n",
       "  0.9678407311439514,\n",
       "  0.9678407311439514,\n",
       "  0.9678407311439514,\n",
       "  0.9663093686103821,\n",
       "  0.9617151618003845,\n",
       "  0.964777946472168,\n",
       "  0.9693721532821655,\n",
       "  0.9678407311439514,\n",
       "  0.9617151618003845,\n",
       "  0.9663093686103821,\n",
       "  0.9693721532821655,\n",
       "  0.9693721532821655,\n",
       "  0.9678407311439514,\n",
       "  0.9678407311439514,\n",
       "  0.9678407311439514,\n",
       "  0.9678407311439514,\n",
       "  0.9617151618003845,\n",
       "  0.9555895924568176,\n",
       "  0.9693721532821655,\n",
       "  0.9678407311439514,\n",
       "  0.9586523771286011,\n",
       "  0.957120954990387,\n",
       "  0.9663093686103821,\n",
       "  0.9663093686103821,\n",
       "  0.9663093686103821,\n",
       "  0.9663093686103821,\n",
       "  0.9663093686103821,\n",
       "  0.964777946472168,\n",
       "  0.964777946472168,\n",
       "  0.9632465839385986,\n",
       "  0.9632465839385986,\n",
       "  0.9663093686103821,\n",
       "  0.964777946472168,\n",
       "  0.9663093686103821,\n",
       "  0.964777946472168,\n",
       "  0.9663093686103821,\n",
       "  0.9663093686103821,\n",
       "  0.9678407311439514,\n",
       "  0.9678407311439514,\n",
       "  0.9678407311439514,\n",
       "  0.9663093686103821,\n",
       "  0.9663093686103821,\n",
       "  0.9663093686103821,\n",
       "  0.9663093686103821,\n",
       "  0.9678407311439514,\n",
       "  0.972434937953949,\n",
       "  0.9678407311439514,\n",
       "  0.9632465839385986,\n",
       "  0.9663093686103821,\n",
       "  0.9678407311439514,\n",
       "  0.9678407311439514,\n",
       "  0.9663093686103821,\n",
       "  0.9678407311439514,\n",
       "  0.9663093686103821,\n",
       "  0.964777946472168,\n",
       "  0.9663093686103821,\n",
       "  0.9678407311439514,\n",
       "  0.9678407311439514,\n",
       "  0.9678407311439514,\n",
       "  0.9693721532821655,\n",
       "  0.9693721532821655,\n",
       "  0.9678407311439514,\n",
       "  0.9709035158157349,\n",
       "  0.9709035158157349,\n",
       "  0.9617151618003845,\n",
       "  0.9663093686103821,\n",
       "  0.9709035158157349,\n",
       "  0.9678407311439514,\n",
       "  0.9632465839385986,\n",
       "  0.9709035158157349,\n",
       "  0.972434937953949,\n",
       "  0.9709035158157349,\n",
       "  0.9617151618003845,\n",
       "  0.9678407311439514,\n",
       "  0.9739663004875183,\n",
       "  0.9754977226257324,\n",
       "  0.9663093686103821,\n",
       "  0.964777946472168,\n",
       "  0.9709035158157349,\n",
       "  0.9739663004875183,\n",
       "  0.9693721532821655,\n",
       "  0.9663093686103821,\n",
       "  0.9709035158157349,\n",
       "  0.972434937953949,\n",
       "  0.9678407311439514,\n",
       "  0.9709035158157349,\n",
       "  0.972434937953949,\n",
       "  0.9739663004875183,\n",
       "  0.9693721532821655,\n",
       "  0.9693721532821655,\n",
       "  0.972434937953949,\n",
       "  0.9709035158157349,\n",
       "  0.9678407311439514,\n",
       "  0.9693721532821655,\n",
       "  0.9709035158157349,\n",
       "  0.972434937953949,\n",
       "  0.972434937953949,\n",
       "  0.9739663004875183,\n",
       "  0.9739663004875183,\n",
       "  0.9709035158157349,\n",
       "  0.9709035158157349,\n",
       "  0.9739663004875183,\n",
       "  0.9739663004875183,\n",
       "  0.9693721532821655,\n",
       "  0.9693721532821655,\n",
       "  0.9739663004875183,\n",
       "  0.9754977226257324,\n",
       "  0.972434937953949,\n",
       "  0.9693721532821655,\n",
       "  0.972434937953949,\n",
       "  0.9739663004875183,\n",
       "  0.972434937953949,\n",
       "  0.972434937953949,\n",
       "  0.9739663004875183,\n",
       "  0.9754977226257324,\n",
       "  0.9739663004875183,\n",
       "  0.9739663004875183,\n",
       "  0.9739663004875183,\n",
       "  0.9739663004875183,\n",
       "  0.972434937953949,\n",
       "  0.9770290851593018,\n",
       "  0.9770290851593018,\n",
       "  0.9770290851593018,\n",
       "  0.9754977226257324,\n",
       "  0.9770290851593018,\n",
       "  0.9770290851593018,\n",
       "  0.9770290851593018,\n",
       "  0.9754977226257324,\n",
       "  0.9770290851593018,\n",
       "  0.9754977226257324,\n",
       "  0.9754977226257324,\n",
       "  0.9739663004875183,\n",
       "  0.9754977226257324,\n",
       "  0.9770290851593018,\n",
       "  0.9739663004875183,\n",
       "  0.9770290851593018,\n",
       "  0.9739663004875183,\n",
       "  0.9754977226257324,\n",
       "  0.9785605072975159,\n",
       "  0.9770290851593018,\n",
       "  0.9785605072975159,\n",
       "  0.9754977226257324,\n",
       "  0.9754977226257324,\n",
       "  0.9754977226257324,\n",
       "  0.9754977226257324,\n",
       "  0.9785605072975159,\n",
       "  0.9770290851593018,\n",
       "  0.9800918698310852,\n",
       "  0.9770290851593018,\n",
       "  0.9770290851593018,\n",
       "  0.9754977226257324,\n",
       "  0.9754977226257324,\n",
       "  0.9770290851593018,\n",
       "  0.9785605072975159,\n",
       "  0.9800918698310852,\n",
       "  0.9770290851593018,\n",
       "  0.9785605072975159,\n",
       "  0.9800918698310852,\n",
       "  0.9785605072975159,\n",
       "  0.9770290851593018,\n",
       "  0.9785605072975159,\n",
       "  0.9800918698310852,\n",
       "  0.9770290851593018,\n",
       "  0.9770290851593018,\n",
       "  0.9785605072975159,\n",
       "  0.9800918698310852,\n",
       "  0.9800918698310852,\n",
       "  0.9754977226257324,\n",
       "  0.9770290851593018,\n",
       "  0.9800918698310852,\n",
       "  0.9785605072975159,\n",
       "  0.9770290851593018,\n",
       "  0.9785605072975159,\n",
       "  0.9800918698310852,\n",
       "  0.9816232919692993,\n",
       "  0.9785605072975159,\n",
       "  0.9754977226257324,\n",
       "  0.9800918698310852,\n",
       "  0.9816232919692993,\n",
       "  0.9785605072975159,\n",
       "  0.9754977226257324,\n",
       "  0.9785605072975159,\n",
       "  0.9770290851593018,\n",
       "  0.9754977226257324,\n",
       "  0.9800918698310852,\n",
       "  0.9800918698310852,\n",
       "  0.9785605072975159,\n",
       "  0.9785605072975159,\n",
       "  0.9816232919692993,\n",
       "  0.9816232919692993,\n",
       "  0.9754977226257324,\n",
       "  0.9816232919692993,\n",
       "  0.9800918698310852,\n",
       "  0.9800918698310852,\n",
       "  0.9785605072975159,\n",
       "  0.9770290851593018,\n",
       "  0.9709035158157349,\n",
       "  0.9770290851593018,\n",
       "  0.9709035158157349,\n",
       "  0.9800918698310852,\n",
       "  0.9739663004875183,\n",
       "  0.9831546545028687,\n",
       "  0.9770290851593018,\n",
       "  0.9770290851593018,\n",
       "  0.9816232919692993,\n",
       "  0.9785605072975159,\n",
       "  0.9754977226257324,\n",
       "  0.9816232919692993,\n",
       "  0.9816232919692993,\n",
       "  0.9831546545028687,\n",
       "  0.9770290851593018,\n",
       "  0.9800918698310852,\n",
       "  0.9816232919692993,\n",
       "  0.9816232919692993,\n",
       "  0.9831546545028687,\n",
       "  0.9816232919692993,\n",
       "  0.9831546545028687,\n",
       "  0.9785605072975159,\n",
       "  0.9770290851593018,\n",
       "  0.9816232919692993,\n",
       "  0.9800918698310852,\n",
       "  0.9770290851593018,\n",
       "  0.9831546545028687,\n",
       "  0.9816232919692993,\n",
       "  0.9800918698310852,\n",
       "  0.9831546545028687,\n",
       "  0.9831546545028687,\n",
       "  0.9846860766410828,\n",
       "  0.9816232919692993,\n",
       "  0.9831546545028687,\n",
       "  0.9816232919692993,\n",
       "  0.9816232919692993,\n",
       "  0.9785605072975159,\n",
       "  0.9816232919692993,\n",
       "  0.9800918698310852,\n",
       "  0.9816232919692993,\n",
       "  0.9816232919692993,\n",
       "  0.9816232919692993,\n",
       "  0.9816232919692993,\n",
       "  0.9816232919692993,\n",
       "  0.9816232919692993,\n",
       "  0.9816232919692993,\n",
       "  0.9816232919692993,\n",
       "  0.9816232919692993,\n",
       "  0.9816232919692993,\n",
       "  0.9831546545028687,\n",
       "  0.9816232919692993,\n",
       "  0.9831546545028687,\n",
       "  0.9816232919692993,\n",
       "  0.9831546545028687,\n",
       "  0.9816232919692993,\n",
       "  0.9785605072975159,\n",
       "  0.9846860766410828,\n",
       "  0.9831546545028687,\n",
       "  0.9831546545028687,\n",
       "  0.9770290851593018,\n",
       "  0.9831546545028687,\n",
       "  0.9816232919692993,\n",
       "  0.9831546545028687,\n",
       "  0.9831546545028687,\n",
       "  0.9831546545028687,\n",
       "  0.9816232919692993,\n",
       "  0.9816232919692993,\n",
       "  0.9816232919692993,\n",
       "  0.9846860766410828,\n",
       "  0.9831546545028687,\n",
       "  0.9846860766410828,\n",
       "  0.9785605072975159,\n",
       "  0.9846860766410828,\n",
       "  0.9816232919692993,\n",
       "  0.9846860766410828,\n",
       "  0.9800918698310852,\n",
       "  0.9831546545028687,\n",
       "  0.9831546545028687,\n",
       "  0.9862174391746521,\n",
       "  0.9816232919692993,\n",
       "  0.9846860766410828,\n",
       "  0.9831546545028687,\n",
       "  0.9831546545028687,\n",
       "  0.9846860766410828,\n",
       "  0.9862174391746521,\n",
       "  0.9831546545028687,\n",
       "  0.9831546545028687,\n",
       "  0.9862174391746521,\n",
       "  0.9846860766410828,\n",
       "  0.9877488613128662,\n",
       "  0.9846860766410828,\n",
       "  0.9846860766410828,\n",
       "  0.9846860766410828,\n",
       "  0.9892802238464355,\n",
       "  0.9846860766410828,\n",
       "  0.9846860766410828,\n",
       "  0.9892802238464355,\n",
       "  0.9862174391746521,\n",
       "  0.9831546545028687,\n",
       "  0.9846860766410828,\n",
       "  0.9846860766410828,\n",
       "  0.9846860766410828,\n",
       "  0.9846860766410828,\n",
       "  0.9846860766410828,\n",
       "  0.9862174391746521,\n",
       "  0.9846860766410828,\n",
       "  0.9862174391746521,\n",
       "  0.9846860766410828,\n",
       "  0.9846860766410828,\n",
       "  0.9877488613128662,\n",
       "  0.9862174391746521,\n",
       "  0.9831546545028687,\n",
       "  0.9877488613128662,\n",
       "  0.9862174391746521,\n",
       "  0.9877488613128662,\n",
       "  0.9862174391746521,\n",
       "  0.9846860766410828,\n",
       "  0.9846860766410828,\n",
       "  0.9862174391746521,\n",
       "  0.9862174391746521,\n",
       "  0.9862174391746521,\n",
       "  0.9862174391746521,\n",
       "  0.9892802238464355,\n",
       "  0.9877488613128662,\n",
       "  0.9862174391746521,\n",
       "  0.9831546545028687,\n",
       "  0.9877488613128662,\n",
       "  0.9862174391746521,\n",
       "  0.9862174391746521,\n",
       "  0.9877488613128662,\n",
       "  0.9877488613128662,\n",
       "  0.9877488613128662,\n",
       "  0.9877488613128662,\n",
       "  0.9862174391746521,\n",
       "  0.9877488613128662,\n",
       "  0.9846860766410828,\n",
       "  0.9862174391746521,\n",
       "  0.9831546545028687,\n",
       "  0.9877488613128662,\n",
       "  0.9862174391746521,\n",
       "  0.9862174391746521,\n",
       "  0.9831546545028687,\n",
       "  0.9831546545028687,\n",
       "  0.9846860766410828,\n",
       "  0.9846860766410828,\n",
       "  0.9831546545028687,\n",
       "  0.9831546545028687,\n",
       "  0.9846860766410828,\n",
       "  0.9831546545028687,\n",
       "  0.9846860766410828,\n",
       "  0.9846860766410828,\n",
       "  0.9862174391746521,\n",
       "  0.9831546545028687,\n",
       "  0.9846860766410828,\n",
       "  0.9862174391746521,\n",
       "  0.9846860766410828,\n",
       "  0.9846860766410828,\n",
       "  0.9846860766410828,\n",
       "  0.9862174391746521,\n",
       "  0.9846860766410828,\n",
       "  0.9831546545028687,\n",
       "  0.9877488613128662,\n",
       "  0.9877488613128662,\n",
       "  0.9862174391746521,\n",
       "  0.9831546545028687,\n",
       "  0.9862174391746521,\n",
       "  0.9877488613128662,\n",
       "  0.9862174391746521,\n",
       "  0.9831546545028687,\n",
       "  0.9862174391746521,\n",
       "  0.9846860766410828,\n",
       "  0.9846860766410828,\n",
       "  0.9846860766410828,\n",
       "  0.9846860766410828,\n",
       "  0.9862174391746521,\n",
       "  0.9877488613128662,\n",
       "  0.9877488613128662,\n",
       "  0.9892802238464355,\n",
       "  0.9877488613128662,\n",
       "  0.9877488613128662,\n",
       "  0.9862174391746521,\n",
       "  0.9877488613128662,\n",
       "  0.9892802238464355,\n",
       "  0.9892802238464355,\n",
       "  0.9862174391746521,\n",
       "  0.9846860766410828,\n",
       "  0.9831546545028687,\n",
       "  0.9892802238464355,\n",
       "  0.9862174391746521,\n",
       "  0.9862174391746521,\n",
       "  0.9846860766410828,\n",
       "  0.9892802238464355,\n",
       "  0.9862174391746521,\n",
       "  0.9877488613128662,\n",
       "  0.9877488613128662,\n",
       "  0.9892802238464355,\n",
       "  0.9877488613128662,\n",
       "  0.9846860766410828,\n",
       "  0.9877488613128662,\n",
       "  0.9862174391746521,\n",
       "  0.9862174391746521,\n",
       "  0.9877488613128662,\n",
       "  0.9877488613128662,\n",
       "  0.9877488613128662,\n",
       "  0.9862174391746521,\n",
       "  0.9862174391746521,\n",
       "  0.9862174391746521,\n",
       "  0.9862174391746521,\n",
       "  0.9892802238464355,\n",
       "  0.9892802238464355,\n",
       "  0.9892802238464355,\n",
       "  0.9862174391746521,\n",
       "  0.9877488613128662,\n",
       "  0.9877488613128662,\n",
       "  0.9908116459846497,\n",
       "  0.9908116459846497,\n",
       "  0.9908116459846497,\n",
       "  0.9908116459846497,\n",
       "  0.9908116459846497,\n",
       "  0.9908116459846497,\n",
       "  0.9908116459846497,\n",
       "  0.9877488613128662,\n",
       "  0.9892802238464355,\n",
       "  0.9877488613128662,\n",
       "  0.9892802238464355,\n",
       "  0.9862174391746521,\n",
       "  0.9908116459846497,\n",
       "  0.9908116459846497,\n",
       "  0.9877488613128662,\n",
       "  0.9892802238464355,\n",
       "  0.9892802238464355,\n",
       "  0.9908116459846497,\n",
       "  0.9831546545028687,\n",
       "  0.9877488613128662,\n",
       "  0.9877488613128662,\n",
       "  0.9892802238464355,\n",
       "  0.9877488613128662,\n",
       "  0.9862174391746521,\n",
       "  0.9892802238464355,\n",
       "  0.9877488613128662,\n",
       "  0.9892802238464355,\n",
       "  0.9877488613128662,\n",
       "  0.9892802238464355,\n",
       "  0.9862174391746521,\n",
       "  0.9877488613128662,\n",
       "  0.9846860766410828,\n",
       "  0.9892802238464355,\n",
       "  0.9846860766410828,\n",
       "  0.9908116459846497,\n",
       "  0.9846860766410828,\n",
       "  0.9877488613128662,\n",
       "  0.9877488613128662,\n",
       "  0.9877488613128662,\n",
       "  0.9831546545028687,\n",
       "  0.9892802238464355,\n",
       "  0.9892802238464355,\n",
       "  0.9877488613128662,\n",
       "  0.9892802238464355,\n",
       "  0.9892802238464355,\n",
       "  0.9892802238464355,\n",
       "  0.9877488613128662,\n",
       "  0.9877488613128662,\n",
       "  0.9877488613128662,\n",
       "  0.9877488613128662,\n",
       "  0.9846860766410828,\n",
       "  0.9877488613128662,\n",
       "  0.9892802238464355,\n",
       "  0.9877488613128662,\n",
       "  0.9892802238464355,\n",
       "  0.9892802238464355,\n",
       "  0.9908116459846497,\n",
       "  0.992343008518219,\n",
       "  0.9908116459846497,\n",
       "  0.9892802238464355,\n",
       "  0.9908116459846497,\n",
       "  0.9908116459846497,\n",
       "  0.9892802238464355,\n",
       "  0.9892802238464355,\n",
       "  0.9892802238464355,\n",
       "  0.9908116459846497,\n",
       "  0.9892802238464355,\n",
       "  0.9877488613128662,\n",
       "  0.9862174391746521,\n",
       "  0.9877488613128662,\n",
       "  0.9877488613128662,\n",
       "  0.9877488613128662,\n",
       "  0.9862174391746521,\n",
       "  0.9908116459846497,\n",
       "  0.9877488613128662,\n",
       "  0.9908116459846497,\n",
       "  0.9892802238464355,\n",
       "  0.9908116459846497,\n",
       "  0.9892802238464355,\n",
       "  0.9877488613128662,\n",
       "  0.9846860766410828,\n",
       "  0.9877488613128662,\n",
       "  0.9892802238464355,\n",
       "  0.9892802238464355,\n",
       "  0.9892802238464355,\n",
       "  0.9862174391746521,\n",
       "  0.9892802238464355,\n",
       "  0.9846860766410828,\n",
       "  0.9862174391746521,\n",
       "  0.9877488613128662,\n",
       "  0.9908116459846497,\n",
       "  0.9862174391746521,\n",
       "  0.9908116459846497,\n",
       "  0.9877488613128662,\n",
       "  0.9877488613128662,\n",
       "  0.9846860766410828,\n",
       "  0.9877488613128662,\n",
       "  0.9877488613128662,\n",
       "  0.9877488613128662,\n",
       "  0.9846860766410828,\n",
       "  0.9892802238464355,\n",
       "  0.9892802238464355,\n",
       "  0.9831546545028687,\n",
       "  0.9862174391746521,\n",
       "  0.9892802238464355,\n",
       "  0.9877488613128662,\n",
       "  0.9892802238464355,\n",
       "  0.9877488613128662,\n",
       "  0.9908116459846497,\n",
       "  0.9892802238464355,\n",
       "  0.9892802238464355,\n",
       "  0.9846860766410828,\n",
       "  0.9862174391746521,\n",
       "  0.9877488613128662,\n",
       "  0.9846860766410828,\n",
       "  0.9877488613128662,\n",
       "  0.9908116459846497,\n",
       "  0.9892802238464355,\n",
       "  0.9846860766410828,\n",
       "  0.9816232919692993,\n",
       "  0.9892802238464355,\n",
       "  0.9892802238464355,\n",
       "  0.9908116459846497,\n",
       "  0.9846860766410828,\n",
       "  0.9892802238464355,\n",
       "  0.9831546545028687,\n",
       "  0.9862174391746521,\n",
       "  0.9862174391746521,\n",
       "  0.9892802238464355,\n",
       "  0.9877488613128662,\n",
       "  0.9892802238464355,\n",
       "  0.9846860766410828,\n",
       "  0.9877488613128662,\n",
       "  0.992343008518219,\n",
       "  0.992343008518219,\n",
       "  0.9908116459846497,\n",
       "  0.9892802238464355,\n",
       "  0.992343008518219,\n",
       "  0.9908116459846497,\n",
       "  0.9892802238464355,\n",
       "  0.9892802238464355,\n",
       "  0.9892802238464355,\n",
       "  0.9892802238464355,\n",
       "  0.9908116459846497,\n",
       "  0.992343008518219,\n",
       "  0.9831546545028687,\n",
       "  0.9831546545028687,\n",
       "  0.9908116459846497,\n",
       "  0.9892802238464355,\n",
       "  0.9908116459846497,\n",
       "  0.9892802238464355,\n",
       "  0.9892802238464355,\n",
       "  0.9892802238464355,\n",
       "  0.9877488613128662,\n",
       "  0.9892802238464355,\n",
       "  0.9892802238464355,\n",
       "  0.9908116459846497,\n",
       "  0.9908116459846497,\n",
       "  0.992343008518219,\n",
       "  0.9877488613128662,\n",
       "  0.9877488613128662,\n",
       "  0.9800918698310852,\n",
       "  0.9846860766410828,\n",
       "  0.9877488613128662,\n",
       "  0.9877488613128662,\n",
       "  0.9908116459846497,\n",
       "  0.9908116459846497,\n",
       "  0.992343008518219,\n",
       "  0.9862174391746521,\n",
       "  0.9908116459846497,\n",
       "  0.9908116459846497,\n",
       "  0.992343008518219,\n",
       "  0.9846860766410828,\n",
       "  0.9892802238464355,\n",
       "  0.9908116459846497,\n",
       "  0.992343008518219,\n",
       "  0.9846860766410828,\n",
       "  0.9877488613128662,\n",
       "  0.9892802238464355,\n",
       "  0.9938744306564331,\n",
       "  0.9862174391746521,\n",
       "  0.9862174391746521,\n",
       "  0.992343008518219,\n",
       "  0.992343008518219,\n",
       "  0.9908116459846497,\n",
       "  0.9908116459846497,\n",
       "  0.992343008518219,\n",
       "  0.992343008518219,\n",
       "  0.9908116459846497,\n",
       "  0.9908116459846497,\n",
       "  0.992343008518219,\n",
       "  0.9908116459846497,\n",
       "  0.9938744306564331,\n",
       "  0.9862174391746521,\n",
       "  0.992343008518219,\n",
       "  0.9892802238464355,\n",
       "  0.9908116459846497,\n",
       "  0.9846860766410828,\n",
       "  0.9877488613128662,\n",
       "  0.9908116459846497,\n",
       "  0.992343008518219,\n",
       "  0.9862174391746521,\n",
       "  0.9862174391746521,\n",
       "  0.9908116459846497,\n",
       "  0.9892802238464355,\n",
       "  0.9892802238464355,\n",
       "  0.9908116459846497,\n",
       "  0.9892802238464355,\n",
       "  0.9892802238464355,\n",
       "  0.9908116459846497,\n",
       "  0.9908116459846497,\n",
       "  0.9908116459846497,\n",
       "  0.9892802238464355,\n",
       "  0.9908116459846497,\n",
       "  0.9892802238464355,\n",
       "  0.992343008518219,\n",
       "  0.9892802238464355,\n",
       "  0.992343008518219,\n",
       "  0.9908116459846497,\n",
       "  0.9908116459846497,\n",
       "  0.9892802238464355,\n",
       "  0.9892802238464355,\n",
       "  0.9938744306564331,\n",
       "  0.9877488613128662,\n",
       "  0.992343008518219,\n",
       "  0.9816232919692993,\n",
       "  0.9862174391746521,\n",
       "  0.9892802238464355,\n",
       "  0.9892802238464355,\n",
       "  0.9862174391746521,\n",
       "  0.9908116459846497,\n",
       "  0.9877488613128662,\n",
       "  0.9908116459846497,\n",
       "  0.9908116459846497,\n",
       "  0.992343008518219,\n",
       "  0.992343008518219,\n",
       "  0.992343008518219],\n",
       " 'val_loss': [0.4782261848449707,\n",
       "  0.5427414774894714,\n",
       "  0.6154030561447144,\n",
       "  0.6352424025535583,\n",
       "  0.6136655211448669,\n",
       "  0.5632244348526001,\n",
       "  0.4974566102027893,\n",
       "  0.43214601278305054,\n",
       "  0.3865799009799957,\n",
       "  0.3757115304470062,\n",
       "  0.3786836564540863,\n",
       "  0.3580365478992462,\n",
       "  0.3342973589897156,\n",
       "  0.3294755816459656,\n",
       "  0.33356964588165283,\n",
       "  0.33426931500434875,\n",
       "  0.32757967710494995,\n",
       "  0.3146890103816986,\n",
       "  0.30129972100257874,\n",
       "  0.2906821370124817,\n",
       "  0.2839304208755493,\n",
       "  0.2795650064945221,\n",
       "  0.2762313485145569,\n",
       "  0.2746151089668274,\n",
       "  0.2746167480945587,\n",
       "  0.2741885483264923,\n",
       "  0.2714300751686096,\n",
       "  0.2665104269981384,\n",
       "  0.2610345184803009,\n",
       "  0.25656527280807495,\n",
       "  0.25306886434555054,\n",
       "  0.2502273917198181,\n",
       "  0.2480858564376831,\n",
       "  0.2465042918920517,\n",
       "  0.24535644054412842,\n",
       "  0.24451109766960144,\n",
       "  0.24311210215091705,\n",
       "  0.24062030017375946,\n",
       "  0.23758356273174286,\n",
       "  0.23550249636173248,\n",
       "  0.23413138091564178,\n",
       "  0.2340078353881836,\n",
       "  0.23281393945217133,\n",
       "  0.2307729572057724,\n",
       "  0.22919568419456482,\n",
       "  0.22804684937000275,\n",
       "  0.22772622108459473,\n",
       "  0.2278037667274475,\n",
       "  0.22885626554489136,\n",
       "  0.23102055490016937,\n",
       "  0.23798692226409912,\n",
       "  0.2350355088710785,\n",
       "  0.22992108762264252,\n",
       "  0.22410494089126587,\n",
       "  0.22049720585346222,\n",
       "  0.21903195977210999,\n",
       "  0.21860629320144653,\n",
       "  0.21994882822036743,\n",
       "  0.22290509939193726,\n",
       "  0.2249961942434311,\n",
       "  0.22432339191436768,\n",
       "  0.2224007397890091,\n",
       "  0.22060465812683105,\n",
       "  0.21880444884300232,\n",
       "  0.21815146505832672,\n",
       "  0.2182556539773941,\n",
       "  0.21828126907348633,\n",
       "  0.21804973483085632,\n",
       "  0.21892400085926056,\n",
       "  0.21989281475543976,\n",
       "  0.2198859006166458,\n",
       "  0.21859565377235413,\n",
       "  0.21671432256698608,\n",
       "  0.21574555337429047,\n",
       "  0.21491244435310364,\n",
       "  0.21349382400512695,\n",
       "  0.21379712224006653,\n",
       "  0.2147696316242218,\n",
       "  0.21588149666786194,\n",
       "  0.2159806489944458,\n",
       "  0.2147621214389801,\n",
       "  0.21233521401882172,\n",
       "  0.20939134061336517,\n",
       "  0.20875750482082367,\n",
       "  0.2088088095188141,\n",
       "  0.2100897878408432,\n",
       "  0.21286650002002716,\n",
       "  0.2145855575799942,\n",
       "  0.2119683176279068,\n",
       "  0.20856264233589172,\n",
       "  0.20610737800598145,\n",
       "  0.2054768204689026,\n",
       "  0.2062791883945465,\n",
       "  0.20825135707855225,\n",
       "  0.20942547917366028,\n",
       "  0.2077367752790451,\n",
       "  0.20554809272289276,\n",
       "  0.2053176462650299,\n",
       "  0.20517942309379578,\n",
       "  0.20402084290981293,\n",
       "  0.20293772220611572,\n",
       "  0.20218373835086823,\n",
       "  0.20092250406742096,\n",
       "  0.20009909570217133,\n",
       "  0.2007102072238922,\n",
       "  0.20096735656261444,\n",
       "  0.20048029720783234,\n",
       "  0.20215678215026855,\n",
       "  0.20453563332557678,\n",
       "  0.20478078722953796,\n",
       "  0.20126423239707947,\n",
       "  0.19829301536083221,\n",
       "  0.19773666560649872,\n",
       "  0.198459193110466,\n",
       "  0.19879648089408875,\n",
       "  0.19857293367385864,\n",
       "  0.19826914370059967,\n",
       "  0.19811797142028809,\n",
       "  0.19886916875839233,\n",
       "  0.19937269389629364,\n",
       "  0.19788889586925507,\n",
       "  0.19503365457057953,\n",
       "  0.19343486428260803,\n",
       "  0.19315268099308014,\n",
       "  0.19332675635814667,\n",
       "  0.1956387758255005,\n",
       "  0.19697687029838562,\n",
       "  0.19563044607639313,\n",
       "  0.1915760338306427,\n",
       "  0.19021674990653992,\n",
       "  0.189683198928833,\n",
       "  0.18910810351371765,\n",
       "  0.1886179894208908,\n",
       "  0.18955880403518677,\n",
       "  0.190039724111557,\n",
       "  0.1897016316652298,\n",
       "  0.18831029534339905,\n",
       "  0.18852044641971588,\n",
       "  0.18794891238212585,\n",
       "  0.18464118242263794,\n",
       "  0.18359117209911346,\n",
       "  0.18336790800094604,\n",
       "  0.1836470663547516,\n",
       "  0.18579797446727753,\n",
       "  0.18600665032863617,\n",
       "  0.18377122282981873,\n",
       "  0.18264219164848328,\n",
       "  0.18098092079162598,\n",
       "  0.1811968982219696,\n",
       "  0.18251648545265198,\n",
       "  0.18148738145828247,\n",
       "  0.1803022176027298,\n",
       "  0.17868368327617645,\n",
       "  0.17770560085773468,\n",
       "  0.17709332704544067,\n",
       "  0.17690354585647583,\n",
       "  0.17775492370128632,\n",
       "  0.1789732426404953,\n",
       "  0.178847074508667,\n",
       "  0.17659246921539307,\n",
       "  0.17536334693431854,\n",
       "  0.17532292008399963,\n",
       "  0.17660070955753326,\n",
       "  0.1797526329755783,\n",
       "  0.18002700805664062,\n",
       "  0.1767646223306656,\n",
       "  0.17349602282047272,\n",
       "  0.173098623752594,\n",
       "  0.17178639769554138,\n",
       "  0.17352430522441864,\n",
       "  0.1764136552810669,\n",
       "  0.17405341565608978,\n",
       "  0.170217826962471,\n",
       "  0.16992326080799103,\n",
       "  0.16905014216899872,\n",
       "  0.16953857243061066,\n",
       "  0.17395588755607605,\n",
       "  0.17777638137340546,\n",
       "  0.17210587859153748,\n",
       "  0.16504450142383575,\n",
       "  0.16515512764453888,\n",
       "  0.1646062731742859,\n",
       "  0.16244512796401978,\n",
       "  0.16275174915790558,\n",
       "  0.16510634124279022,\n",
       "  0.1634262502193451,\n",
       "  0.1602029949426651,\n",
       "  0.16015806794166565,\n",
       "  0.1600387990474701,\n",
       "  0.15985353291034698,\n",
       "  0.15963292121887207,\n",
       "  0.16003257036209106,\n",
       "  0.16040438413619995,\n",
       "  0.15921561419963837,\n",
       "  0.1581937074661255,\n",
       "  0.15806014835834503,\n",
       "  0.15904851257801056,\n",
       "  0.15953108668327332,\n",
       "  0.15855811536312103,\n",
       "  0.15664087235927582,\n",
       "  0.15673907101154327,\n",
       "  0.15682412683963776,\n",
       "  0.15876919031143188,\n",
       "  0.16005738079547882,\n",
       "  0.15705743432044983,\n",
       "  0.1545988768339157,\n",
       "  0.15359501540660858,\n",
       "  0.15263810753822327,\n",
       "  0.15257509052753448,\n",
       "  0.1548253297805786,\n",
       "  0.1551220864057541,\n",
       "  0.15262795984745026,\n",
       "  0.15150316059589386,\n",
       "  0.15138700604438782,\n",
       "  0.15103964507579803,\n",
       "  0.14990369975566864,\n",
       "  0.1485244482755661,\n",
       "  0.14686119556427002,\n",
       "  0.14589689671993256,\n",
       "  0.14605212211608887,\n",
       "  0.1451236605644226,\n",
       "  0.14339576661586761,\n",
       "  0.14563243091106415,\n",
       "  0.1417224109172821,\n",
       "  0.1438504457473755,\n",
       "  0.14352616667747498,\n",
       "  0.1406804621219635,\n",
       "  0.140634223818779,\n",
       "  0.14086945354938507,\n",
       "  0.13936200737953186,\n",
       "  0.13815951347351074,\n",
       "  0.1362745612859726,\n",
       "  0.13645103573799133,\n",
       "  0.13699568808078766,\n",
       "  0.13519859313964844,\n",
       "  0.13621458411216736,\n",
       "  0.13589055836200714,\n",
       "  0.14015567302703857,\n",
       "  0.14136292040348053,\n",
       "  0.1381675899028778,\n",
       "  0.13866770267486572,\n",
       "  0.14034588634967804,\n",
       "  0.14098863303661346,\n",
       "  0.14214356243610382,\n",
       "  0.14354568719863892,\n",
       "  0.13863247632980347,\n",
       "  0.13636955618858337,\n",
       "  0.13771995902061462,\n",
       "  0.13053251802921295,\n",
       "  0.13207685947418213,\n",
       "  0.12872785329818726,\n",
       "  0.129912868142128,\n",
       "  0.12698805332183838,\n",
       "  0.1266631931066513,\n",
       "  0.12832042574882507,\n",
       "  0.1291191428899765,\n",
       "  0.1278027594089508,\n",
       "  0.13131840527057648,\n",
       "  0.12425388395786285,\n",
       "  0.12986773252487183,\n",
       "  0.12527422606945038,\n",
       "  0.13379937410354614,\n",
       "  0.12208812683820724,\n",
       "  0.13133016228675842,\n",
       "  0.12297873198986053,\n",
       "  0.12269764393568039,\n",
       "  0.12455661594867706,\n",
       "  0.12358003109693527,\n",
       "  0.12395704537630081,\n",
       "  0.12466886639595032,\n",
       "  0.12563778460025787,\n",
       "  0.1212233304977417,\n",
       "  0.12175847589969635,\n",
       "  0.12072725594043732,\n",
       "  0.12004509568214417,\n",
       "  0.1198967769742012,\n",
       "  0.12022879719734192,\n",
       "  0.11815240234136581,\n",
       "  0.1190418004989624,\n",
       "  0.11953140795230865,\n",
       "  0.12035784870386124,\n",
       "  0.1221274882555008,\n",
       "  0.1225367859005928,\n",
       "  0.1230522096157074,\n",
       "  0.12349684536457062,\n",
       "  0.12774555385112762,\n",
       "  0.11781466007232666,\n",
       "  0.12399671971797943,\n",
       "  0.11661764234304428,\n",
       "  0.11822708696126938,\n",
       "  0.11621386557817459,\n",
       "  0.11643228679895401,\n",
       "  0.11505220830440521,\n",
       "  0.11442769318819046,\n",
       "  0.11564768850803375,\n",
       "  0.11493369936943054,\n",
       "  0.11392579227685928,\n",
       "  0.11264116317033768,\n",
       "  0.11185049265623093,\n",
       "  0.11141088604927063,\n",
       "  0.11243142187595367,\n",
       "  0.11150138825178146,\n",
       "  0.12295186519622803,\n",
       "  0.11949286609888077,\n",
       "  0.1265217512845993,\n",
       "  0.12290339916944504,\n",
       "  0.126620352268219,\n",
       "  0.11731728166341782,\n",
       "  0.13103333115577698,\n",
       "  0.11303677409887314,\n",
       "  0.1261509507894516,\n",
       "  0.11170084029436111,\n",
       "  0.1310419738292694,\n",
       "  0.11242800205945969,\n",
       "  0.12042704224586487,\n",
       "  0.11832980066537857,\n",
       "  0.11305899918079376,\n",
       "  0.12352319061756134,\n",
       "  0.11311667412519455,\n",
       "  0.11755961179733276,\n",
       "  0.11475946754217148,\n",
       "  0.11792576313018799,\n",
       "  0.11365866661071777,\n",
       "  0.11917246878147125,\n",
       "  0.11222491413354874,\n",
       "  0.11130551993846893,\n",
       "  0.11109265685081482,\n",
       "  0.11038126051425934,\n",
       "  0.10993854701519012,\n",
       "  0.11070564389228821,\n",
       "  0.11439052224159241,\n",
       "  0.11189846694469452,\n",
       "  0.11768915504217148,\n",
       "  0.11240838468074799,\n",
       "  0.11337891221046448,\n",
       "  0.11080268025398254,\n",
       "  0.10907643288373947,\n",
       "  0.10839153081178665,\n",
       "  0.10769905894994736,\n",
       "  0.1069042980670929,\n",
       "  0.10796406120061874,\n",
       "  0.11065275967121124,\n",
       "  0.11318708211183548,\n",
       "  0.11066565662622452,\n",
       "  0.11680480092763901,\n",
       "  0.10985372215509415,\n",
       "  0.11693980544805527,\n",
       "  0.1066407710313797,\n",
       "  0.11293140798807144,\n",
       "  0.10668867081403732,\n",
       "  0.11323143541812897,\n",
       "  0.10858456045389175,\n",
       "  0.11014685034751892,\n",
       "  0.10844021290540695,\n",
       "  0.10918550938367844,\n",
       "  0.10683039575815201,\n",
       "  0.10672681778669357,\n",
       "  0.10644759982824326,\n",
       "  0.10596317797899246,\n",
       "  0.10507601499557495,\n",
       "  0.10451441258192062,\n",
       "  0.1070791557431221,\n",
       "  0.10544653981924057,\n",
       "  0.10564024746417999,\n",
       "  0.10418147593736649,\n",
       "  0.10340704768896103,\n",
       "  0.09999576210975647,\n",
       "  0.10030440986156464,\n",
       "  0.10110550373792648,\n",
       "  0.10222473740577698,\n",
       "  0.1042390689253807,\n",
       "  0.10378889739513397,\n",
       "  0.1052011027932167,\n",
       "  0.10644346475601196,\n",
       "  0.10082953423261642,\n",
       "  0.10162486135959625,\n",
       "  0.10076123476028442,\n",
       "  0.10157157480716705,\n",
       "  0.10699831694364548,\n",
       "  0.10565169155597687,\n",
       "  0.11006733030080795,\n",
       "  0.10901850461959839,\n",
       "  0.10576238483190536,\n",
       "  0.10494617372751236,\n",
       "  0.10330724716186523,\n",
       "  0.10217618942260742,\n",
       "  0.10309622436761856,\n",
       "  0.10084953159093857,\n",
       "  0.10068945586681366,\n",
       "  0.10125355422496796,\n",
       "  0.10072296857833862,\n",
       "  0.10143808275461197,\n",
       "  0.1011013463139534,\n",
       "  0.10005822032690048,\n",
       "  0.09821486473083496,\n",
       "  0.09750822186470032,\n",
       "  0.09698252379894257,\n",
       "  0.10103657841682434,\n",
       "  0.1006535142660141,\n",
       "  0.09906290471553802,\n",
       "  0.0991765484213829,\n",
       "  0.10206171870231628,\n",
       "  0.09994689375162125,\n",
       "  0.1059579998254776,\n",
       "  0.09774114191532135,\n",
       "  0.10438752919435501,\n",
       "  0.09890340268611908,\n",
       "  0.10437050461769104,\n",
       "  0.10042774677276611,\n",
       "  0.09979807585477829,\n",
       "  0.10067281126976013,\n",
       "  0.10142224282026291,\n",
       "  0.10125065594911575,\n",
       "  0.10773526132106781,\n",
       "  0.10437551885843277,\n",
       "  0.10582698881626129,\n",
       "  0.10603123158216476,\n",
       "  0.10807041823863983,\n",
       "  0.10640595853328705,\n",
       "  0.10464680939912796,\n",
       "  0.10202310979366302,\n",
       "  0.10208147764205933,\n",
       "  0.1015867292881012,\n",
       "  0.10080297291278839,\n",
       "  0.09964274615049362,\n",
       "  0.09950442612171173,\n",
       "  0.10124415159225464,\n",
       "  0.10025642812252045,\n",
       "  0.09830936789512634,\n",
       "  0.0973593220114708,\n",
       "  0.1084149181842804,\n",
       "  0.09738628566265106,\n",
       "  0.10852862149477005,\n",
       "  0.09939533472061157,\n",
       "  0.11285432428121567,\n",
       "  0.10963891446590424,\n",
       "  0.1338149607181549,\n",
       "  0.105450838804245,\n",
       "  0.13019564747810364,\n",
       "  0.11107342690229416,\n",
       "  0.12617486715316772,\n",
       "  0.10877053439617157,\n",
       "  0.10922294855117798,\n",
       "  0.10380703210830688,\n",
       "  0.10757416486740112,\n",
       "  0.10215023905038834,\n",
       "  0.10752832144498825,\n",
       "  0.10082703083753586,\n",
       "  0.10442294180393219,\n",
       "  0.09918320924043655,\n",
       "  0.10567576438188553,\n",
       "  0.09723380953073502,\n",
       "  0.09998703002929688,\n",
       "  0.09543189406394958,\n",
       "  0.09467785060405731,\n",
       "  0.09554125368595123,\n",
       "  0.09939654916524887,\n",
       "  0.09462063014507294,\n",
       "  0.09776777774095535,\n",
       "  0.09587211161851883,\n",
       "  0.09474639594554901,\n",
       "  0.09907461702823639,\n",
       "  0.09298060834407806,\n",
       "  0.10060374438762665,\n",
       "  0.09261531382799149,\n",
       "  0.09292704612016678,\n",
       "  0.09281855821609497,\n",
       "  0.09304597228765488,\n",
       "  0.09498661011457443,\n",
       "  0.09541811048984528,\n",
       "  0.09609360992908478,\n",
       "  0.0966111421585083,\n",
       "  0.09783954173326492,\n",
       "  0.09846439957618713,\n",
       "  0.09880371391773224,\n",
       "  0.09998392313718796,\n",
       "  0.10088223963975906,\n",
       "  0.10229311138391495,\n",
       "  0.1025535985827446,\n",
       "  0.09941219538450241,\n",
       "  0.09827569872140884,\n",
       "  0.09672755748033524,\n",
       "  0.09688083082437515,\n",
       "  0.09757158160209656,\n",
       "  0.10099276155233383,\n",
       "  0.1064235046505928,\n",
       "  0.09815116226673126,\n",
       "  0.10264051705598831,\n",
       "  0.09943138808012009,\n",
       "  0.10513151437044144,\n",
       "  0.09476882964372635,\n",
       "  0.09394732117652893,\n",
       "  0.09102900326251984,\n",
       "  0.09559371322393417,\n",
       "  0.09216677397489548,\n",
       "  0.09812640398740768,\n",
       "  0.09424979984760284,\n",
       "  0.09766814112663269,\n",
       "  0.09596158564090729,\n",
       "  0.09707821905612946,\n",
       "  0.09509129077196121,\n",
       "  0.09541229158639908,\n",
       "  0.09432540088891983,\n",
       "  0.09306252747774124,\n",
       "  0.09226898849010468,\n",
       "  0.09948428720235825,\n",
       "  0.09256836026906967,\n",
       "  0.10656820982694626,\n",
       "  0.09455148875713348,\n",
       "  0.11314014345407486,\n",
       "  0.09684818238019943,\n",
       "  0.10735686123371124,\n",
       "  0.09578543901443481,\n",
       "  0.10869622975587845,\n",
       "  0.09262432903051376,\n",
       "  0.09819607436656952,\n",
       "  0.09119784086942673,\n",
       "  0.09684079885482788,\n",
       "  0.09721270203590393,\n",
       "  0.09248346090316772,\n",
       "  0.09421782195568085,\n",
       "  0.09572300314903259,\n",
       "  0.09771276265382767,\n",
       "  0.0949421375989914,\n",
       "  0.09485987573862076,\n",
       "  0.0936475470662117,\n",
       "  0.09343705326318741,\n",
       "  0.09352672100067139,\n",
       "  0.09298040717840195,\n",
       "  0.09101570397615433,\n",
       "  0.0964946374297142,\n",
       "  0.09784063696861267,\n",
       "  0.09115180373191833,\n",
       "  0.09155086427927017,\n",
       "  0.09177590906620026,\n",
       "  0.09361347556114197,\n",
       "  0.08830273151397705,\n",
       "  0.09265933930873871,\n",
       "  0.0917397290468216,\n",
       "  0.09719475358724594,\n",
       "  0.09323045611381531,\n",
       "  0.09586569666862488,\n",
       "  0.09220056980848312,\n",
       "  0.09165524691343307,\n",
       "  0.09038595110177994,\n",
       "  0.09070868790149689,\n",
       "  0.09088385850191116,\n",
       "  0.09322316199541092,\n",
       "  0.091983862221241,\n",
       "  0.09357818216085434,\n",
       "  0.09296164661645889,\n",
       "  0.09183811396360397,\n",
       "  0.09216898679733276,\n",
       "  0.09102997183799744,\n",
       "  0.0893622487783432,\n",
       "  0.09107409417629242,\n",
       "  0.09107892215251923,\n",
       "  0.09289928525686264,\n",
       "  0.09432346373796463,\n",
       "  0.09546877443790436,\n",
       "  0.09653399884700775,\n",
       "  0.09941127151250839,\n",
       "  0.09719882905483246,\n",
       "  0.09726525843143463,\n",
       "  0.09490527957677841,\n",
       "  0.09326988458633423,\n",
       "  0.0925540179014206,\n",
       "  0.0918939858675003,\n",
       "  0.09105164557695389,\n",
       "  0.09126677364110947,\n",
       "  0.091632179915905,\n",
       "  0.09308677911758423,\n",
       "  0.0916304662823677,\n",
       "  0.09719937294721603,\n",
       "  0.09172949939966202,\n",
       "  0.0986810177564621,\n",
       "  0.09404518455266953,\n",
       "  0.10715844482183456,\n",
       "  0.09780289977788925,\n",
       "  0.1061335876584053,\n",
       "  0.09935580939054489,\n",
       "  0.11498958617448807,\n",
       "  0.10655322670936584,\n",
       "  0.10177221149206161,\n",
       "  0.09871304780244827,\n",
       "  0.0985761284828186,\n",
       "  0.09550353139638901,\n",
       "  0.09272715449333191,\n",
       "  0.09424834698438644,\n",
       "  0.09468544274568558,\n",
       "  0.09550889581441879,\n",
       "  0.09936388581991196,\n",
       "  0.10033481568098068,\n",
       "  0.09977800399065018,\n",
       "  0.1003049984574318,\n",
       "  0.09712692350149155,\n",
       "  0.09787791967391968,\n",
       "  0.09061776101589203,\n",
       "  0.0966537818312645,\n",
       "  0.08744550496339798,\n",
       "  0.09399307519197464,\n",
       "  0.08748836815357208,\n",
       "  0.08990144729614258,\n",
       "  0.09089058637619019,\n",
       "  0.0966884046792984,\n",
       "  0.09520908445119858,\n",
       "  0.09702545404434204,\n",
       "  0.097749724984169,\n",
       "  0.09788063913583755,\n",
       "  0.09718362241983414,\n",
       "  0.09054100513458252,\n",
       "  0.08902581036090851,\n",
       "  0.08856824040412903,\n",
       "  0.08864661306142807,\n",
       "  0.09007487446069717,\n",
       "  0.09223318099975586,\n",
       "  0.08993776887655258,\n",
       "  0.08915600180625916,\n",
       "  0.08870106190443039,\n",
       "  0.08937041461467743,\n",
       "  0.09586826711893082,\n",
       "  0.09416112303733826,\n",
       "  0.09697910398244858,\n",
       "  0.0943303033709526,\n",
       "  0.09335509687662125,\n",
       "  0.08870308101177216,\n",
       "  0.08929862827062607,\n",
       "  0.08654560893774033,\n",
       "  0.08876796066761017,\n",
       "  0.08915109187364578,\n",
       "  0.09276582300662994,\n",
       "  0.09473203867673874,\n",
       "  0.09455666691064835,\n",
       "  0.09736436605453491,\n",
       "  0.09839515388011932,\n",
       "  0.10013753175735474,\n",
       "  0.09938792139291763,\n",
       "  0.09739777445793152,\n",
       "  0.0936213806271553,\n",
       "  0.09133985638618469,\n",
       "  0.08932226896286011,\n",
       "  0.08987006545066833,\n",
       "  0.08751881867647171,\n",
       "  0.08819984644651413,\n",
       "  0.09050193428993225,\n",
       "  0.09089601039886475,\n",
       "  0.09255461394786835,\n",
       "  0.09602296352386475,\n",
       "  0.09724356234073639,\n",
       "  0.09879230707883835,\n",
       "  0.0991581454873085,\n",
       "  0.09792663156986237,\n",
       "  0.09528869390487671,\n",
       "  0.09204870462417603,\n",
       "  0.08843724429607391,\n",
       "  0.08714431524276733,\n",
       "  0.08633730560541153,\n",
       "  0.08668318390846252,\n",
       "  0.08840473741292953,\n",
       "  0.09368360042572021,\n",
       "  0.09299412369728088,\n",
       "  0.09816455096006393,\n",
       "  0.09735990315675735,\n",
       "  0.09854722023010254,\n",
       "  0.10019571334123611,\n",
       "  0.0981655865907669,\n",
       "  0.10134901851415634,\n",
       "  0.09662691503763199,\n",
       "  0.10672826319932938,\n",
       "  0.1002834141254425,\n",
       "  0.09304530918598175,\n",
       "  0.09402401000261307,\n",
       "  0.09302705526351929,\n",
       "  0.09476307034492493,\n",
       "  0.08959022164344788,\n",
       "  0.09213074296712875,\n",
       "  0.09151605516672134,\n",
       "  0.09289803355932236,\n",
       "  0.09374379366636276,\n",
       "  0.09392959624528885,\n",
       "  0.09657132625579834,\n",
       "  0.09813793748617172,\n",
       "  0.09493010491132736,\n",
       "  0.10346013307571411,\n",
       "  0.09790179133415222,\n",
       "  0.10422925651073456,\n",
       "  0.09932466596364975,\n",
       "  0.1034848690032959,\n",
       "  0.10436362773180008,\n",
       "  0.11633671820163727,\n",
       "  0.09767916053533554,\n",
       "  0.09527797251939774,\n",
       "  0.09045343846082687,\n",
       "  0.08709286153316498,\n",
       "  0.08833196014165878,\n",
       "  0.08570851385593414,\n",
       "  0.08793913573026657,\n",
       "  0.08706078678369522,\n",
       "  0.0953727588057518,\n",
       "  0.09069796651601791,\n",
       "  0.10142135620117188,\n",
       "  0.09044090658426285,\n",
       "  0.09258291125297546,\n",
       "  0.0896301418542862,\n",
       "  0.08866957575082779,\n",
       "  0.09010498970746994,\n",
       "  0.09214940667152405,\n",
       "  0.0931822806596756,\n",
       "  0.09387406706809998,\n",
       "  0.09508959949016571,\n",
       "  0.09504038840532303,\n",
       "  0.09555109590291977,\n",
       "  0.09637433290481567,\n",
       "  0.09534549713134766,\n",
       "  0.09649789333343506,\n",
       "  0.09294098615646362,\n",
       "  0.09102941304445267,\n",
       "  0.0890546441078186,\n",
       "  0.09149657189846039,\n",
       "  0.0904134064912796,\n",
       "  0.08834542334079742,\n",
       "  0.08873949944972992,\n",
       "  0.09338673204183578,\n",
       "  0.09051674604415894,\n",
       "  0.09414036571979523,\n",
       "  0.09261337667703629,\n",
       "  0.09605421125888824,\n",
       "  0.09407701343297958,\n",
       "  0.09667059779167175,\n",
       "  0.09908377379179001,\n",
       "  0.1015879362821579,\n",
       "  0.09555672109127045,\n",
       "  0.09622976928949356,\n",
       "  0.09623095393180847,\n",
       "  0.09584379196166992,\n",
       "  0.10054011642932892,\n",
       "  0.0968995988368988,\n",
       "  0.10388553142547607,\n",
       "  0.0930270403623581,\n",
       "  0.09466011077165604,\n",
       "  0.09014524519443512,\n",
       "  0.09659045189619064,\n",
       "  0.0908726155757904,\n",
       "  0.09417006373405457,\n",
       "  0.09096382558345795,\n",
       "  0.103887639939785,\n",
       "  0.09501490741968155,\n",
       "  0.10089153796434402,\n",
       "  0.09942923486232758,\n",
       "  0.10663799196481705,\n",
       "  0.09838198870420456,\n",
       "  0.09847424924373627,\n",
       "  0.10087059438228607,\n",
       "  0.10173919796943665,\n",
       "  0.0867822915315628,\n",
       "  0.08730961382389069,\n",
       "  0.08686583489179611,\n",
       "  0.08958996832370758,\n",
       "  0.08505254983901978,\n",
       "  0.0919841006398201,\n",
       "  0.09209573268890381,\n",
       "  0.10067189484834671,\n",
       "  0.09446167945861816,\n",
       "  0.09877011924982071,\n",
       "  0.10282241553068161,\n",
       "  0.1007985770702362,\n",
       "  0.0938488245010376,\n",
       "  0.09724965691566467,\n",
       "  0.09660007804632187,\n",
       "  0.1054682508111,\n",
       "  0.08633396774530411,\n",
       "  0.09650160372257233,\n",
       "  0.0860346183180809,\n",
       "  0.09777916967868805,\n",
       "  0.08664848655462265,\n",
       "  0.10220292955636978,\n",
       "  0.09412618726491928,\n",
       "  0.10909970104694366,\n",
       "  0.09553280472755432,\n",
       "  0.10212605446577072,\n",
       "  0.09674316644668579,\n",
       "  0.10248477756977081,\n",
       "  0.09400153160095215,\n",
       "  0.08760519325733185,\n",
       "  0.08508946746587753,\n",
       "  0.0824485719203949,\n",
       "  0.08351831138134003,\n",
       "  0.08103564381599426,\n",
       "  0.08163094520568848,\n",
       "  0.08202039450407028,\n",
       "  0.08393494784832001,\n",
       "  0.08671418577432632,\n",
       "  0.09034346789121628,\n",
       "  0.09416589885950089,\n",
       "  0.0956701785326004,\n",
       "  0.10167332738637924,\n",
       "  0.11042848229408264,\n",
       "  0.09605677425861359,\n",
       "  0.09734683483839035,\n",
       "  0.09315855801105499,\n",
       "  0.09360913187265396,\n",
       "  0.09334303438663483,\n",
       "  0.09349673986434937,\n",
       "  0.09439834207296371,\n",
       "  0.09205044060945511,\n",
       "  0.0902426615357399,\n",
       "  0.08889012038707733,\n",
       "  0.08975084871053696,\n",
       "  0.08906161040067673,\n",
       "  0.1001727506518364,\n",
       "  0.0937754437327385,\n",
       "  0.11347655951976776,\n",
       "  0.08673583716154099,\n",
       "  0.0975252240896225,\n",
       "  0.08749785274267197,\n",
       "  0.08766891807317734,\n",
       "  0.09110087156295776,\n",
       "  0.08953580260276794,\n",
       "  0.09436299651861191,\n",
       "  0.09209633618593216,\n",
       "  0.09447886794805527,\n",
       "  0.09113140404224396,\n",
       "  0.09976266324520111,\n",
       "  0.09376727789640427,\n",
       "  0.09801284968852997,\n",
       "  0.09288715571165085,\n",
       "  0.10006057471036911,\n",
       "  0.09805652499198914,\n",
       "  0.09458088874816895,\n",
       "  0.09301377832889557,\n",
       "  0.09888899326324463,\n",
       "  0.0973091721534729,\n",
       "  0.09172190725803375,\n",
       "  0.09356378018856049,\n",
       "  0.09227652847766876,\n",
       "  0.0933036357164383,\n",
       "  0.09204620122909546,\n",
       "  0.0917258933186531,\n",
       "  0.09210487455129623,\n",
       "  0.09212077409029007,\n",
       "  0.08861222863197327,\n",
       "  0.09186367690563202,\n",
       "  0.08963612467050552,\n",
       "  0.09556815773248672,\n",
       "  0.0929117202758789,\n",
       "  0.09857292473316193,\n",
       "  0.09514015913009644,\n",
       "  0.09969660639762878,\n",
       "  0.0985037088394165,\n",
       "  0.09324929118156433,\n",
       "  0.09477578103542328,\n",
       "  0.09808428585529327,\n",
       "  0.10345926880836487,\n",
       "  0.09197259694337845,\n",
       "  0.09590214490890503,\n",
       "  0.08977188915014267,\n",
       "  0.09175799787044525,\n",
       "  0.09133689850568771,\n",
       "  0.09939856082201004,\n",
       "  0.09318607300519943,\n",
       "  0.0972156673669815,\n",
       "  0.09483025223016739,\n",
       "  0.0993705466389656,\n",
       "  0.093748077750206,\n",
       "  0.09756826609373093,\n",
       "  0.09258806705474854,\n",
       "  0.09554792195558548,\n",
       "  0.09157615154981613,\n",
       "  0.09308979660272598,\n",
       "  0.09607750177383423,\n",
       "  0.09733781963586807,\n",
       "  0.10198494791984558,\n",
       "  0.09928306192159653,\n",
       "  0.10854454338550568,\n",
       "  0.09920629858970642,\n",
       "  0.11535361409187317,\n",
       "  0.10198181122541428,\n",
       "  0.09611605852842331,\n",
       "  0.09099746495485306,\n",
       "  0.0910709947347641,\n",
       "  0.0875636488199234,\n",
       "  0.08926914632320404,\n",
       "  0.08844615519046783,\n",
       "  0.08634742349386215,\n",
       "  0.08627991378307343,\n",
       "  0.08842619508504868,\n",
       "  0.08969545364379883,\n",
       "  0.09134754538536072],\n",
       " 'val_accuracy': [0.7422360181808472,\n",
       "  0.7453415989875793,\n",
       "  0.7453415989875793,\n",
       "  0.7453415989875793,\n",
       "  0.7453415989875793,\n",
       "  0.7453415989875793,\n",
       "  0.7453415989875793,\n",
       "  0.7453415989875793,\n",
       "  0.7484471797943115,\n",
       "  0.7857142686843872,\n",
       "  0.8416149020195007,\n",
       "  0.8633540272712708,\n",
       "  0.8602484464645386,\n",
       "  0.8602484464645386,\n",
       "  0.8540372848510742,\n",
       "  0.8571428656578064,\n",
       "  0.8726708292961121,\n",
       "  0.8726708292961121,\n",
       "  0.8819875717163086,\n",
       "  0.888198733329773,\n",
       "  0.8944099545478821,\n",
       "  0.9006211161613464,\n",
       "  0.9006211161613464,\n",
       "  0.8944099545478821,\n",
       "  0.8913043737411499,\n",
       "  0.8913043737411499,\n",
       "  0.8913043737411499,\n",
       "  0.8975155353546143,\n",
       "  0.9006211161613464,\n",
       "  0.9068322777748108,\n",
       "  0.909937858581543,\n",
       "  0.9130434989929199,\n",
       "  0.9161490797996521,\n",
       "  0.9161490797996521,\n",
       "  0.9161490797996521,\n",
       "  0.9130434989929199,\n",
       "  0.9130434989929199,\n",
       "  0.9130434989929199,\n",
       "  0.909937858581543,\n",
       "  0.9130434989929199,\n",
       "  0.9192546606063843,\n",
       "  0.9223602414131165,\n",
       "  0.9192546606063843,\n",
       "  0.9192546606063843,\n",
       "  0.9192546606063843,\n",
       "  0.9161490797996521,\n",
       "  0.9161490797996521,\n",
       "  0.9161490797996521,\n",
       "  0.9223602414131165,\n",
       "  0.9223602414131165,\n",
       "  0.9192546606063843,\n",
       "  0.9192546606063843,\n",
       "  0.9192546606063843,\n",
       "  0.9161490797996521,\n",
       "  0.9130434989929199,\n",
       "  0.9161490797996521,\n",
       "  0.9130434989929199,\n",
       "  0.9161490797996521,\n",
       "  0.9161490797996521,\n",
       "  0.9161490797996521,\n",
       "  0.9161490797996521,\n",
       "  0.9161490797996521,\n",
       "  0.9161490797996521,\n",
       "  0.9192546606063843,\n",
       "  0.9161490797996521,\n",
       "  0.9161490797996521,\n",
       "  0.9161490797996521,\n",
       "  0.9161490797996521,\n",
       "  0.9161490797996521,\n",
       "  0.9161490797996521,\n",
       "  0.9161490797996521,\n",
       "  0.9192546606063843,\n",
       "  0.9192546606063843,\n",
       "  0.9192546606063843,\n",
       "  0.9192546606063843,\n",
       "  0.9192546606063843,\n",
       "  0.9192546606063843,\n",
       "  0.9161490797996521,\n",
       "  0.9161490797996521,\n",
       "  0.9161490797996521,\n",
       "  0.9161490797996521,\n",
       "  0.9192546606063843,\n",
       "  0.9223602414131165,\n",
       "  0.9223602414131165,\n",
       "  0.9223602414131165,\n",
       "  0.9223602414131165,\n",
       "  0.9192546606063843,\n",
       "  0.9161490797996521,\n",
       "  0.9192546606063843,\n",
       "  0.9223602414131165,\n",
       "  0.9254658222198486,\n",
       "  0.9223602414131165,\n",
       "  0.9223602414131165,\n",
       "  0.9223602414131165,\n",
       "  0.9192546606063843,\n",
       "  0.9192546606063843,\n",
       "  0.9223602414131165,\n",
       "  0.9223602414131165,\n",
       "  0.9223602414131165,\n",
       "  0.9223602414131165,\n",
       "  0.9223602414131165,\n",
       "  0.9223602414131165,\n",
       "  0.9223602414131165,\n",
       "  0.9254658222198486,\n",
       "  0.9223602414131165,\n",
       "  0.9223602414131165,\n",
       "  0.9254658222198486,\n",
       "  0.9223602414131165,\n",
       "  0.9223602414131165,\n",
       "  0.9223602414131165,\n",
       "  0.9254658222198486,\n",
       "  0.9223602414131165,\n",
       "  0.9192546606063843,\n",
       "  0.9223602414131165,\n",
       "  0.9223602414131165,\n",
       "  0.9254658222198486,\n",
       "  0.9254658222198486,\n",
       "  0.9223602414131165,\n",
       "  0.9223602414131165,\n",
       "  0.9254658222198486,\n",
       "  0.9223602414131165,\n",
       "  0.9254658222198486,\n",
       "  0.9192546606063843,\n",
       "  0.9223602414131165,\n",
       "  0.9192546606063843,\n",
       "  0.9254658222198486,\n",
       "  0.9254658222198486,\n",
       "  0.9254658222198486,\n",
       "  0.9192546606063843,\n",
       "  0.9161490797996521,\n",
       "  0.9192546606063843,\n",
       "  0.9223602414131165,\n",
       "  0.9161490797996521,\n",
       "  0.9223602414131165,\n",
       "  0.9254658222198486,\n",
       "  0.9254658222198486,\n",
       "  0.9254658222198486,\n",
       "  0.9285714030265808,\n",
       "  0.9285714030265808,\n",
       "  0.9223602414131165,\n",
       "  0.9223602414131165,\n",
       "  0.9223602414131165,\n",
       "  0.9223602414131165,\n",
       "  0.9285714030265808,\n",
       "  0.9254658222198486,\n",
       "  0.9254658222198486,\n",
       "  0.9223602414131165,\n",
       "  0.9223602414131165,\n",
       "  0.9223602414131165,\n",
       "  0.9223602414131165,\n",
       "  0.9223602414131165,\n",
       "  0.9254658222198486,\n",
       "  0.9254658222198486,\n",
       "  0.9254658222198486,\n",
       "  0.9316770434379578,\n",
       "  0.9254658222198486,\n",
       "  0.9254658222198486,\n",
       "  0.9254658222198486,\n",
       "  0.9254658222198486,\n",
       "  0.9254658222198486,\n",
       "  0.9316770434379578,\n",
       "  0.9347826242446899,\n",
       "  0.9285714030265808,\n",
       "  0.9223602414131165,\n",
       "  0.9223602414131165,\n",
       "  0.9254658222198486,\n",
       "  0.9285714030265808,\n",
       "  0.9285714030265808,\n",
       "  0.9254658222198486,\n",
       "  0.9285714030265808,\n",
       "  0.9223602414131165,\n",
       "  0.9223602414131165,\n",
       "  0.9347826242446899,\n",
       "  0.9285714030265808,\n",
       "  0.9347826242446899,\n",
       "  0.9316770434379578,\n",
       "  0.9285714030265808,\n",
       "  0.9223602414131165,\n",
       "  0.9285714030265808,\n",
       "  0.9316770434379578,\n",
       "  0.9285714030265808,\n",
       "  0.9285714030265808,\n",
       "  0.9316770434379578,\n",
       "  0.9316770434379578,\n",
       "  0.9254658222198486,\n",
       "  0.9285714030265808,\n",
       "  0.9347826242446899,\n",
       "  0.9347826242446899,\n",
       "  0.9347826242446899,\n",
       "  0.9347826242446899,\n",
       "  0.9347826242446899,\n",
       "  0.9347826242446899,\n",
       "  0.9347826242446899,\n",
       "  0.9347826242446899,\n",
       "  0.9316770434379578,\n",
       "  0.9347826242446899,\n",
       "  0.9347826242446899,\n",
       "  0.9285714030265808,\n",
       "  0.9316770434379578,\n",
       "  0.9347826242446899,\n",
       "  0.9316770434379578,\n",
       "  0.9347826242446899,\n",
       "  0.9285714030265808,\n",
       "  0.9285714030265808,\n",
       "  0.9347826242446899,\n",
       "  0.9347826242446899,\n",
       "  0.9285714030265808,\n",
       "  0.9285714030265808,\n",
       "  0.9347826242446899,\n",
       "  0.9316770434379578,\n",
       "  0.9316770434379578,\n",
       "  0.9316770434379578,\n",
       "  0.9347826242446899,\n",
       "  0.9347826242446899,\n",
       "  0.9347826242446899,\n",
       "  0.9347826242446899,\n",
       "  0.9347826242446899,\n",
       "  0.9378882050514221,\n",
       "  0.9347826242446899,\n",
       "  0.9347826242446899,\n",
       "  0.9347826242446899,\n",
       "  0.9316770434379578,\n",
       "  0.9316770434379578,\n",
       "  0.9316770434379578,\n",
       "  0.9347826242446899,\n",
       "  0.9378882050514221,\n",
       "  0.9347826242446899,\n",
       "  0.9347826242446899,\n",
       "  0.9316770434379578,\n",
       "  0.9378882050514221,\n",
       "  0.9378882050514221,\n",
       "  0.9347826242446899,\n",
       "  0.9378882050514221,\n",
       "  0.9347826242446899,\n",
       "  0.9378882050514221,\n",
       "  0.9347826242446899,\n",
       "  0.9347826242446899,\n",
       "  0.9285714030265808,\n",
       "  0.9285714030265808,\n",
       "  0.9347826242446899,\n",
       "  0.9378882050514221,\n",
       "  0.9378882050514221,\n",
       "  0.9347826242446899,\n",
       "  0.9316770434379578,\n",
       "  0.9347826242446899,\n",
       "  0.9316770434379578,\n",
       "  0.9285714030265808,\n",
       "  0.9347826242446899,\n",
       "  0.9347826242446899,\n",
       "  0.9378882050514221,\n",
       "  0.9347826242446899,\n",
       "  0.9316770434379578,\n",
       "  0.9378882050514221,\n",
       "  0.9378882050514221,\n",
       "  0.9378882050514221,\n",
       "  0.9316770434379578,\n",
       "  0.9378882050514221,\n",
       "  0.9378882050514221,\n",
       "  0.9378882050514221,\n",
       "  0.9409937858581543,\n",
       "  0.9409937858581543,\n",
       "  0.9409937858581543,\n",
       "  0.9472049474716187,\n",
       "  0.9409937858581543,\n",
       "  0.9378882050514221,\n",
       "  0.9472049474716187,\n",
       "  0.9409937858581543,\n",
       "  0.9409937858581543,\n",
       "  0.9378882050514221,\n",
       "  0.9378882050514221,\n",
       "  0.9378882050514221,\n",
       "  0.9472049474716187,\n",
       "  0.9409937858581543,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9440993666648865,\n",
       "  0.9409937858581543,\n",
       "  0.9503105878829956,\n",
       "  0.9472049474716187,\n",
       "  0.9440993666648865,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9503105878829956,\n",
       "  0.9472049474716187,\n",
       "  0.9503105878829956,\n",
       "  0.9409937858581543,\n",
       "  0.9503105878829956,\n",
       "  0.9409937858581543,\n",
       "  0.9472049474716187,\n",
       "  0.9503105878829956,\n",
       "  0.9503105878829956,\n",
       "  0.9534161686897278,\n",
       "  0.9534161686897278,\n",
       "  0.9534161686897278,\n",
       "  0.9472049474716187,\n",
       "  0.9534161686897278,\n",
       "  0.95652174949646,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.95652174949646,\n",
       "  0.9534161686897278,\n",
       "  0.9534161686897278,\n",
       "  0.9472049474716187,\n",
       "  0.9409937858581543,\n",
       "  0.9440993666648865,\n",
       "  0.9472049474716187,\n",
       "  0.9440993666648865,\n",
       "  0.9503105878829956,\n",
       "  0.9472049474716187,\n",
       "  0.95652174949646,\n",
       "  0.9503105878829956,\n",
       "  0.9503105878829956,\n",
       "  0.9440993666648865,\n",
       "  0.9534161686897278,\n",
       "  0.9472049474716187,\n",
       "  0.9440993666648865,\n",
       "  0.95652174949646,\n",
       "  0.9472049474716187,\n",
       "  0.95652174949646,\n",
       "  0.9440993666648865,\n",
       "  0.9534161686897278,\n",
       "  0.9534161686897278,\n",
       "  0.9534161686897278,\n",
       "  0.9378882050514221,\n",
       "  0.95652174949646,\n",
       "  0.95652174949646,\n",
       "  0.9534161686897278,\n",
       "  0.9534161686897278,\n",
       "  0.95652174949646,\n",
       "  0.95652174949646,\n",
       "  0.9503105878829956,\n",
       "  0.95652174949646,\n",
       "  0.95652174949646,\n",
       "  0.95652174949646,\n",
       "  0.95652174949646,\n",
       "  0.9534161686897278,\n",
       "  0.95652174949646,\n",
       "  0.9534161686897278,\n",
       "  0.9534161686897278,\n",
       "  0.9596273303031921,\n",
       "  0.9596273303031921,\n",
       "  0.9503105878829956,\n",
       "  0.9472049474716187,\n",
       "  0.9596273303031921,\n",
       "  0.95652174949646,\n",
       "  0.95652174949646,\n",
       "  0.9409937858581543,\n",
       "  0.95652174949646,\n",
       "  0.9596273303031921,\n",
       "  0.95652174949646,\n",
       "  0.9440993666648865,\n",
       "  0.95652174949646,\n",
       "  0.9596273303031921,\n",
       "  0.9534161686897278,\n",
       "  0.95652174949646,\n",
       "  0.9596273303031921,\n",
       "  0.9596273303031921,\n",
       "  0.9627329111099243,\n",
       "  0.9596273303031921,\n",
       "  0.9658384919166565,\n",
       "  0.9596273303031921,\n",
       "  0.95652174949646,\n",
       "  0.9658384919166565,\n",
       "  0.9596273303031921,\n",
       "  0.9596273303031921,\n",
       "  0.9596273303031921,\n",
       "  0.9658384919166565,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9658384919166565,\n",
       "  0.9689440727233887,\n",
       "  0.9596273303031921,\n",
       "  0.9596273303031921,\n",
       "  0.9689440727233887,\n",
       "  0.9627329111099243,\n",
       "  0.9689440727233887,\n",
       "  0.9658384919166565,\n",
       "  0.9534161686897278,\n",
       "  0.9596273303031921,\n",
       "  0.95652174949646,\n",
       "  0.9596273303031921,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9720497131347656,\n",
       "  0.9689440727233887,\n",
       "  0.9627329111099243,\n",
       "  0.9658384919166565,\n",
       "  0.9658384919166565,\n",
       "  0.9658384919166565,\n",
       "  0.9689440727233887,\n",
       "  0.9658384919166565,\n",
       "  0.9658384919166565,\n",
       "  0.9658384919166565,\n",
       "  0.9658384919166565,\n",
       "  0.9720497131347656,\n",
       "  0.9720497131347656,\n",
       "  0.9596273303031921,\n",
       "  0.9627329111099243,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9627329111099243,\n",
       "  0.9689440727233887,\n",
       "  0.9596273303031921,\n",
       "  0.9720497131347656,\n",
       "  0.9596273303031921,\n",
       "  0.9627329111099243,\n",
       "  0.9596273303031921,\n",
       "  0.9658384919166565,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9658384919166565,\n",
       "  0.95652174949646,\n",
       "  0.9596273303031921,\n",
       "  0.9627329111099243,\n",
       "  0.9658384919166565,\n",
       "  0.95652174949646,\n",
       "  0.95652174949646,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9658384919166565,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9658384919166565,\n",
       "  0.9689440727233887,\n",
       "  0.9627329111099243,\n",
       "  0.9658384919166565,\n",
       "  0.9596273303031921,\n",
       "  0.9689440727233887,\n",
       "  0.9596273303031921,\n",
       "  0.95652174949646,\n",
       "  0.9596273303031921,\n",
       "  0.9689440727233887,\n",
       "  0.9503105878829956,\n",
       "  0.95652174949646,\n",
       "  0.9596273303031921,\n",
       "  0.95652174949646,\n",
       "  0.95652174949646,\n",
       "  0.9627329111099243,\n",
       "  0.95652174949646,\n",
       "  0.9627329111099243,\n",
       "  0.9596273303031921,\n",
       "  0.9658384919166565,\n",
       "  0.9596273303031921,\n",
       "  0.9658384919166565,\n",
       "  0.9658384919166565,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9720497131347656,\n",
       "  0.9689440727233887,\n",
       "  0.9720497131347656,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9658384919166565,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9720497131347656,\n",
       "  0.9720497131347656,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9720497131347656,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9720497131347656,\n",
       "  0.9627329111099243,\n",
       "  0.9720497131347656,\n",
       "  0.9658384919166565,\n",
       "  0.9689440727233887,\n",
       "  0.9596273303031921,\n",
       "  0.9689440727233887,\n",
       "  0.9751552939414978,\n",
       "  0.9720497131347656,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9720497131347656,\n",
       "  0.9658384919166565,\n",
       "  0.9720497131347656,\n",
       "  0.9658384919166565,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9720497131347656,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9596273303031921,\n",
       "  0.9720497131347656,\n",
       "  0.9658384919166565,\n",
       "  0.9689440727233887,\n",
       "  0.9596273303031921,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.97826087474823,\n",
       "  0.9627329111099243,\n",
       "  0.9658384919166565,\n",
       "  0.9751552939414978,\n",
       "  0.9720497131347656,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9720497131347656,\n",
       "  0.9720497131347656,\n",
       "  0.9720497131347656,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9720497131347656,\n",
       "  0.9751552939414978,\n",
       "  0.9689440727233887,\n",
       "  0.9720497131347656,\n",
       "  0.9720497131347656,\n",
       "  0.9720497131347656,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9720497131347656,\n",
       "  0.9720497131347656,\n",
       "  0.9720497131347656,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9751552939414978,\n",
       "  0.9720497131347656,\n",
       "  0.9689440727233887,\n",
       "  0.9720497131347656,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9720497131347656,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9720497131347656,\n",
       "  0.9720497131347656,\n",
       "  0.9720497131347656,\n",
       "  0.9720497131347656,\n",
       "  0.9720497131347656,\n",
       "  0.9720497131347656,\n",
       "  0.9720497131347656,\n",
       "  0.9689440727233887,\n",
       "  0.9720497131347656,\n",
       "  0.9720497131347656,\n",
       "  0.9720497131347656,\n",
       "  0.9689440727233887,\n",
       "  0.9720497131347656,\n",
       "  0.9720497131347656,\n",
       "  0.9720497131347656,\n",
       "  0.9720497131347656,\n",
       "  0.9720497131347656,\n",
       "  0.9751552939414978,\n",
       "  0.9720497131347656,\n",
       "  0.9689440727233887,\n",
       "  0.9720497131347656,\n",
       "  0.97826087474823,\n",
       "  0.9720497131347656,\n",
       "  0.9627329111099243,\n",
       "  0.9689440727233887,\n",
       "  0.9751552939414978,\n",
       "  0.9720497131347656,\n",
       "  0.95652174949646,\n",
       "  0.9658384919166565,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9658384919166565,\n",
       "  0.9689440727233887,\n",
       "  0.9751552939414978,\n",
       "  0.97826087474823,\n",
       "  0.9751552939414978,\n",
       "  0.9720497131347656,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9720497131347656,\n",
       "  0.9720497131347656,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9720497131347656,\n",
       "  0.9813664555549622,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9720497131347656,\n",
       "  0.9720497131347656,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9720497131347656,\n",
       "  0.97826087474823,\n",
       "  0.9720497131347656,\n",
       "  0.9689440727233887,\n",
       "  0.9720497131347656,\n",
       "  0.9751552939414978,\n",
       "  0.9720497131347656,\n",
       "  0.9689440727233887,\n",
       "  0.97826087474823,\n",
       "  0.97826087474823,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9720497131347656,\n",
       "  0.9720497131347656,\n",
       "  0.9689440727233887,\n",
       "  0.9720497131347656,\n",
       "  0.9751552939414978,\n",
       "  0.9720497131347656,\n",
       "  0.9720497131347656,\n",
       "  0.9720497131347656,\n",
       "  0.9751552939414978,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.97826087474823,\n",
       "  0.97826087474823,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9751552939414978,\n",
       "  0.9720497131347656,\n",
       "  0.9720497131347656,\n",
       "  0.9720497131347656,\n",
       "  0.9689440727233887,\n",
       "  0.9751552939414978,\n",
       "  0.97826087474823,\n",
       "  0.97826087474823,\n",
       "  0.9751552939414978,\n",
       "  0.97826087474823,\n",
       "  0.9813664555549622,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9720497131347656,\n",
       "  0.9720497131347656,\n",
       "  0.9689440727233887,\n",
       "  0.9720497131347656,\n",
       "  0.9751552939414978,\n",
       "  0.9720497131347656,\n",
       "  0.9627329111099243,\n",
       "  0.9689440727233887,\n",
       "  0.9751552939414978,\n",
       "  0.97826087474823,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9720497131347656,\n",
       "  0.97826087474823,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.97826087474823,\n",
       "  0.9720497131347656,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9720497131347656,\n",
       "  0.9751552939414978,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9720497131347656,\n",
       "  0.9720497131347656,\n",
       "  0.9689440727233887,\n",
       "  0.95652174949646,\n",
       "  0.9720497131347656,\n",
       "  0.9751552939414978,\n",
       "  0.9689440727233887,\n",
       "  0.9720497131347656,\n",
       "  0.9813664555549622,\n",
       "  0.9751552939414978,\n",
       "  0.9689440727233887,\n",
       "  0.9751552939414978,\n",
       "  0.9813664555549622,\n",
       "  0.9720497131347656,\n",
       "  0.9658384919166565,\n",
       "  0.9689440727233887,\n",
       "  0.9813664555549622,\n",
       "  0.97826087474823,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9720497131347656,\n",
       "  0.9751552939414978,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9751552939414978,\n",
       "  0.9720497131347656,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9844720363616943,\n",
       "  0.9844720363616943,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9751552939414978,\n",
       "  0.9720497131347656,\n",
       "  0.9689440727233887,\n",
       "  0.9720497131347656,\n",
       "  0.9813664555549622,\n",
       "  0.9720497131347656,\n",
       "  0.9689440727233887,\n",
       "  0.9720497131347656,\n",
       "  0.9751552939414978,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9720497131347656,\n",
       "  0.9720497131347656,\n",
       "  0.9720497131347656,\n",
       "  0.9720497131347656,\n",
       "  0.97826087474823,\n",
       "  0.9689440727233887,\n",
       "  0.9658384919166565,\n",
       "  0.9720497131347656,\n",
       "  0.9844720363616943,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9813664555549622,\n",
       "  0.9751552939414978,\n",
       "  0.9658384919166565,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9720497131347656,\n",
       "  0.9658384919166565,\n",
       "  0.9751552939414978,\n",
       "  0.9813664555549622,\n",
       "  0.9658384919166565,\n",
       "  0.9658384919166565,\n",
       "  0.9813664555549622,\n",
       "  0.9813664555549622,\n",
       "  0.9720497131347656,\n",
       "  0.9689440727233887,\n",
       "  0.9813664555549622,\n",
       "  0.9844720363616943,\n",
       "  0.9689440727233887,\n",
       "  0.9658384919166565,\n",
       "  0.9751552939414978,\n",
       "  0.97826087474823,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9751552939414978,\n",
       "  0.9844720363616943,\n",
       "  0.9689440727233887,\n",
       "  0.9627329111099243,\n",
       "  0.9751552939414978,\n",
       "  0.97826087474823,\n",
       "  0.9720497131347656,\n",
       "  0.9658384919166565,\n",
       "  0.97826087474823,\n",
       "  0.9751552939414978,\n",
       "  0.9689440727233887,\n",
       "  0.9658384919166565,\n",
       "  0.9720497131347656,\n",
       "  0.9720497131347656,\n",
       "  0.9751552939414978,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9813664555549622,\n",
       "  0.9813664555549622,\n",
       "  0.9720497131347656,\n",
       "  0.9720497131347656,\n",
       "  0.9813664555549622,\n",
       "  0.9813664555549622,\n",
       "  0.9813664555549622,\n",
       "  0.9813664555549622,\n",
       "  0.9720497131347656,\n",
       "  0.9720497131347656,\n",
       "  0.97826087474823,\n",
       "  0.97826087474823,\n",
       "  0.9689440727233887,\n",
       "  0.9658384919166565,\n",
       "  0.9689440727233887,\n",
       "  0.9844720363616943,\n",
       "  0.9720497131347656,\n",
       "  0.9720497131347656,\n",
       "  0.9720497131347656,\n",
       "  0.9720497131347656,\n",
       "  0.9720497131347656,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9689440727233887,\n",
       "  0.97826087474823,\n",
       "  0.97826087474823,\n",
       "  0.9720497131347656,\n",
       "  0.9627329111099243,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9844720363616943,\n",
       "  0.9813664555549622,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9813664555549622,\n",
       "  0.9751552939414978,\n",
       "  0.9658384919166565,\n",
       "  0.9689440727233887,\n",
       "  0.9844720363616943,\n",
       "  0.9751552939414978,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9813664555549622,\n",
       "  0.9751552939414978,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9751552939414978,\n",
       "  0.9844720363616943,\n",
       "  0.9720497131347656,\n",
       "  0.9720497131347656,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9751552939414978,\n",
       "  0.9844720363616943,\n",
       "  0.97826087474823,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9844720363616943,\n",
       "  0.97826087474823,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9844720363616943,\n",
       "  0.9844720363616943,\n",
       "  0.9689440727233887,\n",
       "  0.9658384919166565,\n",
       "  0.9751552939414978,\n",
       "  0.97826087474823,\n",
       "  0.9720497131347656,\n",
       "  0.9689440727233887,\n",
       "  0.9844720363616943,\n",
       "  0.97826087474823,\n",
       "  0.9751552939414978,\n",
       "  0.9720497131347656,\n",
       "  0.97826087474823,\n",
       "  0.9813664555549622,\n",
       "  0.9720497131347656,\n",
       "  0.9658384919166565,\n",
       "  0.97826087474823,\n",
       "  0.9844720363616943,\n",
       "  0.9751552939414978,\n",
       "  0.9720497131347656,\n",
       "  0.97826087474823,\n",
       "  0.97826087474823,\n",
       "  0.9689440727233887,\n",
       "  0.9720497131347656,\n",
       "  0.9720497131347656,\n",
       "  0.9751552939414978,\n",
       "  0.9658384919166565,\n",
       "  0.9689440727233887,\n",
       "  0.97826087474823,\n",
       "  0.9844720363616943,\n",
       "  0.9689440727233887,\n",
       "  0.9720497131347656,\n",
       "  0.9844720363616943,\n",
       "  0.9844720363616943,\n",
       "  0.9813664555549622,\n",
       "  0.9844720363616943,\n",
       "  0.9844720363616943,\n",
       "  0.9844720363616943,\n",
       "  0.9720497131347656]}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history #val_loss :(테스트셋으로 실험한결과의 오차값), val_accuracy : (학습셋으로 측정한 정확도 값)이 들어있는 것을 확인하자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_vloss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_acc=history.history['val_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# x값을 지정하고 정확도를 파란색으로, 오차를 빨간색으로 표시\n",
    "x_len = numpy.arange(len(y_acc))\n",
    "plt.plot(x_len, y_vloss, \"o\", c=\"red\", markersize=3)\n",
    "plt.plot(x_len, y_acc, \"o\", c=\"blue\", markersize=3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#테스트 결과는 어느정도 지나면 과적합이 일어난다.\n",
    "# EarlyStopping()함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 회귀 문제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 집값 알아보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('example/080228-master/deeplearning/dataset/housing.csv', delim_whitespace=True, header=None) #data 분류가 ,가 아니라 whitespace기 때문에, 그것으로 분류한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 506 entries, 0 to 505\n",
      "Data columns (total 14 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   0       506 non-null    float64\n",
      " 1   1       506 non-null    float64\n",
      " 2   2       506 non-null    float64\n",
      " 3   3       506 non-null    int64  \n",
      " 4   4       506 non-null    float64\n",
      " 5   5       506 non-null    float64\n",
      " 6   6       506 non-null    float64\n",
      " 7   7       506 non-null    float64\n",
      " 8   8       506 non-null    int64  \n",
      " 9   9       506 non-null    float64\n",
      " 10  10      506 non-null    float64\n",
      " 11  11      506 non-null    float64\n",
      " 12  12      506 non-null    float64\n",
      " 13  13      506 non-null    float64\n",
      "dtypes: float64(12), int64(2)\n",
      "memory usage: 55.5 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        0     1     2   3      4      5     6       7   8      9     10  \\\n",
      "0  0.00632  18.0  2.31   0  0.538  6.575  65.2  4.0900   1  296.0  15.3   \n",
      "1  0.02731   0.0  7.07   0  0.469  6.421  78.9  4.9671   2  242.0  17.8   \n",
      "2  0.02729   0.0  7.07   0  0.469  7.185  61.1  4.9671   2  242.0  17.8   \n",
      "3  0.03237   0.0  2.18   0  0.458  6.998  45.8  6.0622   3  222.0  18.7   \n",
      "4  0.06905   0.0  2.18   0  0.458  7.147  54.2  6.0622   3  222.0  18.7   \n",
      "\n",
      "       11    12    13  \n",
      "0  396.90  4.98  24.0  \n",
      "1  396.90  9.14  21.6  \n",
      "2  392.83  4.03  34.7  \n",
      "3  394.63  2.94  33.4  \n",
      "4  396.90  5.33  36.2  \n"
     ]
    }
   ],
   "source": [
    "print(df.head(5))"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  -  선형 회귀 데이터는 참과 거짓을 구분할 필요가 없어 출력층에 활성화 함수를 지정할 필요가 없다\n",
    "  -  분류가 아닌 값을 직접 비교하기 때문에, 마지막의 활성화함수가 필요하지 않다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = df.values\n",
    "X = dataset[:,0:13].astype(float)\n",
    "Y = dataset[:,13]\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(30, input_dim=13, activation='relu'))\n",
    "model.add(Dense(6, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_56 (Dense)             (None, 30)                420       \n",
      "_________________________________________________________________\n",
      "dense_57 (Dense)             (None, 6)                 186       \n",
      "_________________________________________________________________\n",
      "dense_58 (Dense)             (None, 1)                 7         \n",
      "=================================================================\n",
      "Total params: 613\n",
      "Trainable params: 613\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 1층 모델 : 가중치 파라미터 13*30 = 390개/ 바이오스 파라미터 30 개, 합 = 420개\n",
    "# 2층 모델 : 가중치 파라미터 30*6 = 180개/ 바이오스 파라미터 6 개, 합 =  186개\n",
    "# 3층 모델 : 가중치 파라미터 6*1 = 6개 / 바이오스 파라미터 1개, 합 = 7개\n",
    "# 3개의 모델 총 420 + 186 + 7 = 613개\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 15077.5957\n",
      "Epoch 2/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 11726.3672\n",
      "Epoch 3/200\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 8898.9189\n",
      "Epoch 4/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 6684.2354\n",
      "Epoch 5/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 4954.5854\n",
      "Epoch 6/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 3593.1023\n",
      "Epoch 7/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 2605.0811\n",
      "Epoch 8/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 1851.1051\n",
      "Epoch 9/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 1265.8185\n",
      "Epoch 10/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 827.3121\n",
      "Epoch 11/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 517.9334\n",
      "Epoch 12/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 334.4679\n",
      "Epoch 13/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 234.3185\n",
      "Epoch 14/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 177.4533\n",
      "Epoch 15/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 148.5884\n",
      "Epoch 16/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 131.2672\n",
      "Epoch 17/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 116.8794\n",
      "Epoch 18/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 108.0435\n",
      "Epoch 19/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 101.6754\n",
      "Epoch 20/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 96.9686\n",
      "Epoch 21/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 93.9309\n",
      "Epoch 22/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 92.1549\n",
      "Epoch 23/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 89.4605\n",
      "Epoch 24/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 86.7813\n",
      "Epoch 25/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 84.5789\n",
      "Epoch 26/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 82.5154\n",
      "Epoch 27/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 80.4267\n",
      "Epoch 28/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 78.9477\n",
      "Epoch 29/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 77.4740\n",
      "Epoch 30/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 76.0199\n",
      "Epoch 31/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 74.6426\n",
      "Epoch 32/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 73.3621\n",
      "Epoch 33/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 72.0656\n",
      "Epoch 34/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 70.8398\n",
      "Epoch 35/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 69.8248\n",
      "Epoch 36/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 68.7812\n",
      "Epoch 37/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 68.0477\n",
      "Epoch 38/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 67.2840\n",
      "Epoch 39/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 66.6532\n",
      "Epoch 40/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 66.1264\n",
      "Epoch 41/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 65.5582\n",
      "Epoch 42/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 65.0409\n",
      "Epoch 43/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 64.4399\n",
      "Epoch 44/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 64.0828\n",
      "Epoch 45/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 63.7043\n",
      "Epoch 46/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 63.2821\n",
      "Epoch 47/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 62.8548\n",
      "Epoch 48/200\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 62.4719\n",
      "Epoch 49/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 62.0961\n",
      "Epoch 50/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 61.7771\n",
      "Epoch 51/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 61.3905\n",
      "Epoch 52/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 61.0846\n",
      "Epoch 53/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 60.8692\n",
      "Epoch 54/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 60.5264\n",
      "Epoch 55/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 60.2537\n",
      "Epoch 56/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 60.0384\n",
      "Epoch 57/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 59.7211\n",
      "Epoch 58/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 59.4769\n",
      "Epoch 59/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 59.2930\n",
      "Epoch 60/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 59.0212\n",
      "Epoch 61/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 58.8848\n",
      "Epoch 62/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 58.6370\n",
      "Epoch 63/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 58.3807\n",
      "Epoch 64/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 58.1372\n",
      "Epoch 65/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 57.9284\n",
      "Epoch 66/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 57.7276\n",
      "Epoch 67/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 57.5212\n",
      "Epoch 68/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 57.2901\n",
      "Epoch 69/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 57.0900\n",
      "Epoch 70/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 56.8924\n",
      "Epoch 71/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 56.6615\n",
      "Epoch 72/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 56.4734\n",
      "Epoch 73/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 29.64 - 0s 3ms/step - loss: 56.2568\n",
      "Epoch 74/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 56.0379\n",
      "Epoch 75/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 55.8535\n",
      "Epoch 76/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 55.6431\n",
      "Epoch 77/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 55.4143\n",
      "Epoch 78/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 55.1990\n",
      "Epoch 79/200\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 55.0649\n",
      "Epoch 80/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 54.8362\n",
      "Epoch 81/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 54.6859\n",
      "Epoch 82/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 54.4078\n",
      "Epoch 83/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 54.2461\n",
      "Epoch 84/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 54.0905\n",
      "Epoch 85/200\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 53.8037\n",
      "Epoch 86/200\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 53.5547\n",
      "Epoch 87/200\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 53.4072\n",
      "Epoch 88/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 53.2219\n",
      "Epoch 89/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 52.9790\n",
      "Epoch 90/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 52.7919\n",
      "Epoch 91/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 52.5774\n",
      "Epoch 92/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 52.3909\n",
      "Epoch 93/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 52.1115\n",
      "Epoch 94/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 51.8813\n",
      "Epoch 95/200\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 51.8911\n",
      "Epoch 96/200\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 51.5751\n",
      "Epoch 97/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 51.3654\n",
      "Epoch 98/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 51.2676\n",
      "Epoch 99/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 51.0828\n",
      "Epoch 100/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 50.8632\n",
      "Epoch 101/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 50.7075\n",
      "Epoch 102/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 50.5911\n",
      "Epoch 103/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 50.3947\n",
      "Epoch 104/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 50.2335\n",
      "Epoch 105/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 50.1017\n",
      "Epoch 106/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 50.0792\n",
      "Epoch 107/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 49.8313\n",
      "Epoch 108/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 49.6830\n",
      "Epoch 109/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 49.5195\n",
      "Epoch 110/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 49.3867\n",
      "Epoch 111/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 49.1625\n",
      "Epoch 112/200\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 49.0382\n",
      "Epoch 113/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 48.8876\n",
      "Epoch 114/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 48.7522\n",
      "Epoch 115/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 48.5951\n",
      "Epoch 116/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 48.4832\n",
      "Epoch 117/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 48.3074\n",
      "Epoch 118/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 48.1490\n",
      "Epoch 119/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 48.0709\n",
      "Epoch 120/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 47.9283\n",
      "Epoch 121/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 47.7240\n",
      "Epoch 122/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 47.5944\n",
      "Epoch 123/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 47.4807\n",
      "Epoch 124/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 47.3373\n",
      "Epoch 125/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 47.1599\n",
      "Epoch 126/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 47.1497\n",
      "Epoch 127/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 46.9190\n",
      "Epoch 128/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 46.7520\n",
      "Epoch 129/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 46.6755\n",
      "Epoch 130/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 46.5913\n",
      "Epoch 131/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 46.4375\n",
      "Epoch 132/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 46.2264\n",
      "Epoch 133/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 46.1275\n",
      "Epoch 134/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 46.0365\n",
      "Epoch 135/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 45.8991\n",
      "Epoch 136/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 45.7953\n",
      "Epoch 137/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 45.6171\n",
      "Epoch 138/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 45.5568\n",
      "Epoch 139/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 45.4640\n",
      "Epoch 140/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 45.2819\n",
      "Epoch 141/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 45.1161\n",
      "Epoch 142/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 45.0522\n",
      "Epoch 143/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 44.9433\n",
      "Epoch 144/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 44.8091\n",
      "Epoch 145/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 44.6774\n",
      "Epoch 146/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 44.5684\n",
      "Epoch 147/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 44.4297\n",
      "Epoch 148/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 44.3368\n",
      "Epoch 149/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 44.2557\n",
      "Epoch 150/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 44.0945\n",
      "Epoch 151/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 44.0676\n",
      "Epoch 152/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 43.9817\n",
      "Epoch 153/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 43.7687\n",
      "Epoch 154/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 43.6434\n",
      "Epoch 155/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 43.6107\n",
      "Epoch 156/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 43.4130\n",
      "Epoch 157/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 43.2760\n",
      "Epoch 158/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 43.1886\n",
      "Epoch 159/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 43.0319\n",
      "Epoch 160/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 42.9618\n",
      "Epoch 161/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 42.8363\n",
      "Epoch 162/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 42.7409\n",
      "Epoch 163/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 42.6688\n",
      "Epoch 164/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 42.5182\n",
      "Epoch 165/200\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 42.3994\n",
      "Epoch 166/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 42.2715\n",
      "Epoch 167/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 42.1766\n",
      "Epoch 168/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 42.0081\n",
      "Epoch 169/200\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 41.9466\n",
      "Epoch 170/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 41.8427\n",
      "Epoch 171/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 41.7052\n",
      "Epoch 172/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 41.6006\n",
      "Epoch 173/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 41.5036\n",
      "Epoch 174/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 41.3587\n",
      "Epoch 175/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 41.3462\n",
      "Epoch 176/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 41.1421\n",
      "Epoch 177/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 41.0902\n",
      "Epoch 178/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 41.1251\n",
      "Epoch 179/200\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 40.8140\n",
      "Epoch 180/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 40.7037\n",
      "Epoch 181/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 40.6603\n",
      "Epoch 182/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 40.4587\n",
      "Epoch 183/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 40.4881\n",
      "Epoch 184/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 40.3651\n",
      "Epoch 185/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 40.2174\n",
      "Epoch 186/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 40.2209\n",
      "Epoch 187/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 40.0242\n",
      "Epoch 188/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 39.9019\n",
      "Epoch 189/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 39.7434\n",
      "Epoch 190/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 39.7207\n",
      "Epoch 191/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 39.7158\n",
      "Epoch 192/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 39.5618\n",
      "Epoch 193/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 39.5379\n",
      "Epoch 194/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 39.3114\n",
      "Epoch 195/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 39.2556\n",
      "Epoch 196/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 39.0538\n",
      "Epoch 197/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 38.9570\n",
      "Epoch 198/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 38.8584\n",
      "Epoch 199/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 38.7598\n",
      "Epoch 200/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 38.6848\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x285282d6288>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, epochs=200, batch_size=100)#배치가 작으면, 특잇값들 때문에 loss에도 영향을 준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_prediction = model.predict(X_test).flatten() # predict : 학습이 다 된 모델에 테스트 값을 넣어 결과를 도출하는 함수 / flatten() : 데이터 배열이 몇차원이던간에, 1차원으로 줄여주는 함수."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([27.256178 , 19.810131 , 13.655229 , 25.42116  , 13.645573 ,\n",
       "       24.788548 , 23.251385 , 19.127535 , 27.632849 , 14.704895 ,\n",
       "       13.552894 , 22.988932 , 14.729843 , 20.653463 , 17.365917 ,\n",
       "       18.57893  , 10.711501 , 29.462286 , 19.826971 , 27.222221 ,\n",
       "       16.671936 , 32.370335 , 19.82445  ,  8.70704  , 25.429167 ,\n",
       "       32.271816 , 22.153875 , 22.25573  , 24.759542 , 22.739569 ,\n",
       "       32.944893 , 21.300095 , 24.398632 , 29.197212 , 19.05807  ,\n",
       "       22.249212 , 30.442572 , 12.208619 , 12.586983 , 11.338567 ,\n",
       "       20.661514 , 28.363438 , 25.038116 , 23.749857 , 24.152733 ,\n",
       "       28.506851 , 28.369566 , 23.823137 , 22.73484  , 27.04038  ,\n",
       "        8.855124 , 22.082651 , 21.852095 , 34.131107 , 19.270636 ,\n",
       "       21.75644  , 27.482044 , 27.7384   , 28.658567 , 35.99109  ,\n",
       "       17.089098 , 38.729378 , 25.314766 , 20.282816 , 22.654081 ,\n",
       "       11.594791 , 25.075575 , 31.693197 , 25.67836  , 13.971081 ,\n",
       "       19.62271  , 30.445543 , 17.180584 , 13.181005 , 36.271744 ,\n",
       "       23.96587  , 19.525768 , 31.397032 , 13.583186 , 15.048692 ,\n",
       "       22.3751   , 22.69384  , 24.701925 , 26.806969 , 18.999752 ,\n",
       "       26.163208 , 24.048906 , 20.630812 , 24.795378 , 15.484034 ,\n",
       "       30.463303 , 24.546396 , 34.487087 , 20.499342 , 23.235903 ,\n",
       "       25.416777 , 27.113445 , 20.845713 , 22.918745 , 16.56571  ,\n",
       "       31.737885 , 28.366005 , 25.373846 , 22.641981 , 25.607485 ,\n",
       "       20.397795 , 25.494064 , 25.28734  , 26.556522 , 24.773577 ,\n",
       "       20.707092 , 22.101229 ,  6.4861126, 22.02627  , 37.22995  ,\n",
       "       20.199366 , 26.465145 , 24.261703 , 25.187784 , 28.11135  ,\n",
       "       18.625082 , 23.638865 , 10.159908 , 37.121746 , 28.84851  ,\n",
       "       27.31197  , 26.189    , 25.661163 , 18.650124 , 20.082584 ,\n",
       "       20.171843 , 14.031874 , 19.024672 , 21.938536 , 22.775948 ,\n",
       "       18.33903  , 17.84691  , 18.16215  , 22.017254 , 20.183954 ,\n",
       "       11.77413  , 10.573087 ,  9.72729  , 30.784967 , 18.172794 ,\n",
       "       25.345867 , 22.783764 , 20.079853 , 23.879696 , 22.155262 ,\n",
       "       19.791956 , 25.141586 ], dtype=float32)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "실제가격: 50.000, 예상가격: 27.256\n",
      "실제가격: 11.300, 예상가격: 19.810\n",
      "실제가격: 8.700, 예상가격: 13.655\n",
      "실제가격: 27.500, 예상가격: 25.421\n",
      "실제가격: 9.500, 예상가격: 13.646\n",
      "실제가격: 22.600, 예상가격: 24.789\n",
      "실제가격: 23.100, 예상가격: 23.251\n",
      "실제가격: 17.800, 예상가격: 19.128\n",
      "실제가격: 20.700, 예상가격: 27.633\n",
      "실제가격: 14.800, 예상가격: 14.705\n",
      "실제가격: 7.200, 예상가격: 13.553\n",
      "실제가격: 20.600, 예상가격: 22.989\n",
      "실제가격: 8.300, 예상가격: 14.730\n",
      "실제가격: 18.000, 예상가격: 20.653\n",
      "실제가격: 13.600, 예상가격: 17.366\n",
      "실제가격: 22.500, 예상가격: 18.579\n",
      "실제가격: 11.700, 예상가격: 10.712\n",
      "실제가격: 36.400, 예상가격: 29.462\n",
      "실제가격: 50.000, 예상가격: 19.827\n",
      "실제가격: 48.300, 예상가격: 27.222\n",
      "실제가격: 19.100, 예상가격: 16.672\n",
      "실제가격: 36.200, 예상가격: 32.370\n",
      "실제가격: 14.100, 예상가격: 19.824\n",
      "실제가격: 10.900, 예상가격: 8.707\n",
      "실제가격: 23.700, 예상가격: 25.429\n",
      "실제가격: 30.100, 예상가격: 32.272\n",
      "실제가격: 20.500, 예상가격: 22.154\n",
      "실제가격: 21.200, 예상가격: 22.256\n",
      "실제가격: 20.400, 예상가격: 24.760\n",
      "실제가격: 23.100, 예상가격: 22.740\n"
     ]
    }
   ],
   "source": [
    "for i in range(30):\n",
    "    label = Y_test[i]\n",
    "    prediction = Y_prediction[i]\n",
    "    print(\"실제가격: {:.3f}, 예상가격: {:.3f}\".format(label, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 이미지 인식의 꽃, CNN\n",
    "  - 미국 국립표준기술원(NIST)이 고등학생과 인구조사국 직원 등이 쓴 손글씨를 이용해 만든 데이터를 이용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras import utils\n",
    "\n",
    "import numpy\n",
    "import sys\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(X_train, Y_class_train), (X_test, Y_class_test) = mnist.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습셋 이미지 수 : 60000개\n",
      "테스트셋 이미지 수 : 10000개\n"
     ]
    }
   ],
   "source": [
    "print(\"학습셋 이미지 수 : %d개\" % (X_train.shape[0]))\n",
    "print(\"테스트셋 이미지 수 : %d개\" % (X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(X_train[0], cmap='Greys')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
      "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
      "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
      "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
      "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
      "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t3\t18\t18\t18\t126\t136\t175\t26\t166\t255\t247\t127\t0\t0\t0\t0\t\n",
      "0\t0\t0\t0\t0\t0\t0\t0\t30\t36\t94\t154\t170\t253\t253\t253\t253\t253\t225\t172\t253\t242\t195\t64\t0\t0\t0\t0\t\n",
      "0\t0\t0\t0\t0\t0\t0\t49\t238\t253\t253\t253\t253\t253\t253\t253\t253\t251\t93\t82\t82\t56\t39\t0\t0\t0\t0\t0\t\n",
      "0\t0\t0\t0\t0\t0\t0\t18\t219\t253\t253\t253\t253\t253\t198\t182\t247\t241\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
      "0\t0\t0\t0\t0\t0\t0\t0\t80\t156\t107\t253\t253\t205\t11\t0\t43\t154\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
      "0\t0\t0\t0\t0\t0\t0\t0\t0\t14\t1\t154\t253\t90\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
      "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t139\t253\t190\t2\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
      "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t11\t190\t253\t70\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
      "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t35\t241\t225\t160\t108\t1\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
      "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t81\t240\t253\t253\t119\t25\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
      "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t45\t186\t253\t253\t150\t27\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
      "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t16\t93\t252\t253\t187\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
      "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t249\t253\t249\t64\t0\t0\t0\t0\t0\t0\t0\t\n",
      "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t46\t130\t183\t253\t253\t207\t2\t0\t0\t0\t0\t0\t0\t0\t\n",
      "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t39\t148\t229\t253\t253\t253\t250\t182\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
      "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t24\t114\t221\t253\t253\t253\t253\t201\t78\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
      "0\t0\t0\t0\t0\t0\t0\t0\t23\t66\t213\t253\t253\t253\t253\t198\t81\t2\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
      "0\t0\t0\t0\t0\t0\t18\t171\t219\t253\t253\t253\t253\t195\t80\t9\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
      "0\t0\t0\t0\t55\t172\t226\t253\t253\t253\t253\t244\t133\t11\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
      "0\t0\t0\t0\t136\t253\t253\t253\t212\t135\t132\t16\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
      "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
      "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
      "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n"
     ]
    }
   ],
   "source": [
    "for x in X_train[0]:\n",
    "    for i in x:\n",
    "        sys.stdout.write('%d\\t' % i)\n",
    "    sys.stdout.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], 784) # 가로 28, 세로 28의 2차원 배열을 784개의 1차원 배열로 바꿔주기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float64')\n",
    "X_train = X_train / 255 #케라스는 데이터를 0에서 1 사이의 값으로 변환한 다음 구동할 때 최적의 성능을 보여 255로 나눠 0~1사이 값으로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.reshape(X_test.shape[0], 784).astype('float64') / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class : 5 \n"
     ]
    }
   ],
   "source": [
    "print(\"class : %d \" % (Y_class_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "Y_train = utils.to_categorical(Y_class_train, 10) #딥러닝의 분류 문제를 해결하기위해 원 - 핫 - 인코딩 방식을 적용해주기,\n",
    "Y_test = utils.to_categorical(Y_class_test, 10) # 즉 0또는 1로만 이루어진 벡터로 값 수정하기\n",
    "\n",
    "print(Y_train[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_dim=784, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1층 모델 : 가중치 파라미터 784*512 = 401,408개/ 바이오스 파라미터 512 개, 합 = 401,920개\n",
    "# 2층 모델 : 가중치 파라미터 512*10 = 5120개/ 바이오스 파라미터 10 개, 합 = 5,130개\n",
    "\n",
    "# 2개의 모델 총 401,920 + 5,130 = 407,050개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_61 (Dense)             (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_62 (Dense)             (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 407,050\n",
      "Trainable params: 407,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "import os\n",
    "# 모델 최적화 설정\n",
    "MODEL_DIR = './model/'\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.mkdir(MODEL_DIR)\n",
    "\n",
    "modelpath=\"./model/{epoch:02d}-{val_loss:.4f}.hdf5\"\n",
    "checkpointer = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=1, save_best_only=True)\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.14711, saving model to ./model\\01-0.1471.hdf5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.14711 to 0.10306, saving model to ./model\\02-0.1031.hdf5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.10306 to 0.08953, saving model to ./model\\03-0.0895.hdf5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.08953 to 0.07822, saving model to ./model\\04-0.0782.hdf5\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.07822 to 0.07204, saving model to ./model\\05-0.0720.hdf5\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.07204 to 0.06765, saving model to ./model\\06-0.0677.hdf5\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.06765 to 0.06715, saving model to ./model\\07-0.0672.hdf5\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.06715 to 0.06245, saving model to ./model\\08-0.0624.hdf5\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.06245\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.06245\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.06245\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.06245\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.06245\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.06245\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.06245\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.06245\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.06245\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.06245\n"
     ]
    }
   ],
   "source": [
    "# 모델의 실행\n",
    "history = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=30, batch_size=200, verbose=0, callbacks=[early_stopping_callback,checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0767 - accuracy: 0.9800\n",
      "\n",
      " Test Accuracy: 0.9800\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n Test Accuracy: %.4f\" % (model.evaluate(X_test, Y_test)[1]))\n",
    "y_vloss = history.history['val_loss']\n",
    "  \n",
    "# 학습셋의 오차\n",
    "y_loss = history.history['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2852a613a48>]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 그래프로 표현\n",
    "x_len = numpy.arange(len(y_loss))\n",
    "plt.plot(x_len, y_vloss, marker='.', c=\"red\", label='Testset_loss')\n",
    "plt.plot(x_len, y_loss, marker='.', c=\"blue\", label='Trainset_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('example/080228-master/deeplearning/dataset/housing.csv', delim_whitespace=True, header=None) #data 분류가 ,가 아니라 whitespace기 때문에, 그것으로 분류한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 506 entries, 0 to 505\n",
      "Data columns (total 14 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   0       506 non-null    float64\n",
      " 1   1       506 non-null    float64\n",
      " 2   2       506 non-null    float64\n",
      " 3   3       506 non-null    int64  \n",
      " 4   4       506 non-null    float64\n",
      " 5   5       506 non-null    float64\n",
      " 6   6       506 non-null    float64\n",
      " 7   7       506 non-null    float64\n",
      " 8   8       506 non-null    int64  \n",
      " 9   9       506 non-null    float64\n",
      " 10  10      506 non-null    float64\n",
      " 11  11      506 non-null    float64\n",
      " 12  12      506 non-null    float64\n",
      " 13  13      506 non-null    float64\n",
      "dtypes: float64(12), int64(2)\n",
      "memory usage: 55.5 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        0     1     2   3      4      5     6       7   8      9     10  \\\n",
      "0  0.00632  18.0  2.31   0  0.538  6.575  65.2  4.0900   1  296.0  15.3   \n",
      "1  0.02731   0.0  7.07   0  0.469  6.421  78.9  4.9671   2  242.0  17.8   \n",
      "2  0.02729   0.0  7.07   0  0.469  7.185  61.1  4.9671   2  242.0  17.8   \n",
      "3  0.03237   0.0  2.18   0  0.458  6.998  45.8  6.0622   3  222.0  18.7   \n",
      "4  0.06905   0.0  2.18   0  0.458  7.147  54.2  6.0622   3  222.0  18.7   \n",
      "\n",
      "       11    12    13  \n",
      "0  396.90  4.98  24.0  \n",
      "1  396.90  9.14  21.6  \n",
      "2  392.83  4.03  34.7  \n",
      "3  394.63  2.94  33.4  \n",
      "4  396.90  5.33  36.2  \n"
     ]
    }
   ],
   "source": [
    "print(df.head(5))"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  -  선형 화귀 데이터는 참과 거짓을 구분할 필요가 없어 출력층에 활성화 함수를 지정할 필요가 없다\n",
    "  -  분류가 아닌 값을 직접 비교하기 때문에, 마지막의 활성화함수가 필요하지 않다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = df.values\n",
    "X = dataset[:,0:13].astype(float)\n",
    "Y = dataset[:,13]\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(30, input_dim=13, activation='relu'))\n",
    "model.add(Dense(6, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_20\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_63 (Dense)             (None, 30)                420       \n",
      "_________________________________________________________________\n",
      "dense_64 (Dense)             (None, 6)                 186       \n",
      "_________________________________________________________________\n",
      "dense_65 (Dense)             (None, 1)                 7         \n",
      "=================================================================\n",
      "Total params: 613\n",
      "Trainable params: 613\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 1층 모델 : 가중치 파라미터 13*30 = 390개/ 바이오스 파라미터 30 개, 합 = 420개\n",
    "# 2층 모델 : 가중치 파라미터 30*6 = 180개/ 바이오스 파라미터 6 개, 합 =  186개\n",
    "# 3층 모델 : 가중치 파라미터 6*1 = 6개 / 바이오스 파라미터 1개, 합 = 7개\n",
    "# 3개의 모델 총 420 + 186 + 7 = 613개\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 360.5990\n",
      "Epoch 2/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 221.1380\n",
      "Epoch 3/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 149.0070\n",
      "Epoch 4/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 114.7813\n",
      "Epoch 5/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 112.8082\n",
      "Epoch 6/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 105.1701\n",
      "Epoch 7/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 104.0931\n",
      "Epoch 8/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 94.9899\n",
      "Epoch 9/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 87.9598\n",
      "Epoch 10/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 83.6289\n",
      "Epoch 11/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 79.9810\n",
      "Epoch 12/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 78.9886\n",
      "Epoch 13/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 76.6508\n",
      "Epoch 14/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 74.8838\n",
      "Epoch 15/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 72.2444\n",
      "Epoch 16/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 71.4569\n",
      "Epoch 17/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 70.4477\n",
      "Epoch 18/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 69.1025\n",
      "Epoch 19/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 68.6260\n",
      "Epoch 20/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 67.3024\n",
      "Epoch 21/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 66.7720\n",
      "Epoch 22/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 65.7892\n",
      "Epoch 23/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 65.3913\n",
      "Epoch 24/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 64.3821\n",
      "Epoch 25/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 63.7646\n",
      "Epoch 26/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 63.0830\n",
      "Epoch 27/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 62.4143\n",
      "Epoch 28/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 61.9188\n",
      "Epoch 29/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 61.3359\n",
      "Epoch 30/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 61.0295\n",
      "Epoch 31/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 60.5040\n",
      "Epoch 32/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 60.0448\n",
      "Epoch 33/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 59.6090\n",
      "Epoch 34/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 59.4466\n",
      "Epoch 35/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 58.7396\n",
      "Epoch 36/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 58.9254\n",
      "Epoch 37/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 58.2744\n",
      "Epoch 38/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 58.4498\n",
      "Epoch 39/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 64.86 - 0s 4ms/step - loss: 57.4635\n",
      "Epoch 40/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 57.7138\n",
      "Epoch 41/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 56.9004\n",
      "Epoch 42/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 56.7158\n",
      "Epoch 43/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 56.2698\n",
      "Epoch 44/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 56.1906\n",
      "Epoch 45/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 55.9137\n",
      "Epoch 46/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 55.1539\n",
      "Epoch 47/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 55.2283\n",
      "Epoch 48/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 54.3409\n",
      "Epoch 49/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 54.4590\n",
      "Epoch 50/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 53.8315\n",
      "Epoch 51/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 54.1014\n",
      "Epoch 52/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 53.3214\n",
      "Epoch 53/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 53.0668\n",
      "Epoch 54/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 52.6458\n",
      "Epoch 55/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 52.6036\n",
      "Epoch 56/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 52.0137\n",
      "Epoch 57/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 52.0674\n",
      "Epoch 58/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 51.4312\n",
      "Epoch 59/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 51.3410\n",
      "Epoch 60/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 50.8426\n",
      "Epoch 61/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 50.6955\n",
      "Epoch 62/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 50.2030\n",
      "Epoch 63/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 49.9390\n",
      "Epoch 64/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 49.6558\n",
      "Epoch 65/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 49.4345\n",
      "Epoch 66/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 49.2206\n",
      "Epoch 67/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 48.9676\n",
      "Epoch 68/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 48.5882\n",
      "Epoch 69/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 48.1776\n",
      "Epoch 70/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 47.9312\n",
      "Epoch 71/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 47.6454\n",
      "Epoch 72/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 47.6117\n",
      "Epoch 73/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 46.7435\n",
      "Epoch 74/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 47.1650\n",
      "Epoch 75/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 46.4718\n",
      "Epoch 76/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 46.9771\n",
      "Epoch 77/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 45.9389\n",
      "Epoch 78/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 45.6579\n",
      "Epoch 79/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 46.3936\n",
      "Epoch 80/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 45.8726\n",
      "Epoch 81/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 45.0265\n",
      "Epoch 82/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 45.3808\n",
      "Epoch 83/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 44.5073\n",
      "Epoch 84/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 44.1791\n",
      "Epoch 85/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 44.4459\n",
      "Epoch 86/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 45.2054\n",
      "Epoch 87/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 43.4566\n",
      "Epoch 88/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 44.3889\n",
      "Epoch 89/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 43.2232\n",
      "Epoch 90/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 42.8816\n",
      "Epoch 91/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 42.7327\n",
      "Epoch 92/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 42.6702\n",
      "Epoch 93/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 42.1791\n",
      "Epoch 94/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 42.0885\n",
      "Epoch 95/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 41.7792\n",
      "Epoch 96/200\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 41.6724\n",
      "Epoch 97/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 41.5272\n",
      "Epoch 98/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 41.6766\n",
      "Epoch 99/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 41.2539\n",
      "Epoch 100/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 41.5086\n",
      "Epoch 101/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 40.5705\n",
      "Epoch 102/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 41.5454\n",
      "Epoch 103/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 2ms/step - loss: 40.9676\n",
      "Epoch 104/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 39.8126\n",
      "Epoch 105/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 40.5862\n",
      "Epoch 106/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 41.5184\n",
      "Epoch 107/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 40.2374\n",
      "Epoch 108/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 40.2156\n",
      "Epoch 109/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 40.0170\n",
      "Epoch 110/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 39.1070\n",
      "Epoch 111/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 39.1020\n",
      "Epoch 112/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 39.4487\n",
      "Epoch 113/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 39.9012\n",
      "Epoch 114/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 39.1670\n",
      "Epoch 115/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 38.8664\n",
      "Epoch 116/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 38.5182\n",
      "Epoch 117/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 38.2797\n",
      "Epoch 118/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 38.1478\n",
      "Epoch 119/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 38.5653\n",
      "Epoch 120/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 38.4072\n",
      "Epoch 121/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 37.2796\n",
      "Epoch 122/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 37.6810\n",
      "Epoch 123/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 38.1024\n",
      "Epoch 124/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 38.1259\n",
      "Epoch 125/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 37.2905\n",
      "Epoch 126/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 36.9127\n",
      "Epoch 127/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 36.9510\n",
      "Epoch 128/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 36.5680\n",
      "Epoch 129/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 36.3855\n",
      "Epoch 130/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 36.4685\n",
      "Epoch 131/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 36.2771\n",
      "Epoch 132/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 36.1342\n",
      "Epoch 133/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 35.9832\n",
      "Epoch 134/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 36.1279\n",
      "Epoch 135/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 37.1982\n",
      "Epoch 136/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 36.2036\n",
      "Epoch 137/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 36.5036\n",
      "Epoch 138/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 36.5892\n",
      "Epoch 139/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 35.4596\n",
      "Epoch 140/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 35.3382\n",
      "Epoch 141/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 35.1703\n",
      "Epoch 142/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 35.0297\n",
      "Epoch 143/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 35.2074\n",
      "Epoch 144/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 35.3894\n",
      "Epoch 145/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 34.8828\n",
      "Epoch 146/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 34.4671\n",
      "Epoch 147/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 35.3859\n",
      "Epoch 148/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 34.8119\n",
      "Epoch 149/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 34.2941\n",
      "Epoch 150/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 34.0506\n",
      "Epoch 151/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 34.8490\n",
      "Epoch 152/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 35.9783\n",
      "Epoch 153/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 33.7485\n",
      "Epoch 154/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 34.3798\n",
      "Epoch 155/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 34.2156\n",
      "Epoch 156/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 33.7287\n",
      "Epoch 157/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 33.3898\n",
      "Epoch 158/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 33.4094\n",
      "Epoch 159/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 33.5389\n",
      "Epoch 160/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 33.3196\n",
      "Epoch 161/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 33.7675\n",
      "Epoch 162/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 35.8227\n",
      "Epoch 163/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 34.0445\n",
      "Epoch 164/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 33.3439\n",
      "Epoch 165/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 34.0050\n",
      "Epoch 166/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 33.3020\n",
      "Epoch 167/200\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 32.6291\n",
      "Epoch 168/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 32.3405\n",
      "Epoch 169/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 32.3695\n",
      "Epoch 170/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 32.5202\n",
      "Epoch 171/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 32.2123\n",
      "Epoch 172/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 32.1918\n",
      "Epoch 173/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 33.1744\n",
      "Epoch 174/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 33.2440\n",
      "Epoch 175/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 33.6838\n",
      "Epoch 176/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 32.0710\n",
      "Epoch 177/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 32.1227\n",
      "Epoch 178/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 33.1348\n",
      "Epoch 179/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 32.7325\n",
      "Epoch 180/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 33.6681\n",
      "Epoch 181/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 33.1102\n",
      "Epoch 182/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 32.5308\n",
      "Epoch 183/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 31.7743\n",
      "Epoch 184/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 31.7336\n",
      "Epoch 185/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 31.9125\n",
      "Epoch 186/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 31.8490\n",
      "Epoch 187/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 31.5899\n",
      "Epoch 188/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 31.2989\n",
      "Epoch 189/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 31.0902\n",
      "Epoch 190/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 31.1018\n",
      "Epoch 191/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 31.2799\n",
      "Epoch 192/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 30.6469\n",
      "Epoch 193/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 30.7408\n",
      "Epoch 194/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 30.3991\n",
      "Epoch 195/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 30.5150\n",
      "Epoch 196/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 30.5976\n",
      "Epoch 197/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 30.0631\n",
      "Epoch 198/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 30.1514\n",
      "Epoch 199/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 30.6261\n",
      "Epoch 200/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 30.8327\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2852a669e08>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, epochs=200, batch_size=100)#배치가 작으면, 특잇값들 때문에 loss에도 영향을 준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_prediction = model.predict(X_test).flatten() # predict : 학습이 다 된 모델에 테스트 값을 넣어 결과를 도출하는 함수 / flatten() : 데이터 배열이 몇차원이던간에, 1차원으로 줄여주는 함수."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6.90909  ,  9.092985 , 12.22614  , 28.340199 , 29.183456 ,\n",
       "       10.640601 , 19.573355 , 21.179733 , 20.165823 ,  9.329421 ,\n",
       "       18.643288 , 14.051578 , 14.940596 , 11.213565 , 27.697033 ,\n",
       "       18.28029  , 22.348972 , 10.589797 , 30.20563  , 15.64039  ,\n",
       "        5.727418 , 19.010628 , 20.275188 , 18.419212 ,  5.582162 ,\n",
       "       25.186691 , 22.411192 ,  8.84802  , 30.061092 , 25.37513  ,\n",
       "       22.07014  , 34.452946 , 14.757553 , 31.397049 , 27.136227 ,\n",
       "       27.368658 , 39.463345 , 25.41061  , 31.1269   , 25.869719 ,\n",
       "       24.822586 , 17.735445 , 15.49206  , 15.715182 , 18.041958 ,\n",
       "       28.697273 , 16.216301 , 23.603516 , 20.079853 , 20.81625  ,\n",
       "       22.267452 , 24.31379  ,  7.3910503, 25.707428 , 15.049638 ,\n",
       "       30.417114 , 20.869984 , 35.57802  , 25.869968 , 27.279793 ,\n",
       "       10.632657 , 15.564782 , 17.551502 , 25.353397 , 24.968143 ,\n",
       "       24.646587 , 29.107887 , 27.215752 , 14.272014 , 23.761015 ,\n",
       "       12.1351385, 28.957355 , 13.608916 , 18.53997  , 23.552689 ,\n",
       "       31.266312 , 10.969033 , 17.360804 , 15.515218 , 38.527668 ,\n",
       "       14.603505 , 19.340332 , 22.96038  , 18.107555 , 40.68724  ,\n",
       "       20.974424 , 26.652756 , 12.326661 , 26.724892 , 14.553662 ,\n",
       "       30.301308 , 20.898304 , 29.402985 , 23.861958 , 25.79831  ,\n",
       "       11.8229265, 21.29483  , 28.472805 , 28.93376  , 24.251328 ,\n",
       "       11.675769 , 13.188997 , 34.90462  , 25.753414 , 29.89093  ,\n",
       "       32.169888 , 13.656677 , 26.861668 , 21.10346  , 25.227673 ,\n",
       "       25.486462 , 18.916544 , 33.285416 , 29.17951  , 25.844278 ,\n",
       "       24.90183  , 25.213726 , 19.2688   , 24.33049  , 14.128207 ,\n",
       "       24.803429 , 24.755775 , 24.685686 , 28.804    , 23.625942 ,\n",
       "       25.563457 , 14.276294 , 19.393362 , 32.961216 , 29.94299  ,\n",
       "       27.364828 , 24.855051 ,  3.5017116, 22.12344  , 35.028164 ,\n",
       "       23.929272 , 30.786522 , 13.571334 , 19.812792 , 16.376308 ,\n",
       "       28.994774 , 22.846249 , 19.734863 , 24.678799 , 14.579236 ,\n",
       "       17.972479 , 20.241236 , 21.33416  , 14.056533 , 31.65714  ,\n",
       "       11.675948 , 16.341438 ], dtype=float32)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "실제가격: 15.600, 예상가격: 6.909\n",
      "실제가격: 18.600, 예상가격: 9.093\n",
      "실제가격: 6.300, 예상가격: 12.226\n",
      "실제가격: 33.100, 예상가격: 28.340\n",
      "실제가격: 24.400, 예상가격: 29.183\n",
      "실제가격: 15.000, 예상가격: 10.641\n",
      "실제가격: 16.800, 예상가격: 19.573\n",
      "실제가격: 18.900, 예상가격: 21.180\n",
      "실제가격: 21.400, 예상가격: 20.166\n",
      "실제가격: 8.500, 예상가격: 9.329\n",
      "실제가격: 21.900, 예상가격: 18.643\n",
      "실제가격: 13.500, 예상가격: 14.052\n",
      "실제가격: 27.100, 예상가격: 14.941\n",
      "실제가격: 8.100, 예상가격: 11.214\n",
      "실제가격: 21.600, 예상가격: 27.697\n",
      "실제가격: 17.800, 예상가격: 18.280\n",
      "실제가격: 28.100, 예상가격: 22.349\n",
      "실제가격: 13.600, 예상가격: 10.590\n",
      "실제가격: 24.000, 예상가격: 30.206\n",
      "실제가격: 15.200, 예상가격: 15.640\n",
      "실제가격: 7.000, 예상가격: 5.727\n",
      "실제가격: 16.400, 예상가격: 19.011\n",
      "실제가격: 18.700, 예상가격: 20.275\n",
      "실제가격: 18.800, 예상가격: 18.419\n",
      "실제가격: 13.100, 예상가격: 5.582\n",
      "실제가격: 20.300, 예상가격: 25.187\n",
      "실제가격: 21.400, 예상가격: 22.411\n",
      "실제가격: 5.000, 예상가격: 8.848\n",
      "실제가격: 35.400, 예상가격: 30.061\n",
      "실제가격: 22.300, 예상가격: 25.375\n"
     ]
    }
   ],
   "source": [
    "for i in range(30):\n",
    "    label = Y_test[i]\n",
    "    prediction = Y_prediction[i]\n",
    "    print(\"실제가격: {:.3f}, 예상가격: {:.3f}\".format(label, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 이미지 인식의 꽃, CNN\n",
    "  - 미국 국립표준기술원(NIST)이 고등학생과 인구조사국 직원 등이 쓴 손글씨를 이용해 만든 데이터를 이용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras import utils\n",
    "\n",
    "import numpy\n",
    "import sys\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(X_train, Y_class_train), (X_test, Y_class_test) = mnist.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습셋 이미지 수 : 60000개\n",
      "테스트셋 이미지 수 : 10000개\n"
     ]
    }
   ],
   "source": [
    "print(\"학습셋 이미지 수 : %d개\" % (X_train.shape[0]))\n",
    "print(\"테스트셋 이미지 수 : %d개\" % (X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAOSklEQVR4nO3df4xU9bnH8c8jgqgQg7JQYsnd3kZNjcnd4kiuQQiXegnyDxDsTUlsaCTdxh9JMcRcw02sPxJDzKUVo2myvSD0ptdaBQQTc4sSEkOi1VFRQfydtWxZYYlKhSgt8Nw/9nCz4sx3lpkzc4Z93q9kMzPnOWfP47gfzsx8z5mvubsAjHznFN0AgNYg7EAQhB0IgrADQRB2IIhzW7mziRMnemdnZyt3CYTS29urQ4cOWaVaQ2E3s3mS1kgaJem/3H1Vav3Ozk6Vy+VGdgkgoVQqVa3V/TLezEZJelTSDZKulLTEzK6s9/cBaK5G3rNPl/SBu3/k7n+T9HtJC/JpC0DeGgn7pZL2DXncly37GjPrNrOymZUHBgYa2B2ARjQS9kofAnzj3Ft373H3kruXOjo6GtgdgEY0EvY+SVOHPP62pP2NtQOgWRoJ+yuSLjOz75jZGEk/krQ1n7YA5K3uoTd3P25mt0v6owaH3ta5+57cOgOQq4bG2d39WUnP5tQLgCbidFkgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EARhB4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeCaGgWV7S/kydPJuvHjh1r6v43bNhQtXb06NHktm+//Xay/tBDDyXrK1eurFp75JFHktuef/75yfrq1auT9VtuuSVZL0JDYTezXklfSDoh6bi7l/JoCkD+8jiy/4u7H8rh9wBoIt6zA0E0GnaXtM3MXjWz7kormFm3mZXNrDwwMNDg7gDUq9Gwz3D3aZJukHSbmc06fQV373H3kruXOjo6GtwdgHo1FHZ335/dHpS0WdL0PJoCkL+6w25mF5rZ+FP3Jc2VtDuvxgDkq5FP4ydL2mxmp37P/7j7/+bS1Qhz+PDhZP3EiRPJ+htvvJGsb9u2rWrt888/T27b09OTrBeps7MzWV+xYkWyvnbt2qq1iy66KLntzJkzk/U5c+Yk6+2o7rC7+0eS/inHXgA0EUNvQBCEHQiCsANBEHYgCMIOBMElrjno6+tL1ru6upL1zz77LMduzh7nnJM+1qSGzqTal6EuW7asam3SpEnJbceNG5esn41ng3JkB4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEgGGfPwSWXXJKsT548OVlv53H2uXPnJuu1/ts3bdpUtXbeeeclt509e3ayjjPDkR0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgmCcPQe1rqtev359sv7UU08l69dee22yvnjx4mQ95brrrkvWt2zZkqyPGTMmWf/kk0+q1tasWZPcFvniyA4EQdiBIAg7EARhB4Ig7EAQhB0IgrADQZi7t2xnpVLJy+Vyy/Z3tjh27FiyXmsse+XKlVVrDz74YHLbHTt2JOuzZs1K1tFeSqWSyuWyVarVPLKb2TozO2hmu4csu9jMnjOz97PbCXk2DCB/w3kZv17SvNOW3SVpu7tfJml79hhAG6sZdnd/QdKnpy1eIGlDdn+DpIX5tgUgb/V+QDfZ3fslKbutOnGWmXWbWdnMygMDA3XuDkCjmv5pvLv3uHvJ3Utn42R4wEhRb9gPmNkUScpuD+bXEoBmqDfsWyUtze4vlZS+DhJA4Wpez25mj0uaLWmimfVJ+oWkVZL+YGbLJP1Z0g+b2eRIV+v702uZMKH+kc+HH344WZ85c2ayblZxSBdtqGbY3X1JldIPcu4FQBNxuiwQBGEHgiDsQBCEHQiCsANB8FXSI8Dy5cur1l5++eXktps3b07W9+zZk6xfddVVyTraB0d2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQiCcfYRIPVV0z09Pcltt2/fnqwvWLAgWV+4cGGyPmPGjKq1RYsWJbfl8tl8cWQHgiDsQBCEHQiCsANBEHYgCMIOBEHYgSCYsjm4Wte7z5t3+pyeX3f48OG6971u3bpkffHixcn6uHHj6t73SNXQlM0ARgbCDgRB2IEgCDsQBGEHgiDsQBCEHQiC69mDmz59erJe63vj77jjjmT9ySefrFq7+eabk9t++OGHyfqdd96ZrI8fPz5Zj6bmkd3M1pnZQTPbPWTZPWb2FzPblf3Mb26bABo1nJfx6yVVOo3qV+7elf08m29bAPJWM+zu/oKkT1vQC4AmauQDutvN7M3sZf6EaiuZWbeZlc2sPDAw0MDuADSi3rD/WtJ3JXVJ6pe0utqK7t7j7iV3L3V0dNS5OwCNqivs7n7A3U+4+0lJv5GU/kgXQOHqCruZTRnycJGk3dXWBdAeal7PbmaPS5otaaKkA5J+kT3ukuSSeiX9zN37a+2M69lHnq+++ipZf+mll6rWrr/++uS2tf42b7zxxmT9iSeeSNZHotT17DVPqnH3JRUWr224KwAtxemyQBCEHQiCsANBEHYgCMIOBMElrmjI2LFjk/XZs2dXrY0aNSq57fHjx5P1p59+Oll/9913q9auuOKK5LYjEUd2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQiCcXYk7d+/P1nftGlTsv7iiy9WrdUaR6/lmmuuSdYvv/zyhn7/SMORHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeCYJx9hKs15dajjz6arD/22GPJel9f3xn3NFy1rnfv7OxM1s0qfqNyWBzZgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIxtnPAkeOHEnWn3nmmaq1++67L7nte++9V1dPeZgzZ06yvmrVqmT96quvzrOdEa/mkd3MpprZDjPba2Z7zOzn2fKLzew5M3s/u53Q/HYB1Gs4L+OPS1rh7t+T9M+SbjOzKyXdJWm7u18maXv2GECbqhl2d+9399ey+19I2ivpUkkLJG3IVtsgaWGTegSQgzP6gM7MOiV9X9KfJE12935p8B8ESZOqbNNtZmUzK9c6TxtA8ww77GY2TtJGScvd/a/D3c7de9y95O6ljo6OenoEkINhhd3MRmsw6L9z91NfJ3rAzKZk9SmSDjanRQB5qDn0ZoPXCa6VtNfdfzmktFXSUkmrststTelwBDh69Giyvm/fvmT9pptuStZff/31M+4pL3Pnzk3W77333qq1Wl8FzSWq+RrOOPsMST+W9JaZ7cqWrdRgyP9gZssk/VnSD5vSIYBc1Ay7u++UVO2f2B/k2w6AZuF0WSAIwg4EQdiBIAg7EARhB4LgEtdh+vLLL6vWli9fntx2586dyfo777xTT0u5mD9/frJ+9913J+tdXV3J+ujRo8+0JTQJR3YgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCCLMOHtvb2+y/sADDyTrzz//fNXaxx9/XE9Lubnggguq1u6///7ktrfeemuyPmbMmLp6QvvhyA4EQdiBIAg7EARhB4Ig7EAQhB0IgrADQYQZZ9+4cWOyvnbt2qbte9q0acn6kiVLkvVzz03/b+ru7q5aGzt2bHJbxMGRHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeCMHdPr2A2VdJvJX1L0klJPe6+xszukfRTSQPZqivd/dnU7yqVSl4ulxtuGkBlpVJJ5XK54qzLwzmp5rikFe7+mpmNl/SqmT2X1X7l7v+ZV6MAmmc487P3S+rP7n9hZnslXdrsxgDk64zes5tZp6TvS/pTtuh2M3vTzNaZ2YQq23SbWdnMygMDA5VWAdACww67mY2TtFHScnf/q6RfS/qupC4NHvlXV9rO3XvcveTupY6OjsY7BlCXYYXdzEZrMOi/c/dNkuTuB9z9hLuflPQbSdOb1yaARtUMu5mZpLWS9rr7L4csnzJktUWSduffHoC8DOfT+BmSfizpLTPblS1bKWmJmXVJckm9kn7WhP4A5GQ4n8bvlFRp3C45pg6gvXAGHRAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EARhB4Ig7EAQhB0IgrADQRB2IIiaXyWd687MBiR9PGTRREmHWtbAmWnX3tq1L4ne6pVnb//g7hW//62lYf/Gzs3K7l4qrIGEdu2tXfuS6K1ereqNl/FAEIQdCKLosPcUvP+Udu2tXfuS6K1eLemt0PfsAFqn6CM7gBYh7EAQhYTdzOaZ2btm9oGZ3VVED9WYWa+ZvWVmu8ys0Pmlszn0DprZ7iHLLjaz58zs/ey24hx7BfV2j5n9JXvudpnZ/IJ6m2pmO8xsr5ntMbOfZ8sLfe4SfbXkeWv5e3YzGyXpPUn/KqlP0iuSlrj72y1tpAoz65VUcvfCT8Aws1mSjkj6rbtflS17UNKn7r4q+4dygrv/e5v0do+kI0VP453NVjRl6DTjkhZK+okKfO4Sff2bWvC8FXFkny7pA3f/yN3/Jun3khYU0Efbc/cXJH162uIFkjZk9zdo8I+l5ar01hbcvd/dX8vufyHp1DTjhT53ib5aooiwXypp35DHfWqv+d5d0jYze9XMuotupoLJ7t4vDf7xSJpUcD+nqzmNdyudNs142zx39Ux/3qgiwl5pKql2Gv+b4e7TJN0g6bbs5SqGZ1jTeLdKhWnG20K90583qoiw90maOuTxtyXtL6CPitx9f3Z7UNJmtd9U1AdOzaCb3R4suJ//107TeFeaZlxt8NwVOf15EWF/RdJlZvYdMxsj6UeSthbQxzeY2YXZBycyswslzVX7TUW9VdLS7P5SSVsK7OVr2mUa72rTjKvg567w6c/dveU/kuZr8BP5DyX9RxE9VOnrHyW9kf3sKbo3SY9r8GXd3zX4imiZpEskbZf0fnZ7cRv19t+S3pL0pgaDNaWg3q7T4FvDNyXtyn7mF/3cJfpqyfPG6bJAEJxBBwRB2IEgCDsQBGEHgiDsQBCEHQiCsANB/B/B/E1sUrHmQgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(X_train[0], cmap='Greys')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
      "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
      "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
      "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
      "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
      "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t3\t18\t18\t18\t126\t136\t175\t26\t166\t255\t247\t127\t0\t0\t0\t0\t\n",
      "0\t0\t0\t0\t0\t0\t0\t0\t30\t36\t94\t154\t170\t253\t253\t253\t253\t253\t225\t172\t253\t242\t195\t64\t0\t0\t0\t0\t\n",
      "0\t0\t0\t0\t0\t0\t0\t49\t238\t253\t253\t253\t253\t253\t253\t253\t253\t251\t93\t82\t82\t56\t39\t0\t0\t0\t0\t0\t\n",
      "0\t0\t0\t0\t0\t0\t0\t18\t219\t253\t253\t253\t253\t253\t198\t182\t247\t241\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
      "0\t0\t0\t0\t0\t0\t0\t0\t80\t156\t107\t253\t253\t205\t11\t0\t43\t154\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
      "0\t0\t0\t0\t0\t0\t0\t0\t0\t14\t1\t154\t253\t90\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
      "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t139\t253\t190\t2\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
      "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t11\t190\t253\t70\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
      "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t35\t241\t225\t160\t108\t1\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
      "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t81\t240\t253\t253\t119\t25\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
      "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t45\t186\t253\t253\t150\t27\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
      "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t16\t93\t252\t253\t187\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
      "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t249\t253\t249\t64\t0\t0\t0\t0\t0\t0\t0\t\n",
      "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t46\t130\t183\t253\t253\t207\t2\t0\t0\t0\t0\t0\t0\t0\t\n",
      "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t39\t148\t229\t253\t253\t253\t250\t182\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
      "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t24\t114\t221\t253\t253\t253\t253\t201\t78\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
      "0\t0\t0\t0\t0\t0\t0\t0\t23\t66\t213\t253\t253\t253\t253\t198\t81\t2\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
      "0\t0\t0\t0\t0\t0\t18\t171\t219\t253\t253\t253\t253\t195\t80\t9\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
      "0\t0\t0\t0\t55\t172\t226\t253\t253\t253\t253\t244\t133\t11\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
      "0\t0\t0\t0\t136\t253\t253\t253\t212\t135\t132\t16\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
      "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
      "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
      "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n"
     ]
    }
   ],
   "source": [
    "for x in X_train[0]:\n",
    "    for i in x:\n",
    "        sys.stdout.write('%d\\t' % i)\n",
    "    sys.stdout.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], 784) # 가로 28, 세로 28의 2차원 배열을 784개의 1차원 배열로 바꿔주기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float64')\n",
    "X_train = X_train / 255 #케라스는 데이터를 0에서 1 사이의 값으로 변환한 다음 구동할 때 최적의 성능을 보여 255로 나눠 0~1사이 값으로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.reshape(X_test.shape[0], 784).astype('float64') / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class : 5 \n"
     ]
    }
   ],
   "source": [
    "print(\"class : %d \" % (Y_class_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "Y_train = utils.to_categorical(Y_class_train, 10) #딥러닝의 분류 문제를 해결하기위해 원 - 핫 - 인코딩 방식을 적용해주기,\n",
    "Y_test = utils.to_categorical(Y_class_test, 10) # 즉 0또는 1로만 이루어진 벡터로 값 수정하기\n",
    "\n",
    "print(Y_train[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_dim=784, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1층 모델 : 가중치 파라미터 784*512 = 401,408개/ 바이오스 파라미터 512 개, 합 = 401,920개\n",
    "# 2층 모델 : 가중치 파라미터 512*10 = 5120개/ 바이오스 파라미터 10 개, 합 = 5,130개\n",
    "\n",
    "# 2개의 모델 총 401,920 + 5,130 = 407,050개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 407,050\n",
      "Trainable params: 407,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "import os\n",
    "# 모델 최적화 설정\n",
    "MODEL_DIR = './model/'\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.mkdir(MODEL_DIR)\n",
    "\n",
    "modelpath=\"./model/{epoch:02d}-{val_loss:.4f}.hdf5\"\n",
    "checkpointer = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=1, save_best_only=True)\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.15098, saving model to ./model\\01-0.1510.hdf5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.15098 to 0.10518, saving model to ./model\\02-0.1052.hdf5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.10518 to 0.08240, saving model to ./model\\03-0.0824.hdf5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.08240 to 0.07580, saving model to ./model\\04-0.0758.hdf5\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.07580 to 0.06441, saving model to ./model\\05-0.0644.hdf5\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.06441\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.06441 to 0.06295, saving model to ./model\\07-0.0629.hdf5\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.06295 to 0.06105, saving model to ./model\\08-0.0611.hdf5\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.06105\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.06105\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.06105 to 0.05691, saving model to ./model\\11-0.0569.hdf5\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.05691\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.05691\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.05691\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.05691\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.05691\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.05691\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.05691\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.05691\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.05691\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.05691\n"
     ]
    }
   ],
   "source": [
    "# 모델의 실행\n",
    "history = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=30, batch_size=200, verbose=0, callbacks=[early_stopping_callback,checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.0647 - accuracy: 0.9842\n",
      "\n",
      " Test Accuracy: 0.9842\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n Test Accuracy: %.4f\" % (model.evaluate(X_test, Y_test)[1]))\n",
    "y_vloss = history.history['val_loss']\n",
    "  \n",
    "# 학습셋의 오차\n",
    "y_loss = history.history['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1ad0b0fccc8>]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 그래프로 표현\n",
    "x_len = numpy.arange(len(y_loss))\n",
    "plt.plot(x_len, y_vloss, marker='.', c=\"red\", label='Testset_loss')\n",
    "plt.plot(x_len, y_loss, marker='.', c=\"blue\", label='Trainset_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# 그래프로 표현\n",
    "x_len = numpy.arange(len(y_loss))\n",
    "plt.plot(x_len, y_vloss, marker='.', c=\"red\", label='Testset_loss')\n",
    "plt.plot(x_len, y_loss, marker='.', c=\"blue\", label='Trainset_loss')\n",
    "# 그래프에 그리드를 주고 레이블을 표시\n",
    "plt.legend(loc='upper right')\n",
    "# plt.axis([0, 20, 0, 0.35])\n",
    "plt.grid()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 딥러닝 프레임"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convolution Neural Network\n",
    "  - Convolution\n",
    "      - 공간 영역 기반 처리\n",
    "      - mask( 또는 커널이라고 함) 내의 원소값을 이용, 원래의 이미지 값을 변경시켜준다.\n",
    "      - 3 by 3 의 mask를 이미지의 화소값과 곱해주어 나오는 특정값을 더하고, 그 값을 이미지의 가운데 화소 값으로 변경시켜준다.\n",
    "      - 스트라이드 : 이동할 칸의 개수를 의미. ex) 스트라이드 1 : 1칸씩 이동하겠다.\n",
    "  - Convolution의 예시\n",
    "      - Blurring\n",
    "          - 전체 마스크값이 1/9이며 화소값이 급격하게 변하는 부분을 부드럽게 처리한다.\n",
    "      - Edge\n",
    "          - 화소값이 급격히 변하는 부분(Edge)를 처리하는  방법.\n",
    "          - 수직마스크 (좌측이 모두 -1, 중간이 0, 우측이 1) / 수평마스크\n",
    "              - 경계를 검출해주는 프로그래밍.\n",
    "              \n",
    "  - Filtering > Min / Max\n",
    "      - 마스크가 겹치지 않고 마스크 크기만큼 띄어 간다. \n",
    "      - 마스크는 마스크 크기 만큼의 이미지 화소 중에서 최대값 또는 최소값을 중앙에 두어 원래 이미지의 크기를 변경한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - 여러가지 마스크를 만들어, 적용할 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
